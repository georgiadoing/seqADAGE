{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ADAGE models on RNAseq data\n",
    "\n",
    "### Georgia Doing 2021\n",
    "\n",
    "This notebook walks through the training of an RNAseq-based ADAGE model as a continuation of analyses from last year (2021_06_eADAGE_adapt/seqADAGE).\n",
    "\n",
    "Since then we have downloaded a new compendium of RNAseq data and aligned it to the PAO1 and PA14 reference genomes using prokarytoic-optimized parameters.\n",
    "(https://github.com/hoganlab-dartmouth/pa-seq-compendia)\n",
    "\n",
    "The main objectives are to train models, take a first look and save the models for further analysis on forthcoming notebooks\n",
    "1. Load compendia\n",
    "2. Train and save models\n",
    "3. Visualize first-look plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_count_autoencoder\n",
    "import run_model\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from imp import reload\n",
    "import Adage\n",
    "from scipy.stats import hypergeom\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Adage' from '/home/georgia/Dropbox (Hogan Lab)/Digital_notebook/2021_04_23_seqADAGE/seqADAGE/Py/Adage.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for dev purposes, while updating run_model.py need to reload\n",
    "reload(run_count_autoencoder)\n",
    "reload(run_model)\n",
    "reload(Adage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5210 1051\n"
     ]
    }
   ],
   "source": [
    "array_comp = np.loadtxt(open('../data_files/ADAGE_compendium.csv', \"rb\"),delimiter=',',skiprows = 1)\n",
    "a_gene_num = np.size(array_comp, 0)\n",
    "a_samp_num = np.size(array_comp, 1)\n",
    "print(a_gene_num, a_samp_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5543 2288\n"
     ]
    }
   ],
   "source": [
    "seq_comp = np.loadtxt(open('../data_files/rnaseq_compendium_filtered_counts_floor_no_names.csv', \"rb\"),delimiter=',',skiprows = 1)\n",
    "s_gene_num = np.size(seq_comp, 0)\n",
    "s_samp_num = np.size(seq_comp, 1)\n",
    "print(s_gene_num, s_samp_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5543 2288\n"
     ]
    }
   ],
   "source": [
    "seq_comp = np.loadtxt(open('../data_files/rnaseq_compendium_filtered_no_gene_names.csv', \"rb\"),delimiter=',',skiprows = 1)\n",
    "s_gene_num = np.size(seq_comp, 0)\n",
    "s_samp_num = np.size(seq_comp, 1)\n",
    "print(s_gene_num, s_samp_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/georgia/anaconda3/envs/tfk/lib/python3.7/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/georgia/anaconda3/envs/tfk/lib/python3.7/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(8 ,4))\n",
    "sns.distplot(array_comp.flatten(), ax=ax1)\n",
    "sns.distplot(seq_comp.flatten(), ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow\n",
      "(5543, 2288)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TiedWeightsEncoder.call of <TiedWeightsEncoder.TiedWeightsEncoder object at 0x7f84810dbb10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TiedWeightsEncoder.call of <TiedWeightsEncoder.TiedWeightsEncoder object at 0x7f84810dbb10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 8s 34ms/step - loss: 39.7501 - val_loss: 9.2154\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: 6.5600 - val_loss: 6.3662\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 7s 35ms/step - loss: 5.3719 - val_loss: 6.3533\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 7s 34ms/step - loss: 5.3743 - val_loss: 6.3480\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.3444 - val_loss: 6.3478\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: 5.4167 - val_loss: 6.3275\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 7s 36ms/step - loss: 5.3741 - val_loss: 6.3196\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.3382 - val_loss: 6.3156\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 7s 33ms/step - loss: 5.3012 - val_loss: 6.2963\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 7s 33ms/step - loss: 5.2921 - val_loss: 6.2913\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: 5.2809 - val_loss: 6.2782\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.3104 - val_loss: 6.2745\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.2610 - val_loss: 6.2645\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.3423 - val_loss: 6.2585\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.2887 - val_loss: 6.2495\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.2792 - val_loss: 6.2385\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.2909 - val_loss: 6.2238\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.2367 - val_loss: 6.2165\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: 5.2751 - val_loss: 6.2083\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: 5.2318 - val_loss: 6.1993\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.2646 - val_loss: 6.1914\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.1904 - val_loss: 6.1816\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.1855 - val_loss: 6.1713\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.2331 - val_loss: 6.1638\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.2269 - val_loss: 6.1513\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.2166 - val_loss: 6.1484\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.1703 - val_loss: 6.1342\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: 5.2001 - val_loss: 6.1321\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.1831 - val_loss: 6.1292\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.1862 - val_loss: 6.1140\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.1683 - val_loss: 6.0981\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.1731 - val_loss: 6.0953\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.1636 - val_loss: 6.0851\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.1309 - val_loss: 6.0807\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: 5.1742 - val_loss: 6.0652\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.1158 - val_loss: 6.0612\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.1308 - val_loss: 6.0539\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.1435 - val_loss: 6.0501\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.1093 - val_loss: 6.0441\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.1058 - val_loss: 6.0405\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.1177 - val_loss: 6.0258\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.0762 - val_loss: 6.0130\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.0901 - val_loss: 6.0037\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: 5.0997 - val_loss: 6.0000\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.0582 - val_loss: 6.0002\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: 5.0637 - val_loss: 5.9831\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 7s 33ms/step - loss: 5.0734 - val_loss: 5.9737\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: 5.0942 - val_loss: 5.9661\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.0229 - val_loss: 5.9523\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.0307 - val_loss: 5.9484\n",
      "(2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/georgia/anaconda3/envs/tfk/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5549, 600)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(run_count_autoencoder)\n",
    "m = run_count_autoencoder.run_count_autoencoder('../data_files/rnaseq_compendium_filtered.csv',lr=.0001,seed=460,kl1=1e-10,kl2=1e-1, act = 'sigmoid', tied = True, epochs=50, init='glorot_uniform', batch_size=10)\n",
    "tf_adage = Adage.Adage(m.autoencoder, m.history, m.compendium)\n",
    "tf_weights = np.array(pd.read_csv('../outputs/ADAGE_OG_weights.csv', header = None))\n",
    "tf_cost = pd.read_csv('../outputs/ADAGE_OG_cost_log.csv')\n",
    "tf_adage.weights = tf_weights\n",
    "tf_adage.loss = tf_cost['cost']\n",
    "tf_adage.set_hwg_cutoff(2.5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 1.0, 'k_adage')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dict = {\n",
    "    \"tf_adage\": tf_adage,\n",
    "    \"k_adage\": m\n",
    "}\n",
    "fig, ax = plt.subplots(1, 2,figsize=(16 ,4))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "name = 'tf_adage'\n",
    "model_temp = model_dict[name]\n",
    "ax[0].plot(list(range(0,50)), model_temp.loss[0:50], linewidth=1, markersize=2, color = 'orange')\n",
    "ax[0].set(title = name, xlabel = 'Epochs', ylabel = 'Loss')\n",
    "ax[0].set(title = name)\n",
    "\n",
    "name = 'k_adage'\n",
    "model_temp = model_dict[name]\n",
    "ax[1].plot(list(range(0,50)), model_temp.loss, linewidth=1, markersize=2, color = 'orange')\n",
    "ax[1].set(title = name, xlabel = 'Epochs', ylabel = 'Loss')\n",
    "ax[1].set(title = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad_glorot_uniform_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 3.9967 - val_loss: 4.7006\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9052 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9145 - val_loss: 4.7006\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9184 - val_loss: 4.7006\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9599 - val_loss: 4.7007\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9199 - val_loss: 4.7007\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9177 - val_loss: 4.7007\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9296 - val_loss: 4.7008\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9355 - val_loss: 4.7007\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9237 - val_loss: 4.7007\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9167 - val_loss: 4.7011\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.8833 - val_loss: 4.7008\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9366 - val_loss: 4.7011\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9580 - val_loss: 4.7012\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9157 - val_loss: 4.7010\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9176 - val_loss: 4.7011\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9194 - val_loss: 4.7011\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9308 - val_loss: 4.7009\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9320 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.8964 - val_loss: 4.7008\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9395 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9476 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9319 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9275 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9277 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9310 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9411 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9470 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9421 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9119 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9356 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9502 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9194 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9248 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9120 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9167 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9086 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9180 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9388 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9003 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9095 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9202 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9345 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9076 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9368 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9389 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9369 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.8935 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9284 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9287 - val_loss: 4.7010\n",
      "(2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/georgia/anaconda3/envs/tfk/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 7586.1262 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7660.8981 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7421.5964 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7439.0113 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 7466.3986 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7993.9236 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7681.6827 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7831.3484 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7869.8436 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7684.8621 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7717.2547 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 8011.9229 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7831.5705 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7462.0622 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7899.0986 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7438.3902 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7632.5899 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7992.2489 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7648.4366 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7716.9285 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7482.4801 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7760.9367 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7561.1871 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7680.5088 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7699.6397 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7137.2842 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7790.0278 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7846.0149 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7621.9899 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7509.5736 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7916.9377 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7704.4102 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7606.1404 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7498.7636 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7803.8242 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7508.2147 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7373.8941 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7493.7335 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7792.5360 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7364.1453 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7641.6355 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7279.0622 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7517.4978 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7610.2099 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7798.9606 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7506.1642 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7382.6411 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7423.8436 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7780.5971 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7611.3560 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 4.0712 - val_loss: 4.7008\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9410 - val_loss: 4.7010\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9124 - val_loss: 4.7008\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9466 - val_loss: 4.7010\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9413 - val_loss: 4.7008\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9520 - val_loss: 4.7009\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9484 - val_loss: 4.7018\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9103 - val_loss: 4.7007\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9391 - val_loss: 4.7009\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9230 - val_loss: 4.7008\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9255 - val_loss: 4.7009\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9432 - val_loss: 4.7010\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9534 - val_loss: 4.7010\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.8899 - val_loss: 4.7010\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9341 - val_loss: 4.7010\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9219 - val_loss: 4.7010\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9180 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9335 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9132 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9468 - val_loss: 4.7011\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9283 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9228 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9370 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9370 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9437 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9254 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9241 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9239 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9183 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.8888 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9376 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9380 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9257 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9385 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9692 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9428 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9386 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9345 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9516 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9284 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9244 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9584 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9351 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9331 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9446 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9230 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9136 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9573 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9273 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9175 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7621.9007 - val_loss: 15292.4414\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7822.8003 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7693.2880 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7782.5485 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7214.2127 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7383.5616 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7454.6287 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 7422.6547 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7614.9178 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7619.7853 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7655.6506 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7559.8631 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7586.0195 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7465.5226 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7537.5570 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7627.2997 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7480.0613 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7861.0604 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7693.6353 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7535.5789 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7805.5573 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7508.8296 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7994.1070 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7428.6142 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7595.1420 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7825.2187 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7233.7292 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7614.9727 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7302.9471 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7629.9289 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7835.4441 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8029.7862 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7586.6588 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 7385.3808 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7807.4074 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7576.9202 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7659.1461 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7685.1475 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7272.7543 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7754.4704 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7637.0370 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7993.3310 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7282.5575 - val_loss: 15292.4346\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: 7415.0333 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7267.1316 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7482.9402 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7878.1732 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7725.2884 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7605.2116 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7710.3647 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 24.0705 - val_loss: 8.4241\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.8603 - val_loss: 4.6160\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 5.4919 - val_loss: 5.6916\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 6.3402 - val_loss: 7.9874\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 6.7137 - val_loss: 9.0751\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8.7949 - val_loss: 7.9173\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 8.2277 - val_loss: 16.6538\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 25.3127 - val_loss: 41.5570\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 24.7932 - val_loss: 34.0899\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 25.5552 - val_loss: 54.7431\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 24.5260 - val_loss: 11.8340\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 23.4123 - val_loss: 9.9574\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 6.9069 - val_loss: 6.8576\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 6.3945 - val_loss: 6.4573\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.5738 - val_loss: 6.3339\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 5.0310 - val_loss: 6.3279\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9651 - val_loss: 6.3847\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 5.3347 - val_loss: 11.6894\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.5396 - val_loss: 6.8976\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.6603 - val_loss: 6.3345\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 5.5290 - val_loss: 7.7226\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 5.4810 - val_loss: 8.0103\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.2242 - val_loss: 6.1514\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.7955 - val_loss: 6.1704\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.8479 - val_loss: 6.1372\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.7637 - val_loss: 6.1412\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 4.7940 - val_loss: 6.1283\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.7804 - val_loss: 6.1229\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.8145 - val_loss: 6.1257\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.7777 - val_loss: 6.1276\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 4.8198 - val_loss: 6.1225\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.8044 - val_loss: 6.1349\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.8109 - val_loss: 6.1268\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.7953 - val_loss: 6.1245\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.8006 - val_loss: 6.1242\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.7835 - val_loss: 6.1243\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.8482 - val_loss: 6.1174\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.7815 - val_loss: 6.1176\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.7976 - val_loss: 6.1171\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.8429 - val_loss: 6.1172\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.7492 - val_loss: 6.1162\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.8041 - val_loss: 6.1169\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.7439 - val_loss: 6.1169\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.8287 - val_loss: 6.1169\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.7964 - val_loss: 6.1168\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.8364 - val_loss: 6.1168\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.7563 - val_loss: 6.1167\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.8526 - val_loss: 6.1167\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.8277 - val_loss: 6.1167\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.7381 - val_loss: 6.1167\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2128.7157 - val_loss: 2250.0322\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 955.2392 - val_loss: 2226.2637\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 965.1938 - val_loss: 1988.2134\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 913.1228 - val_loss: 2179.3550\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 868.0218 - val_loss: 1923.3186\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 828.5240 - val_loss: 1979.2144\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 919.3128 - val_loss: 2086.9822\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 901.3868 - val_loss: 2020.6437\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 879.7557 - val_loss: 1698.9739\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 908.1968 - val_loss: 1914.7501\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 950.8690 - val_loss: 1839.1437\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 865.7832 - val_loss: 1762.6843\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 879.0077 - val_loss: 1930.3361\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 851.1487 - val_loss: 1813.4519\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 909.9485 - val_loss: 1774.0468\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 876.0262 - val_loss: 1802.2422\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 820.4272 - val_loss: 2032.6853\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 926.5118 - val_loss: 1961.7916\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 899.1624 - val_loss: 1749.2596\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 891.8266 - val_loss: 1843.5841\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 900.1305 - val_loss: 1888.8568\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 852.2597 - val_loss: 1822.2075\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 843.4203 - val_loss: 1844.4958\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 861.7163 - val_loss: 1844.6519\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 893.2524 - val_loss: 1867.6993\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 862.6263 - val_loss: 1857.9902\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 903.7041 - val_loss: 1867.2491\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 893.5596 - val_loss: 1854.5164\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 875.8721 - val_loss: 1852.8770\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 871.9639 - val_loss: 1850.6886\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 832.8792 - val_loss: 1850.9032\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 819.9392 - val_loss: 1848.1958\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 822.4627 - val_loss: 1850.3966\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 845.6325 - val_loss: 1849.2559\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 857.6746 - val_loss: 1848.9614\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 920.9737 - val_loss: 1849.8535\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 907.3268 - val_loss: 1850.6780\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 894.7255 - val_loss: 1850.7419\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 829.8650 - val_loss: 1849.9939\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 825.5561 - val_loss: 1849.5750\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 849.3338 - val_loss: 1849.4109\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 833.1696 - val_loss: 1849.1754\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 843.5443 - val_loss: 1848.9297\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 874.2328 - val_loss: 1849.0767\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 869.6959 - val_loss: 1849.0564\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 825.2608 - val_loss: 1848.7817\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 828.7579 - val_loss: 1848.6807\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 844.5301 - val_loss: 1848.6909\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 849.9384 - val_loss: 1848.5764\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 893.7508 - val_loss: 1848.5852\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 4.0295 - val_loss: 2.6974\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2415 - val_loss: 2.5977\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1921 - val_loss: 2.5807\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1737 - val_loss: 2.5626\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1578 - val_loss: 2.5519\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1217 - val_loss: 2.5058\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1115 - val_loss: 2.5298\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1046 - val_loss: 2.4885\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1025 - val_loss: 2.5474\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1117 - val_loss: 2.5137\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1074 - val_loss: 2.5201\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1128 - val_loss: 2.4909\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0968 - val_loss: 2.4914\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1014 - val_loss: 2.4958\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1039 - val_loss: 2.5747\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1108 - val_loss: 2.5086\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1483 - val_loss: 2.5072\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.1413 - val_loss: 2.5115\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0782 - val_loss: 2.4905\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0723 - val_loss: 2.4821\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0702 - val_loss: 2.4809\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0720 - val_loss: 2.4819\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0708 - val_loss: 2.4865\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0739 - val_loss: 2.4902\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0704 - val_loss: 2.4765\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0601 - val_loss: 2.4774\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0611 - val_loss: 2.4759\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0651 - val_loss: 2.4786\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0623 - val_loss: 2.4766\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0599 - val_loss: 2.4806\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0544 - val_loss: 2.4728\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0569 - val_loss: 2.4757\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0523 - val_loss: 2.4684\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0561 - val_loss: 2.4729\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.0639 - val_loss: 2.4816\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0573 - val_loss: 2.4690\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0554 - val_loss: 2.4694\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0555 - val_loss: 2.4683\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0513 - val_loss: 2.4689\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0499 - val_loss: 2.4728\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.0497 - val_loss: 2.4717\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0536 - val_loss: 2.4678\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.0580 - val_loss: 2.4680\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0531 - val_loss: 2.4640\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.0460 - val_loss: 2.4641\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0522 - val_loss: 2.4623\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0459 - val_loss: 2.4732\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0497 - val_loss: 2.4678\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0501 - val_loss: 2.4664\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0479 - val_loss: 2.4648\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3243.5633 - val_loss: 2473.5557\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 1114.6346 - val_loss: 2101.8882\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 921.5683 - val_loss: 1916.1189\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 877.5852 - val_loss: 1991.4293\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 938.1009 - val_loss: 1914.6512\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 861.5092 - val_loss: 1843.1658\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 873.3093 - val_loss: 1852.7568\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 914.1403 - val_loss: 1962.4247\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 916.1607 - val_loss: 1797.2638\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 850.8726 - val_loss: 1803.5784\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 850.4595 - val_loss: 1898.2136\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 910.3501 - val_loss: 1916.8059\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 829.1546 - val_loss: 1789.0686\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 878.9732 - val_loss: 1883.4695\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 838.2672 - val_loss: 1827.2744\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 888.2365 - val_loss: 1902.8732\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 867.9197 - val_loss: 1809.2518\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 856.5521 - val_loss: 1792.4528\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 900.6089 - val_loss: 1826.9325\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 837.6204 - val_loss: 1911.5537\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 847.1641 - val_loss: 1837.7777\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 856.6832 - val_loss: 1826.5790\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 887.3663 - val_loss: 1812.2209\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 843.1343 - val_loss: 1831.8115\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 895.5766 - val_loss: 1877.1571\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 818.3414 - val_loss: 1845.2241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 868.0888 - val_loss: 1842.3087\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 820.2502 - val_loss: 1831.8447\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 858.0658 - val_loss: 1829.1952\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 852.3370 - val_loss: 1837.1652\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 807.6311 - val_loss: 1837.2999\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 905.9026 - val_loss: 1853.9257\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 816.4044 - val_loss: 1829.6937\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 901.5256 - val_loss: 1834.0173\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 811.9076 - val_loss: 1836.9768\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 847.6511 - val_loss: 1837.3839\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 824.7646 - val_loss: 1840.0811\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 856.7832 - val_loss: 1841.4020\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 920.5180 - val_loss: 1843.5690\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 873.4325 - val_loss: 1844.0463\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 888.2701 - val_loss: 1844.9287\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 847.1908 - val_loss: 1843.8170\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 824.9964 - val_loss: 1844.4883\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 849.2540 - val_loss: 1844.4089\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 854.4563 - val_loss: 1844.3805\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 816.5208 - val_loss: 1844.5077\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 804.5607 - val_loss: 1844.5457\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 850.9014 - val_loss: 1844.5315\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 830.4642 - val_loss: 1844.5858\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 848.2633 - val_loss: 1844.6831\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.2272 - val_loss: 4.9316\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1657 - val_loss: 4.9059\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1393 - val_loss: 4.8879\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0836 - val_loss: 4.8410\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0677 - val_loss: 4.8358\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0776 - val_loss: 4.8792\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0687 - val_loss: 4.8143\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0191 - val_loss: 4.8101\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0300 - val_loss: 4.8086\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0366 - val_loss: 4.8238\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0604 - val_loss: 4.8055\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0589 - val_loss: 4.8075\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0574 - val_loss: 4.8606\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0366 - val_loss: 4.7997\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0064 - val_loss: 4.7987\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0500 - val_loss: 4.7957\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0561 - val_loss: 4.7958\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0481 - val_loss: 4.7954\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0511 - val_loss: 4.8007\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0301 - val_loss: 4.7983\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0168 - val_loss: 4.8148\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0299 - val_loss: 4.7946\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0251 - val_loss: 4.8012\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0264 - val_loss: 4.7940\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0492 - val_loss: 4.7916\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0174 - val_loss: 4.7980\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0412 - val_loss: 4.7932\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0043 - val_loss: 4.7967\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0075 - val_loss: 4.7901\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0355 - val_loss: 4.7913\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0102 - val_loss: 4.7898\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0196 - val_loss: 4.7922\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0203 - val_loss: 4.7926\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0195 - val_loss: 4.7909\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0273 - val_loss: 4.7876\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9925 - val_loss: 4.7886\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0293 - val_loss: 4.7890\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0013 - val_loss: 4.7927\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0438 - val_loss: 4.7936\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0201 - val_loss: 4.7870\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0126 - val_loss: 4.7888\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0364 - val_loss: 4.7899\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0238 - val_loss: 4.7990\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0264 - val_loss: 4.8209\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0177 - val_loss: 4.7897\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0224 - val_loss: 4.7907\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0272 - val_loss: 4.7867\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0103 - val_loss: 4.7863\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9926 - val_loss: 4.7901\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0365 - val_loss: 4.7907\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7880.1721 - val_loss: 15292.8555\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7511.3373 - val_loss: 15292.7656\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 7822.0073 - val_loss: 15292.7402\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7501.7370 - val_loss: 15292.7197\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7820.4763 - val_loss: 15292.6914\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7449.3872 - val_loss: 15292.6650\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7699.0735 - val_loss: 15292.6416\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7484.5686 - val_loss: 15292.6328\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7825.4119 - val_loss: 15292.6211\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7557.5105 - val_loss: 15292.6113\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7727.7846 - val_loss: 15292.5967\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7404.8485 - val_loss: 15292.5947\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7435.7787 - val_loss: 15292.5898\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7769.8157 - val_loss: 15292.5811\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7828.2402 - val_loss: 15292.5723\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7505.5798 - val_loss: 15292.5742\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7972.2534 - val_loss: 15292.5762\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7741.3967 - val_loss: 15292.5654\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7857.0997 - val_loss: 15292.5703\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7674.1965 - val_loss: 15292.5635\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7671.6452 - val_loss: 15292.5615\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7791.2002 - val_loss: 15292.5615\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8012.5876 - val_loss: 15292.5576\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7930.2364 - val_loss: 15292.5566\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7929.3431 - val_loss: 15292.5527\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7939.2729 - val_loss: 15292.5518\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7291.1695 - val_loss: 15292.5566\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7377.6086 - val_loss: 15292.5469\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7819.9285 - val_loss: 15292.5527\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7819.9528 - val_loss: 15292.5488\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7758.9838 - val_loss: 15292.5449\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7804.9423 - val_loss: 15292.5430\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7630.7136 - val_loss: 15292.5391\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7388.8053 - val_loss: 15292.5430\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7578.8664 - val_loss: 15292.5488\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7898.0767 - val_loss: 15292.5381\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7984.2852 - val_loss: 15292.5381\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7765.3470 - val_loss: 15292.5381\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7834.6959 - val_loss: 15292.5381\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7558.7914 - val_loss: 15292.5381\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7674.2694 - val_loss: 15292.5361\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7759.2785 - val_loss: 15292.5361\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7167.8472 - val_loss: 15292.5352\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7279.5961 - val_loss: 15292.5361\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7634.6183 - val_loss: 15292.5361\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8157.7206 - val_loss: 15292.5332\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7614.7983 - val_loss: 15292.5342\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7823.3037 - val_loss: 15292.5332\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7676.0380 - val_loss: 15292.5332\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7666.6097 - val_loss: 15292.5312\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 4.2991 - val_loss: 4.9276\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1222 - val_loss: 4.9041\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1135 - val_loss: 4.8807\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1009 - val_loss: 4.8682\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0994 - val_loss: 4.8544\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0677 - val_loss: 4.8429\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0493 - val_loss: 4.8377\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0370 - val_loss: 4.8233\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0370 - val_loss: 4.8174\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0443 - val_loss: 4.8093\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0399 - val_loss: 4.8058\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0438 - val_loss: 4.8021\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0038 - val_loss: 4.7981\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0175 - val_loss: 4.7947\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9962 - val_loss: 4.7871\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0110 - val_loss: 4.7934\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0505 - val_loss: 4.7857\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0214 - val_loss: 4.7806\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9975 - val_loss: 4.7771\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0146 - val_loss: 4.7991\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9694 - val_loss: 4.7709\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0238 - val_loss: 4.7804\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0103 - val_loss: 4.7668\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9885 - val_loss: 4.7664\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9792 - val_loss: 4.7636\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9782 - val_loss: 4.7615\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0041 - val_loss: 4.7639\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9848 - val_loss: 4.7892\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9615 - val_loss: 4.7680\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9712 - val_loss: 4.7594\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9697 - val_loss: 4.7551\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9714 - val_loss: 4.7540\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9756 - val_loss: 4.7538\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9875 - val_loss: 4.7523\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0042 - val_loss: 4.7525\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9938 - val_loss: 4.7508\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9677 - val_loss: 4.7507\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9654 - val_loss: 4.7585\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9695 - val_loss: 4.7508\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0187 - val_loss: 4.7477\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9942 - val_loss: 4.7495\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9887 - val_loss: 4.7466\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9641 - val_loss: 4.7445\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9763 - val_loss: 4.7473\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9777 - val_loss: 4.7442\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9662 - val_loss: 4.7473\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9749 - val_loss: 4.7479\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9955 - val_loss: 4.7447\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9806 - val_loss: 4.7444\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9404 - val_loss: 4.7439\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7469.8273 - val_loss: 15292.9668\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7795.6588 - val_loss: 15292.8945\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7470.8119 - val_loss: 15292.8574\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7871.8803 - val_loss: 15292.8164\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7460.3062 - val_loss: 15292.7705\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7786.0672 - val_loss: 15292.7520\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7529.9151 - val_loss: 15292.7168\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7589.5198 - val_loss: 15292.7031\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7472.7450 - val_loss: 15292.6855\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7942.2809 - val_loss: 15292.6738\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7650.1655 - val_loss: 15292.6650\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7467.5200 - val_loss: 15292.6602\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7509.6366 - val_loss: 15292.6396\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7341.7542 - val_loss: 15292.6240\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7930.1493 - val_loss: 15292.6367\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7611.5836 - val_loss: 15292.6172\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7665.6825 - val_loss: 15292.6074\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7479.3755 - val_loss: 15292.5967\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7613.8818 - val_loss: 15292.5938\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7703.0672 - val_loss: 15292.5859\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7734.9521 - val_loss: 15292.5967\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7549.1811 - val_loss: 15292.5811\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7583.1874 - val_loss: 15292.5830\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7842.6756 - val_loss: 15292.5664\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7145.6346 - val_loss: 15292.5654\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8030.7411 - val_loss: 15292.5635\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7778.5017 - val_loss: 15292.5713\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7654.1692 - val_loss: 15292.5576\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7624.4673 - val_loss: 15292.5498\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7995.5317 - val_loss: 15292.5645\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7497.9749 - val_loss: 15292.5439\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7737.7594 - val_loss: 15292.5439\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7861.6262 - val_loss: 15292.5430\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7418.4388 - val_loss: 15292.5400\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7611.2642 - val_loss: 15292.5488\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7642.5433 - val_loss: 15292.5488\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7804.0676 - val_loss: 15292.5391\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7581.5379 - val_loss: 15292.5312\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7581.7057 - val_loss: 15292.5293\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7536.8936 - val_loss: 15292.5283\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7776.5612 - val_loss: 15292.5283\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7492.5024 - val_loss: 15292.5244\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7294.4734 - val_loss: 15292.5371\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7713.8403 - val_loss: 15292.5264\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8101.8168 - val_loss: 15292.5215\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7699.1795 - val_loss: 15292.5195\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7811.1813 - val_loss: 15292.5166\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7384.5636 - val_loss: 15292.5166\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7169.7117 - val_loss: 15292.5186\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7663.5585 - val_loss: 15292.5244\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: 30.6170\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 25.6274 - val_loss: 25.3838\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 21.6467 - val_loss: 22.4285\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 19.3500 - val_loss: 20.3945\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 17.5532 - val_loss: 18.8710\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 16.2170 - val_loss: 17.6687\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 15.2875 - val_loss: 16.6945\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 14.3227 - val_loss: 15.8846\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13.6380 - val_loss: 15.2037\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13.0821 - val_loss: 14.6186\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: 28495.5488\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 14495.8600 - val_loss: 26012.7891\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 12876.1212 - val_loss: 24580.2988\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 12534.3397 - val_loss: 23588.0742\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 12032.4132 - val_loss: 22843.0566\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11687.7120 - val_loss: 22257.5059\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11475.8052 - val_loss: 21781.7969\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 10583.6271 - val_loss: 21387.7578\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11309.0859 - val_loss: 21057.2734\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 10546.7570 - val_loss: 20774.3281\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 26.4492 - val_loss: 4.3585\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7.2059 - val_loss: 5.2529\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7.6415 - val_loss: 4.6738\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 6.7686 - val_loss: 3.0603\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.8455 - val_loss: 2.8762\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.0905 - val_loss: 4.9936\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.2713 - val_loss: 3.0301\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.4333 - val_loss: 2.9212\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.1578 - val_loss: 7.8411\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7.0284 - val_loss: 3.1543\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.8664 - val_loss: 3.0595\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.6642 - val_loss: 19.9178\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.6776 - val_loss: 12.3098\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7.3419 - val_loss: 3.0849\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.7967 - val_loss: 3.2005\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.4656 - val_loss: 2.5278\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1526 - val_loss: 2.5311\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1432 - val_loss: 2.4911\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1285 - val_loss: 2.5588\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1151 - val_loss: 2.5130\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1108 - val_loss: 2.5139\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1143 - val_loss: 2.4854\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1207 - val_loss: 2.5043\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1086 - val_loss: 2.5054\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1096 - val_loss: 2.4861\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1002 - val_loss: 2.5067\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1243 - val_loss: 2.4765\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1128 - val_loss: 2.5292\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1028 - val_loss: 3.2899\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1193 - val_loss: 2.4850\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1011 - val_loss: 2.5405\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1083 - val_loss: 2.5658\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1058 - val_loss: 2.4736\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1197 - val_loss: 2.4994\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1255 - val_loss: 2.4810\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1006 - val_loss: 2.5085\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1094 - val_loss: 2.4904\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0922 - val_loss: 2.5099\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0910 - val_loss: 2.4865\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1029 - val_loss: 2.5016\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0893 - val_loss: 2.4933\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1124 - val_loss: 2.5717\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1012 - val_loss: 2.5601\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0713 - val_loss: 2.4600\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0577 - val_loss: 2.4643\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0602 - val_loss: 2.4677\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0612 - val_loss: 2.4645\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0627 - val_loss: 2.4696\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0611 - val_loss: 2.4590\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0559 - val_loss: 2.4577\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 1550.7391 - val_loss: 1985.1315\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 943.4059 - val_loss: 2322.5762\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 933.2728 - val_loss: 2056.8633\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 903.8080 - val_loss: 1828.5607\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 875.1274 - val_loss: 1783.1134\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 891.7130 - val_loss: 2122.4111\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 886.1860 - val_loss: 2131.2046\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 897.9717 - val_loss: 1954.7649\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 837.9235 - val_loss: 1712.1512\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 892.8585 - val_loss: 1895.8445\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 894.9906 - val_loss: 2041.4817\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 907.9356 - val_loss: 1749.1558\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 900.5357 - val_loss: 2157.0342\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 1000.5848 - val_loss: 1945.7920\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 852.0496 - val_loss: 1898.8027\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 873.1585 - val_loss: 1800.3018\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 910.1487 - val_loss: 1911.7681\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 872.9647 - val_loss: 1820.9012\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.5585 - val_loss: 1895.4060\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.6460 - val_loss: 1845.1433\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 892.4427 - val_loss: 1853.3899\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 837.6985 - val_loss: 1836.4091\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.5790 - val_loss: 1862.8438\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 883.2995 - val_loss: 1868.1941\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 874.8548 - val_loss: 1841.4125\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 834.2694 - val_loss: 1847.5588\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 874.6123 - val_loss: 1878.9148\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 870.0486 - val_loss: 1847.2559\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 880.8037 - val_loss: 1856.6249\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 858.8918 - val_loss: 1851.3342\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 835.8626 - val_loss: 1847.0818\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 883.9911 - val_loss: 1846.0209\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 830.6929 - val_loss: 1843.2222\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.3079 - val_loss: 1842.7360\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 859.8781 - val_loss: 1843.7859\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 873.8244 - val_loss: 1843.8627\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 915.5477 - val_loss: 1845.1738\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 863.5399 - val_loss: 1844.4869\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 914.7681 - val_loss: 1843.6851\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 843.6278 - val_loss: 1843.7166\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 865.5110 - val_loss: 1844.0286\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 853.1781 - val_loss: 1843.9796\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 859.8576 - val_loss: 1843.9119\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 910.1833 - val_loss: 1844.1553\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 822.5975 - val_loss: 1844.1368\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 890.9338 - val_loss: 1843.9978\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 869.9302 - val_loss: 1843.9568\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 879.1857 - val_loss: 1844.0519\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 889.5364 - val_loss: 1844.0210\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 830.3269 - val_loss: 1844.0255\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 3.8542 - val_loss: 2.5811\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1535 - val_loss: 2.5363\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1159 - val_loss: 2.5008\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1049 - val_loss: 2.4970\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1031 - val_loss: 2.4970\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1092 - val_loss: 2.4918\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0865 - val_loss: 2.4760\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0955 - val_loss: 2.5271\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0889 - val_loss: 2.5069\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0861 - val_loss: 2.5080\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0821 - val_loss: 2.4776\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0926 - val_loss: 2.5264\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0814 - val_loss: 2.5205\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0877 - val_loss: 2.5763\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0895 - val_loss: 2.4911\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0935 - val_loss: 2.4959\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1006 - val_loss: 2.4827\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0639 - val_loss: 2.4754\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0597 - val_loss: 2.4700\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0587 - val_loss: 2.4658\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0567 - val_loss: 2.4698\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0531 - val_loss: 2.4628\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0628 - val_loss: 2.4665\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0519 - val_loss: 2.4719\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0675 - val_loss: 2.4651\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0562 - val_loss: 2.4629\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0624 - val_loss: 2.4675\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0595 - val_loss: 2.4662\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0544 - val_loss: 2.4655\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0574 - val_loss: 2.4636\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0592 - val_loss: 2.4625\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0545 - val_loss: 2.4647\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0552 - val_loss: 2.4642\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0506 - val_loss: 2.4577\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0577 - val_loss: 2.4728\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0584 - val_loss: 2.4624\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0572 - val_loss: 2.4634\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0537 - val_loss: 2.4648\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0572 - val_loss: 2.4648\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0566 - val_loss: 2.4640\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0640 - val_loss: 2.4663\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0540 - val_loss: 2.4625\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0568 - val_loss: 2.4662\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0574 - val_loss: 2.4596\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0481 - val_loss: 2.4629\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0562 - val_loss: 2.4643\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0526 - val_loss: 2.4641\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0502 - val_loss: 2.4614\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0585 - val_loss: 2.4633\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0521 - val_loss: 2.4617\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 2706.0838 - val_loss: 2380.1592\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 1006.6333 - val_loss: 1986.0786\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 948.9850 - val_loss: 1977.8156\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 971.6287 - val_loss: 2023.7725\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 948.7940 - val_loss: 2020.2411\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 923.0551 - val_loss: 1914.6454\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 875.6685 - val_loss: 1859.4203\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 866.4867 - val_loss: 1906.8762\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 844.0318 - val_loss: 1907.3324\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 852.4875 - val_loss: 1940.4976\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 879.2111 - val_loss: 1913.8528\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 862.3360 - val_loss: 1832.1442\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 835.1611 - val_loss: 1882.2780\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 892.8924 - val_loss: 1994.5096\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 879.7940 - val_loss: 1960.8110\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 859.0290 - val_loss: 1834.7115\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 842.2656 - val_loss: 1857.8665\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 854.4836 - val_loss: 1853.1462\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 888.1523 - val_loss: 1865.6766\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 857.6221 - val_loss: 1897.5009\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 839.9849 - val_loss: 1976.1748\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 879.8180 - val_loss: 1990.4648\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 867.0370 - val_loss: 1856.4271\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 902.1922 - val_loss: 1858.2174\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 861.9375 - val_loss: 1847.0240\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 871.2922 - val_loss: 1835.3280\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 851.9926 - val_loss: 1837.9932\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 858.4943 - val_loss: 1855.6460\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 849.5172 - val_loss: 1844.2883\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 831.2787 - val_loss: 1845.2184\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 886.4648 - val_loss: 1859.4476\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.8836 - val_loss: 1836.1022\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 877.0075 - val_loss: 1838.5013\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 872.8626 - val_loss: 1838.4269\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 812.4622 - val_loss: 1839.0389\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 887.3536 - val_loss: 1840.4883\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 971.9969 - val_loss: 1841.5277\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 913.5058 - val_loss: 1842.0499\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 836.7803 - val_loss: 1841.2671\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 849.1968 - val_loss: 1840.3239\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 910.4394 - val_loss: 1841.3771\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 842.0728 - val_loss: 1841.9673\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 870.1089 - val_loss: 1842.0194\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 920.9549 - val_loss: 1841.8810\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 835.4056 - val_loss: 1841.7678\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 862.8503 - val_loss: 1841.6422\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 865.8207 - val_loss: 1841.5522\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 833.2631 - val_loss: 1841.4509\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 834.8708 - val_loss: 1841.4789\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 871.7186 - val_loss: 1841.5455\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 3.9883 - val_loss: 4.7006\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9535 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9246 - val_loss: 4.7007\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9370 - val_loss: 4.7006\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9387 - val_loss: 4.7013\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9123 - val_loss: 4.7007\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9193 - val_loss: 4.7007\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9371 - val_loss: 4.7007\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9176 - val_loss: 4.7011\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9200 - val_loss: 4.7006\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9280 - val_loss: 4.7007\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9130 - val_loss: 4.7008\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9246 - val_loss: 4.7010\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9245 - val_loss: 4.7012\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9363 - val_loss: 4.7009\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9002 - val_loss: 4.7009\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9059 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9424 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9086 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9458 - val_loss: 4.7011\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9194 - val_loss: 4.7013\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9149 - val_loss: 4.7011\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9067 - val_loss: 4.7011\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9256 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9389 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9651 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9282 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9373 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9397 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9085 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9414 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9197 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9158 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9091 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9544 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9347 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9437 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9314 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9406 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9306 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9720 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9126 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9188 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.8967 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9204 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9283 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9377 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9344 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9459 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9401 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7736.0889 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7504.3450 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7836.0208 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7679.7532 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7619.5719 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7510.6018 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7572.0755 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7425.3376 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7867.2570 - val_loss: 15292.4346\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 7539.7258 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7554.2966 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7345.2409 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7382.8194 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7709.2871 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7942.1743 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7594.4974 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7578.9826 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7559.2329 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7560.7529 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7688.0584 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7659.9179 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7545.3416 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7903.2840 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7642.4495 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7384.3498 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7486.5100 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7772.0015 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7894.5744 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7644.4342 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7355.1970 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7497.4248 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7823.3683 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7309.0920 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7402.4422 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7841.3656 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7697.3314 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7378.5156 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7527.7868 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7550.6716 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7721.1254 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7816.2278 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7759.0989 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7407.1124 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7391.7906 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8056.7631 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7287.0210 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7717.6370 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7471.3147 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7543.7613 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7679.8788 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 4.0530 - val_loss: 4.7011\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9417 - val_loss: 4.7008\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9388 - val_loss: 4.7010\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9297 - val_loss: 4.7013\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9111 - val_loss: 4.7009\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9278 - val_loss: 4.7014\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9257 - val_loss: 4.7009\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9043 - val_loss: 4.7010\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9296 - val_loss: 4.7007\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9361 - val_loss: 4.7010\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9191 - val_loss: 4.7008\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9408 - val_loss: 4.7008\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9137 - val_loss: 4.7009\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9333 - val_loss: 4.7010\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9345 - val_loss: 4.7010\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9280 - val_loss: 4.7010\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9438 - val_loss: 4.7011\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9209 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9488 - val_loss: 4.7011\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9202 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9335 - val_loss: 4.7010\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9294 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9255 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9433 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9106 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9440 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9126 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9382 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9264 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9263 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9064 - val_loss: 4.7010\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9343 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9386 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9160 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9222 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9365 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9470 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9152 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9360 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9556 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9481 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9174 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9041 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9311 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8937 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9412 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9230 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9274 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9105 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9288 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 7925.6198 - val_loss: 15292.4434\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7614.7367 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7852.9083 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7886.1150 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7897.1566 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7528.3496 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7567.2198 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7955.4912 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7587.4556 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7743.2541 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7960.0233 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7702.0635 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7547.9258 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7668.4439 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7535.0948 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7872.9343 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7594.9236 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7638.4569 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7764.2774 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7735.6973 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7562.6329 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7571.4262 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7797.0352 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7521.5822 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7791.3199 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7942.4724 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7322.8566 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8094.2338 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7983.9289 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7896.3096 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7386.2577 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7420.8896 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7937.8450 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7850.1981 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7523.5707 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7896.3941 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7836.2072 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7618.8603 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7906.4183 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8031.0059 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7717.1112 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7643.5707 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7456.6164 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7737.8680 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7817.9226 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7578.4505 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7234.5223 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7663.8344 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7657.5250 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7780.5460 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 23.8912 - val_loss: 50.7319\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 16.0443 - val_loss: 3.4230\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 14.6498 - val_loss: 4.2999\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.5384 - val_loss: 3.6579\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.7841 - val_loss: 13.8170\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 13.5257 - val_loss: 7.2291\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 8.0991 - val_loss: 5.3896\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8.7805 - val_loss: 49.1229\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 8.0925 - val_loss: 9.3688\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 12.1871 - val_loss: 4.9753\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 5.3705 - val_loss: 4.3656\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.9826 - val_loss: 57.0170\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 9.9240 - val_loss: 2.9248\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.0566 - val_loss: 2.8780\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6853 - val_loss: 2.7998\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5265 - val_loss: 2.7834\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.1252 - val_loss: 2.7104\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5401 - val_loss: 2.6914\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4297 - val_loss: 2.6677\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4220 - val_loss: 2.6356\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6269 - val_loss: 2.6240\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3058 - val_loss: 2.6416\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2970 - val_loss: 2.6281\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3431 - val_loss: 2.6270\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.4080 - val_loss: 2.6231\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3699 - val_loss: 2.6093\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5129 - val_loss: 2.6031\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3438 - val_loss: 2.5845\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2799 - val_loss: 2.5912\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2892 - val_loss: 2.6136\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2144 - val_loss: 2.6210\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2010 - val_loss: 2.5955\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1819 - val_loss: 2.5664\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2026 - val_loss: 2.5827\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1843 - val_loss: 2.5809\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2011 - val_loss: 2.5798\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1658 - val_loss: 2.5653\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3031 - val_loss: 2.5696\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3147 - val_loss: 2.5601\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2516 - val_loss: 2.5689\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4382 - val_loss: 2.5759\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4213 - val_loss: 2.5712\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5186 - val_loss: 2.5718\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2522 - val_loss: 2.5814\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2027 - val_loss: 2.5707\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.2198 - val_loss: 2.5649\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1700 - val_loss: 2.5675\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3065 - val_loss: 2.5829\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.3012 - val_loss: 2.5656\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1531 - val_loss: 2.5626\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 1700.6192 - val_loss: 1803.5114\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 933.0554 - val_loss: 2048.2017\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 920.6652 - val_loss: 1816.1564\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 926.8190 - val_loss: 1907.2542\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 947.6891 - val_loss: 1933.4410\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 895.4403 - val_loss: 2099.3418\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 896.6351 - val_loss: 1829.1595\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 899.3554 - val_loss: 1773.1880\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 928.7566 - val_loss: 1778.0907\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 905.7014 - val_loss: 1866.6368\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 947.5553 - val_loss: 1817.7278\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 953.3515 - val_loss: 1919.4891\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 945.5426 - val_loss: 1727.0100\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 883.3118 - val_loss: 2240.6968\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 949.1657 - val_loss: 1962.5410\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 868.1782 - val_loss: 1845.2709\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 915.7906 - val_loss: 2029.3757\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 928.4878 - val_loss: 1894.4039\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 876.9311 - val_loss: 1812.2214\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 885.3474 - val_loss: 1899.7689\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 891.8196 - val_loss: 1795.7350\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 940.1830 - val_loss: 2135.1936\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 877.5082 - val_loss: 1781.2469\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 874.9367 - val_loss: 1804.5942\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 840.7430 - val_loss: 1811.5762\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 809.0981 - val_loss: 1823.5021\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 894.0836 - val_loss: 1830.4572\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 837.5760 - val_loss: 1825.9333\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 854.4299 - val_loss: 1825.3824\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 858.8248 - val_loss: 1799.1160\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 858.3448 - val_loss: 1841.6804\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 814.8421 - val_loss: 1842.8141\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 839.0265 - val_loss: 1797.9659\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 830.8133 - val_loss: 1814.2208\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 859.8040 - val_loss: 1823.2621\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 841.6122 - val_loss: 1825.7953\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 801.1172 - val_loss: 1827.1969\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 837.2113 - val_loss: 1829.1635\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 838.8832 - val_loss: 1831.4762\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 892.5182 - val_loss: 1834.8752\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 836.6494 - val_loss: 1833.5288\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 879.4199 - val_loss: 1833.2834\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 883.2211 - val_loss: 1836.2341\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 849.9534 - val_loss: 1836.3052\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 807.3776 - val_loss: 1836.1825\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 863.6608 - val_loss: 1836.1135\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 832.4720 - val_loss: 1835.8334\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 854.3232 - val_loss: 1835.9904\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 845.1222 - val_loss: 1835.7340\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 848.8959 - val_loss: 1835.4961\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 3.8853 - val_loss: 2.6637\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2099 - val_loss: 2.5388\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1669 - val_loss: 2.5601\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1649 - val_loss: 2.5580\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1569 - val_loss: 2.5575\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1465 - val_loss: 2.5524\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1364 - val_loss: 2.5670\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.1467 - val_loss: 2.5440\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1378 - val_loss: 2.5446\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1285 - val_loss: 2.5376\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1506 - val_loss: 2.5179\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1285 - val_loss: 2.5229\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1263 - val_loss: 2.5224\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1227 - val_loss: 2.5424\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1341 - val_loss: 2.5193\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1138 - val_loss: 2.4786\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.0985 - val_loss: 2.5045\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0929 - val_loss: 2.5604\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1166 - val_loss: 2.4755\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0909 - val_loss: 2.4738\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.1030 - val_loss: 2.5464\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0962 - val_loss: 2.4995\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0912 - val_loss: 2.4908\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.1050 - val_loss: 2.5181\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1144 - val_loss: 2.5045\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0978 - val_loss: 2.5086\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1090 - val_loss: 2.5900\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0953 - val_loss: 2.5805\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0953 - val_loss: 2.5122\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1241 - val_loss: 2.4753\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0631 - val_loss: 2.4690\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0591 - val_loss: 2.4683\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0591 - val_loss: 2.4636\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0597 - val_loss: 2.4648\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0668 - val_loss: 2.4683\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0540 - val_loss: 2.4702\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0509 - val_loss: 2.4671\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0509 - val_loss: 2.4633\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0502 - val_loss: 2.4573\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0558 - val_loss: 2.4643\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0548 - val_loss: 2.4689\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0531 - val_loss: 2.4602\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0486 - val_loss: 2.4622\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0494 - val_loss: 2.4675\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0489 - val_loss: 2.4627\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0511 - val_loss: 2.4680\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0455 - val_loss: 2.4683\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0472 - val_loss: 2.4578\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0402 - val_loss: 2.4584\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0495 - val_loss: 2.4607\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2798.0033 - val_loss: 2295.4141\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 1102.8462 - val_loss: 2004.1547\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 926.3287 - val_loss: 1974.4962\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 866.7395 - val_loss: 1831.4257\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 943.3513 - val_loss: 1953.0502\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 867.9485 - val_loss: 1885.2061\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 880.3283 - val_loss: 1809.2981\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 887.4200 - val_loss: 1837.1414\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 937.8769 - val_loss: 1904.7236\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 843.9255 - val_loss: 1786.8873\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 870.4685 - val_loss: 1904.1931\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 840.4636 - val_loss: 1842.9801\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 821.4958 - val_loss: 1852.0641\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 826.4915 - val_loss: 1738.8300\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 872.1937 - val_loss: 1959.3364\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 859.0009 - val_loss: 1843.2887\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 828.6938 - val_loss: 1765.7301\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 883.7439 - val_loss: 1800.1665\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 878.3050 - val_loss: 1719.9268\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 832.3114 - val_loss: 1973.2175\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 887.3801 - val_loss: 1831.0326\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 874.6583 - val_loss: 1851.5032\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 878.1320 - val_loss: 1821.5513\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 905.0358 - val_loss: 1781.8937\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 869.1362 - val_loss: 1900.8590\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 879.6048 - val_loss: 1802.9742\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 845.8628 - val_loss: 1828.2756\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 902.7571 - val_loss: 1817.9380\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 838.6233 - val_loss: 1757.3685\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 869.7943 - val_loss: 1809.9357\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 920.5594 - val_loss: 1847.3240\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 851.2648 - val_loss: 1832.6729\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 848.5806 - val_loss: 1850.0194\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 798.8695 - val_loss: 1837.3850\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 863.7296 - val_loss: 1850.7206\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 881.4888 - val_loss: 1844.1884\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 862.1185 - val_loss: 1871.3254\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 886.6497 - val_loss: 1855.2987\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 868.5907 - val_loss: 1833.6030\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 851.7519 - val_loss: 1836.7129\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 872.0318 - val_loss: 1840.0128\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 832.1870 - val_loss: 1840.6857\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 844.2089 - val_loss: 1841.4099\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 871.4806 - val_loss: 1842.9292\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 855.5438 - val_loss: 1843.3043\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 814.2037 - val_loss: 1843.1351\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 834.8287 - val_loss: 1843.0046\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 845.7428 - val_loss: 1843.7382\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 863.5915 - val_loss: 1842.8691\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 868.6914 - val_loss: 1842.8359\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 4.2509 - val_loss: 4.9363\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.1814 - val_loss: 4.9322\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1729 - val_loss: 4.8751\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1217 - val_loss: 4.8502\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0579 - val_loss: 4.8324\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0655 - val_loss: 4.8261\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0555 - val_loss: 4.8249\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0515 - val_loss: 4.8100\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0481 - val_loss: 4.8082\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0440 - val_loss: 4.8050\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0250 - val_loss: 4.8051\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0350 - val_loss: 4.8053\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0235 - val_loss: 4.7949\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0375 - val_loss: 4.8007\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0446 - val_loss: 4.8056\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0314 - val_loss: 4.7995\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0423 - val_loss: 4.8050\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0223 - val_loss: 4.7960\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0122 - val_loss: 4.7936\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0506 - val_loss: 4.7970\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0006 - val_loss: 4.7965\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0272 - val_loss: 4.7927\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0405 - val_loss: 4.7933\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0253 - val_loss: 4.7946\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0265 - val_loss: 4.8095\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0189 - val_loss: 4.7912\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0060 - val_loss: 4.7920\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0380 - val_loss: 4.7890\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0099 - val_loss: 4.7927\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0252 - val_loss: 4.7917\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0229 - val_loss: 4.7900\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0366 - val_loss: 4.7925\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0167 - val_loss: 4.7994\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0209 - val_loss: 4.7889\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0195 - val_loss: 4.7882\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0207 - val_loss: 4.7999\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0073 - val_loss: 4.7879\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9992 - val_loss: 4.7874\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0190 - val_loss: 4.7882\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0337 - val_loss: 4.7873\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0250 - val_loss: 4.7893\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9951 - val_loss: 4.7916\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0162 - val_loss: 4.7913\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0341 - val_loss: 4.7900\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0169 - val_loss: 4.7876\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0189 - val_loss: 4.7866\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0368 - val_loss: 4.7891\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0469 - val_loss: 4.7894\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0225 - val_loss: 4.7875\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9970 - val_loss: 4.7870\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7326.1100 - val_loss: 15292.7949\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7432.6742 - val_loss: 15292.7510\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7466.6359 - val_loss: 15292.7383\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7851.1996 - val_loss: 15292.7090\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7606.1836 - val_loss: 15292.7012\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7715.0989 - val_loss: 15292.6719\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7830.3612 - val_loss: 15292.6543\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7435.2926 - val_loss: 15292.6377\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7547.7924 - val_loss: 15292.6201\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7603.7534 - val_loss: 15292.6152\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7539.6173 - val_loss: 15292.6074\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 7756.5046 - val_loss: 15292.6133\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7494.1890 - val_loss: 15292.6045\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7650.4389 - val_loss: 15292.5918\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 8288.4922 - val_loss: 15292.5781\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7501.9366 - val_loss: 15292.5781\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7587.6885 - val_loss: 15292.5801\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7654.6434 - val_loss: 15292.5781\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7326.2051 - val_loss: 15292.5752\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7655.1372 - val_loss: 15292.5645\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7591.7280 - val_loss: 15292.5635\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7681.8380 - val_loss: 15292.5635\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7394.5603 - val_loss: 15292.5615\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7818.2683 - val_loss: 15292.5615\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7742.6391 - val_loss: 15292.5615\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7497.0752 - val_loss: 15292.5566\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7451.1223 - val_loss: 15292.5566\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7493.5485 - val_loss: 15292.5566\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7854.0840 - val_loss: 15292.5439\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7944.4953 - val_loss: 15292.5488\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7403.2674 - val_loss: 15292.5469\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7650.4912 - val_loss: 15292.5488\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7496.6856 - val_loss: 15292.5498\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7549.5111 - val_loss: 15292.5430\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7603.6847 - val_loss: 15292.5430\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7972.4163 - val_loss: 15292.5469\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7942.0007 - val_loss: 15292.5381\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7624.2035 - val_loss: 15292.5400\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7655.7691 - val_loss: 15292.5400\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7537.3511 - val_loss: 15292.5400\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7457.8250 - val_loss: 15292.5391\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7628.2919 - val_loss: 15292.5381\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7315.7235 - val_loss: 15292.5361\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7883.0335 - val_loss: 15292.5391\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7416.7709 - val_loss: 15292.5361\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7875.2777 - val_loss: 15292.5352\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7585.6328 - val_loss: 15292.5332\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8007.7760 - val_loss: 15292.5352\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7398.7587 - val_loss: 15292.5352\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7752.3191 - val_loss: 15292.5332\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.3083 - val_loss: 4.9383\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1130 - val_loss: 4.9021\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1236 - val_loss: 4.8794\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0905 - val_loss: 4.8650\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1120 - val_loss: 4.8500\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0929 - val_loss: 4.8396\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0648 - val_loss: 4.8306\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0293 - val_loss: 4.8257\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0150 - val_loss: 4.8176\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0381 - val_loss: 4.8133\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0493 - val_loss: 4.8046\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0181 - val_loss: 4.7986\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0055 - val_loss: 4.7942\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9880 - val_loss: 4.8144\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0049 - val_loss: 4.7879\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0074 - val_loss: 4.7828\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9938 - val_loss: 4.7821\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0202 - val_loss: 4.7866\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0029 - val_loss: 4.7756\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9959 - val_loss: 4.7795\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9915 - val_loss: 4.7752\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0011 - val_loss: 4.7691\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9635 - val_loss: 4.7680\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9831 - val_loss: 4.7719\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9925 - val_loss: 4.7689\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9802 - val_loss: 4.7615\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9677 - val_loss: 4.7614\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9762 - val_loss: 4.7613\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0169 - val_loss: 4.7649\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0075 - val_loss: 4.7585\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9855 - val_loss: 4.7570\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9948 - val_loss: 4.7638\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9854 - val_loss: 4.7784\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9777 - val_loss: 4.7531\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0015 - val_loss: 4.7522\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9779 - val_loss: 4.7606\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9793 - val_loss: 4.7503\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0055 - val_loss: 4.7498\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9864 - val_loss: 4.7504\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9995 - val_loss: 4.7488\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9687 - val_loss: 4.7487\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9788 - val_loss: 4.7459\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9844 - val_loss: 4.7466\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9529 - val_loss: 4.7470\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9899 - val_loss: 4.7450\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9939 - val_loss: 4.7448\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9584 - val_loss: 4.7440\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9689 - val_loss: 4.7487\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9474 - val_loss: 4.7420\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9700 - val_loss: 4.7440\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7663.8345 - val_loss: 15292.9844\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7955.3423 - val_loss: 15292.9102\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7421.3825 - val_loss: 15292.8438\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7589.7538 - val_loss: 15292.8018\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7356.6529 - val_loss: 15292.7686\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7628.7144 - val_loss: 15292.7529\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7216.5417 - val_loss: 15292.7256\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7377.4578 - val_loss: 15292.7109\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7838.3171 - val_loss: 15292.6816\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7379.0607 - val_loss: 15292.6719\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7482.9076 - val_loss: 15292.6533\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7753.8700 - val_loss: 15292.6562\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7575.9638 - val_loss: 15292.6445\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8041.1419 - val_loss: 15292.6455\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7863.4122 - val_loss: 15292.6260\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7658.2665 - val_loss: 15292.6094\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7459.9046 - val_loss: 15292.6035\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7604.3441 - val_loss: 15292.6123\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7788.6854 - val_loss: 15292.5967\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7783.5992 - val_loss: 15292.5869\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7614.5702 - val_loss: 15292.5840\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7621.2766 - val_loss: 15292.5801\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7603.1691 - val_loss: 15292.5723\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7507.7597 - val_loss: 15292.5830\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7293.8934 - val_loss: 15292.5674\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7763.1079 - val_loss: 15292.5635\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7996.3795 - val_loss: 15292.5625\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7930.4144 - val_loss: 15292.5586\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7336.4941 - val_loss: 15292.5498\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7194.1021 - val_loss: 15292.5498\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7839.2273 - val_loss: 15292.5459\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7686.3724 - val_loss: 15292.5410\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7957.3429 - val_loss: 15292.5410\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7867.4301 - val_loss: 15292.5449\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7510.7366 - val_loss: 15292.5449\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7490.9305 - val_loss: 15292.5488\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7399.2369 - val_loss: 15292.5352\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7771.6383 - val_loss: 15292.5352\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8010.8693 - val_loss: 15292.5332\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7627.0766 - val_loss: 15292.5264\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7799.8896 - val_loss: 15292.5273\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7436.7362 - val_loss: 15292.5273\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7615.4401 - val_loss: 15292.5225\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7681.3392 - val_loss: 15292.5215\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8182.7497 - val_loss: 15292.5215\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7737.5192 - val_loss: 15292.5215\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7657.3162 - val_loss: 15292.5166\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7637.7106 - val_loss: 15292.5312\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7420.8695 - val_loss: 15292.5176\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7621.8466 - val_loss: 15292.5166\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 28490.6562\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 14507.8188 - val_loss: 26010.3555\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 12371.9267 - val_loss: 24578.8750\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 12426.6920 - val_loss: 23586.6074\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 12091.2747 - val_loss: 22841.2520\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 12073.3039 - val_loss: 22254.5586\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 10952.2088 - val_loss: 21781.3965\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11248.5036 - val_loss: 21386.5859\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 10489.5669 - val_loss: 21056.7227\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 10090.4347 - val_loss: 20772.3848\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 21.4679 - val_loss: 4.6218\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.0236 - val_loss: 3.4362\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.1236 - val_loss: 3.5977\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.8865 - val_loss: 15.8704\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.2598 - val_loss: 2.8849\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.0461 - val_loss: 3.4866\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 5.9637 - val_loss: 25.0046\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.4528 - val_loss: 4.4066\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.7934 - val_loss: 4.3006\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.0238 - val_loss: 2.9004\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 8.6264 - val_loss: 3.8777\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.1584 - val_loss: 2.8686\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 6.4405 - val_loss: 3.1174\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0083 - val_loss: 4.1726\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.0453 - val_loss: 6.1943\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.7008 - val_loss: 3.0630\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.5514 - val_loss: 3.0329\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.6682 - val_loss: 2.9566\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.0633 - val_loss: 3.3074\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.6322 - val_loss: 35.3077\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.5683 - val_loss: 3.0722\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.8863 - val_loss: 3.6013\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.5721 - val_loss: 2.5679\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.2212 - val_loss: 2.5540\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1938 - val_loss: 2.5737\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1547 - val_loss: 3.8109\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.2144 - val_loss: 2.4840\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1558 - val_loss: 2.5133\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1731 - val_loss: 2.5072\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1484 - val_loss: 2.5022\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1511 - val_loss: 2.5500\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1527 - val_loss: 2.5216\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1461 - val_loss: 2.4944\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1323 - val_loss: 2.5019\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1412 - val_loss: 2.5121\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1404 - val_loss: 2.5692\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1371 - val_loss: 2.5726\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0854 - val_loss: 2.4808\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0787 - val_loss: 2.4742\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0663 - val_loss: 2.4714\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0700 - val_loss: 2.4754\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0610 - val_loss: 2.4685\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0650 - val_loss: 2.4763\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0605 - val_loss: 2.4636\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0713 - val_loss: 2.4740\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0592 - val_loss: 2.4632\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0636 - val_loss: 2.4718\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0624 - val_loss: 2.4717\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0651 - val_loss: 2.4638\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0591 - val_loss: 2.4604\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 1583.2633 - val_loss: 1935.0166\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 949.6489 - val_loss: 1876.7954\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 959.9306 - val_loss: 1921.9108\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 942.6866 - val_loss: 1870.2678\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 862.5250 - val_loss: 1989.1278\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 923.0127 - val_loss: 2011.3624\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 904.2866 - val_loss: 1855.0571\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 896.6178 - val_loss: 2001.9939\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 920.4296 - val_loss: 1790.2211\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 920.9619 - val_loss: 1791.3093\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 883.9911 - val_loss: 1769.4882\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 894.6303 - val_loss: 1749.9290\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 938.3043 - val_loss: 1742.5166\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 909.9884 - val_loss: 1823.7469\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 892.4529 - val_loss: 2012.3129\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 826.4887 - val_loss: 1764.7854\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 901.2550 - val_loss: 2060.8181\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 940.2400 - val_loss: 1780.0570\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 852.6281 - val_loss: 1745.3019\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 873.4989 - val_loss: 1826.1804\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 977.5110 - val_loss: 1838.1249\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 872.7522 - val_loss: 1818.4343\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 904.9778 - val_loss: 1760.5378\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 847.8346 - val_loss: 1830.9573\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.4920 - val_loss: 1826.5740\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 869.1984 - val_loss: 1820.0847\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 822.5447 - val_loss: 1847.2379\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 899.1309 - val_loss: 1855.7363\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 844.3506 - val_loss: 1824.7043\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.6408 - val_loss: 1830.1732\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 871.8139 - val_loss: 1826.1093\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 864.7099 - val_loss: 1872.0947\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.4671 - val_loss: 1817.0773\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.7931 - val_loss: 1827.7365\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 805.0748 - val_loss: 1829.9351\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 834.6660 - val_loss: 1833.4856\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 838.3424 - val_loss: 1834.5737\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 879.4140 - val_loss: 1834.6934\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 880.1679 - val_loss: 1836.1141\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 850.7927 - val_loss: 1837.1873\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 855.4053 - val_loss: 1838.1315\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 846.4074 - val_loss: 1837.0437\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 887.8135 - val_loss: 1838.6177\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 793.4404 - val_loss: 1838.5178\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 825.0136 - val_loss: 1838.5271\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 831.2046 - val_loss: 1838.5056\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 845.9375 - val_loss: 1838.3446\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 837.8718 - val_loss: 1838.4001\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 800.7321 - val_loss: 1838.1680\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 876.6025 - val_loss: 1838.2421\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 3.8562 - val_loss: 2.5692\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1811 - val_loss: 2.5733\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1251 - val_loss: 2.5782\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1224 - val_loss: 2.5567\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1136 - val_loss: 2.4934\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1117 - val_loss: 2.5203\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1119 - val_loss: 2.5383\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 2.1000 - val_loss: 2.4886\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0971 - val_loss: 2.5137\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0870 - val_loss: 2.4791\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0911 - val_loss: 2.4834\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0887 - val_loss: 2.4839\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0932 - val_loss: 2.5109\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0964 - val_loss: 2.4802\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1001 - val_loss: 2.4888\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0838 - val_loss: 2.4923\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.1002 - val_loss: 2.4936\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: 2.1090 - val_loss: 2.5343\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: 2.0947 - val_loss: 2.4829\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0969 - val_loss: 2.5379\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 8s 37ms/step - loss: 2.0709 - val_loss: 2.4692\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0645 - val_loss: 2.4712\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2.0668 - val_loss: 2.4733\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0597 - val_loss: 2.4685\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0622 - val_loss: 2.4681\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0630 - val_loss: 2.4685\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0640 - val_loss: 2.4675\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2.0572 - val_loss: 2.4738\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0614 - val_loss: 2.4753\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 7s 33ms/step - loss: 2.0676 - val_loss: 2.4697\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2.0635 - val_loss: 2.4772\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2.0624 - val_loss: 2.4680\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0577 - val_loss: 2.4712\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0666 - val_loss: 2.4780\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0638 - val_loss: 2.4715\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0654 - val_loss: 2.4699\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 2.0640 - val_loss: 2.4685\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0579 - val_loss: 2.4720\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0676 - val_loss: 2.4715\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: 2.0581 - val_loss: 2.4703\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 7s 34ms/step - loss: 2.0593 - val_loss: 2.4703\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 7s 34ms/step - loss: 2.0581 - val_loss: 2.4693\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0579 - val_loss: 2.4687\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0612 - val_loss: 2.4702\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0585 - val_loss: 2.4685\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0593 - val_loss: 2.4699\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0626 - val_loss: 2.4715\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0643 - val_loss: 2.4708\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0606 - val_loss: 2.4705\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0563 - val_loss: 2.4703\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 31ms/step - loss: 2771.6478 - val_loss: 2440.7725\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 1028.1559 - val_loss: 2059.6130\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 902.9196 - val_loss: 1867.0026\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 6s 32ms/step - loss: 831.7961 - val_loss: 1824.2953\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 909.8426 - val_loss: 1968.8352\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 867.6045 - val_loss: 1914.5267\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 7s 35ms/step - loss: 834.9866 - val_loss: 1770.5344\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 7s 34ms/step - loss: 871.1657 - val_loss: 1943.4702\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 854.5746 - val_loss: 1966.8512\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 883.1452 - val_loss: 1840.6786\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 860.5076 - val_loss: 1903.7803\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: 828.7979 - val_loss: 1839.7942\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 874.3081 - val_loss: 1855.3754\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 878.8477 - val_loss: 1890.1957\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 849.6503 - val_loss: 1741.7965\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 871.9006 - val_loss: 1841.5023\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 850.6504 - val_loss: 1788.7592\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 878.6926 - val_loss: 1806.1726\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 887.0975 - val_loss: 1910.4966\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 904.8302 - val_loss: 1985.6412\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 888.3653 - val_loss: 1888.2201\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 838.9126 - val_loss: 1764.2410\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 884.5059 - val_loss: 1850.8668\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 864.8748 - val_loss: 1877.8568\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 861.3214 - val_loss: 1756.2715\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 798.5532 - val_loss: 1819.1090\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 905.0649 - val_loss: 1854.6245\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 832.7545 - val_loss: 1829.7069\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 867.9847 - val_loss: 1838.3538\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 853.8271 - val_loss: 1837.5582\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 877.9317 - val_loss: 1844.2734\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 875.6604 - val_loss: 1862.1561\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 913.1060 - val_loss: 1868.5605\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 830.4028 - val_loss: 1829.1407\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 923.9183 - val_loss: 1854.2870\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 935.8196 - val_loss: 1852.2593\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 832.4185 - val_loss: 1849.2444\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 846.0686 - val_loss: 1848.2455\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 877.5084 - val_loss: 1846.5682\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 801.1993 - val_loss: 1844.7770\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 866.8031 - val_loss: 1843.3759\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 849.5223 - val_loss: 1843.9037\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 821.2630 - val_loss: 1842.5522\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 840.2288 - val_loss: 1841.6832\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 885.7430 - val_loss: 1841.2084\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 6s 30ms/step - loss: 792.6123 - val_loss: 1841.2426\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 831.4174 - val_loss: 1841.2246\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 847.7558 - val_loss: 1841.2039\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 852.0012 - val_loss: 1841.2416\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 799.9114 - val_loss: 1841.3184\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 3.9805 - val_loss: 4.7006\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9324 - val_loss: 4.7015\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9199 - val_loss: 4.7007\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9221 - val_loss: 4.7007\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9200 - val_loss: 4.7018\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9249 - val_loss: 4.7007\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9394 - val_loss: 4.7007\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9440 - val_loss: 4.7014\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9251 - val_loss: 4.7007\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9291 - val_loss: 4.7006\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9631 - val_loss: 4.7008\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9029 - val_loss: 4.7009\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9410 - val_loss: 4.7010\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9236 - val_loss: 4.7011\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9097 - val_loss: 4.7010\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9229 - val_loss: 4.7008\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9409 - val_loss: 4.7009\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9586 - val_loss: 4.7011\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 3.9389 - val_loss: 4.7011\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9349 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9233 - val_loss: 4.7011\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9407 - val_loss: 4.7011\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9167 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9389 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9314 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9379 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9302 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9241 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9191 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9591 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9203 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 3.9206 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 7s 35ms/step - loss: 3.9041 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 7s 33ms/step - loss: 3.9136 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 7s 33ms/step - loss: 3.9407 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9087 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9855 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9630 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 3.9503 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 3.9169 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 3.9217 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9254 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9331 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9137 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 3.9304 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9335 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 3.9273 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 3.9195 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 3.9577 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 3.9247 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 9s 39ms/step - loss: 7484.5443 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 8s 38ms/step - loss: 7496.9050 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7642.4064 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7990.6585 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7627.8947 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7495.0977 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7722.7848 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7768.6070 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 7383.4088 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7540.3339 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7384.0048 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7350.9408 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7689.4380 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7388.6934 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7704.6522 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7193.7831 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7476.2808 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7732.2145 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7528.0269 - val_loss: 15292.4346\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 7646.8906 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7655.6197 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7408.1368 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8001.6008 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7180.8145 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8092.3327 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7363.5663 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7531.0439 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7798.1477 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7476.2929 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7420.3301 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7584.0115 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7672.4660 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7895.6158 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7618.1618 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7380.8970 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7606.6963 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7721.0675 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7528.9011 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7460.0704 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7508.3788 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7716.9803 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7594.5688 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7643.6790 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7733.6862 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7572.8895 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7617.2397 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7744.0930 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7677.1510 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7710.6168 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7685.1992 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.0400 - val_loss: 4.7010\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9300 - val_loss: 4.7013\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9337 - val_loss: 4.7011\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9191 - val_loss: 4.7009\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9347 - val_loss: 4.7007\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9181 - val_loss: 4.7011\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9393 - val_loss: 4.7008\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9377 - val_loss: 4.7009\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9151 - val_loss: 4.7010\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9265 - val_loss: 4.7009\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9284 - val_loss: 4.7015\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9574 - val_loss: 4.7012\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9249 - val_loss: 4.7011\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9333 - val_loss: 4.7010\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9510 - val_loss: 4.7012\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9220 - val_loss: 4.7010\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9149 - val_loss: 4.7011\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9159 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9134 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9059 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9309 - val_loss: 4.7010\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9343 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9430 - val_loss: 4.7011\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9351 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9622 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.8911 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9427 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9082 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9078 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9205 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9189 - val_loss: 4.7010\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9284 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9204 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9522 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9434 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9248 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9659 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.8929 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9111 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9220 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9235 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9174 - val_loss: 4.7010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9579 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9313 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9238 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9163 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9266 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9352 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9110 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9220 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 7707.3858 - val_loss: 15292.4424\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7484.5559 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7966.9132 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7574.3009 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7636.6730 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7455.0211 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7753.4287 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7640.7872 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7565.5820 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7402.5466 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7660.8903 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7508.1978 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7787.2370 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 8438.9128 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7409.4781 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7588.3568 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7350.0047 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8108.4070 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7686.5150 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7622.3838 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7665.0732 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7691.7028 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7312.6141 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7716.4896 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7793.5906 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7722.6405 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7522.2471 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7128.4107 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7637.4088 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7817.3681 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7563.6216 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7925.6751 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7734.0281 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7473.4494 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7614.5637 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7697.4651 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7642.3015 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7628.9151 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7565.1589 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7718.7938 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7412.6120 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7839.7783 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7864.2395 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7562.6351 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7587.6708 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7430.6797 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7765.2532 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7624.2677 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7691.5052 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7492.4929 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 27.0711 - val_loss: 2.5896\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7.9252 - val_loss: 5.6633\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0185 - val_loss: 18.6644\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 13.8053 - val_loss: 6.3104\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 8.5310 - val_loss: 3.8471\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.5812 - val_loss: 3.2032\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.1550 - val_loss: 26.3762\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 9.1129 - val_loss: 4.7206\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7.7035 - val_loss: 6.0636\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13.0494 - val_loss: 33.4099\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 18.1069 - val_loss: 12.2282\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 6.5821 - val_loss: 7.4523\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.0665 - val_loss: 5.6495\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.5707 - val_loss: 5.2050\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.7459 - val_loss: 7.0742\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.4039 - val_loss: 11.1821\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.3063 - val_loss: 4.5680\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.2010 - val_loss: 4.3391\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.8110 - val_loss: 4.1021\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7119 - val_loss: 8.2261\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.3487 - val_loss: 3.5807\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.8717 - val_loss: 3.5143\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.9260 - val_loss: 3.7670\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.9170 - val_loss: 3.7797\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.0445 - val_loss: 3.4730\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.8636 - val_loss: 3.4498\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8468 - val_loss: 3.4231\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8703 - val_loss: 3.4926\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.8078 - val_loss: 3.3793\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8238 - val_loss: 3.3592\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.7718 - val_loss: 3.3357\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.7334 - val_loss: 3.3514\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.7298 - val_loss: 3.3755\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.7672 - val_loss: 3.3682\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.7330 - val_loss: 3.3491\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.7262 - val_loss: 3.3629\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.7421 - val_loss: 3.3604\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.7159 - val_loss: 3.3582\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.7333 - val_loss: 3.3588\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.7349 - val_loss: 3.3730\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.7076 - val_loss: 3.3582\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.7389 - val_loss: 3.3625\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.7251 - val_loss: 3.3647\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.7182 - val_loss: 3.3644\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.7512 - val_loss: 3.3636\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.7294 - val_loss: 3.3626\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 2.7405 - val_loss: 3.3637\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.7363 - val_loss: 3.3625\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.7440 - val_loss: 3.3617\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.7196 - val_loss: 3.3606\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2355.9920 - val_loss: 2171.1497\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 944.0162 - val_loss: 1889.0471\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 878.9816 - val_loss: 1884.8661\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 871.8502 - val_loss: 1990.4954\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 873.8649 - val_loss: 1920.2173\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 901.5887 - val_loss: 1764.5138\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 909.6916 - val_loss: 1906.1609\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 935.5549 - val_loss: 1739.4165\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 873.9744 - val_loss: 1984.7153\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 869.8091 - val_loss: 1897.3433\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 852.6332 - val_loss: 1844.9946\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 910.3545 - val_loss: 1788.7406\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 864.0366 - val_loss: 1817.0850\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 864.0279 - val_loss: 1763.3990\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 895.4090 - val_loss: 1985.5702\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 847.9460 - val_loss: 1985.5157\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 881.3606 - val_loss: 1820.8180\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 886.0202 - val_loss: 1813.2010\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 900.1572 - val_loss: 1843.2771\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 878.1161 - val_loss: 1841.3174\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 854.6652 - val_loss: 1838.3889\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 898.9089 - val_loss: 1835.4949\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 889.2477 - val_loss: 1849.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 834.2469 - val_loss: 1797.7100\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 831.0390 - val_loss: 1817.8241\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 839.0309 - val_loss: 1824.0779\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 816.3242 - val_loss: 1825.4817\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 855.2341 - val_loss: 1813.9286\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 828.0459 - val_loss: 1822.2174\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 870.6361 - val_loss: 1827.4591\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 870.6622 - val_loss: 1830.1449\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 862.3069 - val_loss: 1832.3152\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 850.1367 - val_loss: 1834.3239\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 903.6332 - val_loss: 1835.5078\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 808.7806 - val_loss: 1835.5261\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 870.1887 - val_loss: 1836.2362\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 855.7046 - val_loss: 1836.1343\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 865.7955 - val_loss: 1836.9583\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 876.9081 - val_loss: 1836.9014\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 783.5646 - val_loss: 1836.9026\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 822.4407 - val_loss: 1836.8473\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 872.3356 - val_loss: 1836.8496\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 866.7127 - val_loss: 1836.7452\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 817.0435 - val_loss: 1836.8124\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 835.6026 - val_loss: 1836.5653\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 859.2386 - val_loss: 1836.3905\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 809.1122 - val_loss: 1836.4613\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 832.5755 - val_loss: 1836.3981\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 834.9611 - val_loss: 1836.3909\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 789.7817 - val_loss: 1836.4065\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.2506 - val_loss: 2.7160\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1553 - val_loss: 2.5818\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.1216 - val_loss: 2.5235\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0924 - val_loss: 2.4725\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0899 - val_loss: 2.4769\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0860 - val_loss: 2.4710\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0726 - val_loss: 2.4641\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0811 - val_loss: 3.1230\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1432 - val_loss: 2.4915\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0848 - val_loss: 2.4835\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0775 - val_loss: 3.1818\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0773 - val_loss: 2.4538\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0719 - val_loss: 2.4683\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0662 - val_loss: 2.4570\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0788 - val_loss: 2.4523\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0902 - val_loss: 2.4552\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0684 - val_loss: 2.4740\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2.0912 - val_loss: 2.5131\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1470 - val_loss: 2.4696\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0693 - val_loss: 2.4724\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0785 - val_loss: 2.4983\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0647 - val_loss: 2.4740\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0676 - val_loss: 2.5096\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0768 - val_loss: 2.4827\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0695 - val_loss: 2.4492\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0714 - val_loss: 2.6077\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0861 - val_loss: 2.4588\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0909 - val_loss: 2.4566\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1254 - val_loss: 2.4604\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1343 - val_loss: 2.5493\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.6300 - val_loss: 2.4564\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1177 - val_loss: 2.4415\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1876 - val_loss: 2.5368\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0745 - val_loss: 2.4500\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0732 - val_loss: 2.4817\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1136 - val_loss: 2.4662\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0910 - val_loss: 2.4566\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.2160 - val_loss: 12.1742\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.2016 - val_loss: 2.5006\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.3358 - val_loss: 2.7495\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.3696 - val_loss: 2.4756\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1321 - val_loss: 2.4835\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0509 - val_loss: 2.4489\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0323 - val_loss: 2.4390\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0267 - val_loss: 2.4462\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0285 - val_loss: 2.4449\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0297 - val_loss: 2.4421\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0347 - val_loss: 2.4359\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0310 - val_loss: 2.4357\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0277 - val_loss: 2.4421\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 3209.0288 - val_loss: 2318.4102\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 1046.0157 - val_loss: 2068.2891\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 935.9252 - val_loss: 2034.1940\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 899.2560 - val_loss: 2001.1257\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 900.4249 - val_loss: 1898.2058\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 850.1607 - val_loss: 1884.5372\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 869.2707 - val_loss: 1948.6450\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 796.9511 - val_loss: 1704.0396\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 867.9819 - val_loss: 1769.0588\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 837.5397 - val_loss: 1809.3368\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 897.9635 - val_loss: 2062.3701\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 859.5540 - val_loss: 1891.5104\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 871.3617 - val_loss: 1879.0753\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 829.5638 - val_loss: 1855.1293\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 817.2070 - val_loss: 1964.9177\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 883.1501 - val_loss: 1796.4039\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 864.1650 - val_loss: 1844.0304\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 893.5566 - val_loss: 1892.4180\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 858.3219 - val_loss: 1843.3877\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 826.0274 - val_loss: 1857.1392\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 869.0441 - val_loss: 1855.1847\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 859.2375 - val_loss: 1852.6659\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 854.4658 - val_loss: 1843.9484\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 852.5045 - val_loss: 1861.0527\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 827.0429 - val_loss: 1828.1772\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 844.9933 - val_loss: 1856.8237\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 852.8983 - val_loss: 1832.2272\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 860.0504 - val_loss: 1838.6917\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 843.7320 - val_loss: 1840.8752\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 831.5177 - val_loss: 1840.6930\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 890.4447 - val_loss: 1842.6399\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 885.1962 - val_loss: 1844.6112\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 919.7503 - val_loss: 1846.4830\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 850.6776 - val_loss: 1845.8479\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 831.8410 - val_loss: 1845.1843\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 933.5581 - val_loss: 1844.9679\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 889.5082 - val_loss: 1846.2698\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 850.7687 - val_loss: 1846.3716\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 828.0909 - val_loss: 1846.3878\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 821.7919 - val_loss: 1846.2247\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 892.1773 - val_loss: 1846.3416\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 866.2152 - val_loss: 1846.2493\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 915.6802 - val_loss: 1846.2931\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 850.8085 - val_loss: 1846.3013\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 833.2794 - val_loss: 1846.3398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 871.6351 - val_loss: 1846.2533\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 858.2749 - val_loss: 1846.2201\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 808.6928 - val_loss: 1846.2472\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 826.0879 - val_loss: 1846.2538\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 842.5568 - val_loss: 1846.2595\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 30ms/step - loss: 4.2621 - val_loss: 4.9657\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.1347 - val_loss: 4.9065\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.1026 - val_loss: 4.8792\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0953 - val_loss: 4.8449\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0729 - val_loss: 4.8282\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0608 - val_loss: 4.8317\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0760 - val_loss: 4.8109\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0380 - val_loss: 4.8668\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0440 - val_loss: 4.8059\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.0377 - val_loss: 4.8064\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0403 - val_loss: 4.8027\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0504 - val_loss: 4.8198\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0468 - val_loss: 4.8156\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0243 - val_loss: 4.7987\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0394 - val_loss: 4.7962\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0315 - val_loss: 4.7995\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0305 - val_loss: 4.7973\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0190 - val_loss: 4.7970\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0365 - val_loss: 4.7949\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 4.0479 - val_loss: 4.7984\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0113 - val_loss: 4.7939\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0335 - val_loss: 4.7949\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0178 - val_loss: 4.7968\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0355 - val_loss: 4.8271\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0422 - val_loss: 4.7933\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.0198 - val_loss: 4.7936\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.0252 - val_loss: 4.7937\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0215 - val_loss: 4.7901\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0433 - val_loss: 4.7914\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.0202 - val_loss: 4.7880\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.0334 - val_loss: 4.7927\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0124 - val_loss: 4.7885\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0174 - val_loss: 4.7915\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9996 - val_loss: 4.7929\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0250 - val_loss: 4.7923\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0338 - val_loss: 4.7914\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.0384 - val_loss: 4.7963\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0224 - val_loss: 4.7904\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0080 - val_loss: 4.7890\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0162 - val_loss: 4.7883\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9486 - val_loss: 4.7153\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 3.9481 - val_loss: 4.7156\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9348 - val_loss: 4.7152\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 3.9303 - val_loss: 4.7152\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9495 - val_loss: 4.7154\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9357 - val_loss: 4.7161\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9444 - val_loss: 4.7151\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 3.9341 - val_loss: 4.7154\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 3.9490 - val_loss: 4.7156\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9550 - val_loss: 4.7152\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 30ms/step - loss: 7707.1714 - val_loss: 15292.7988\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 7765.7750 - val_loss: 15292.7402\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7367.3595 - val_loss: 15292.7168\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7685.5220 - val_loss: 15292.6934\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7376.8077 - val_loss: 15292.6777\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7666.7146 - val_loss: 15292.6416\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7517.8576 - val_loss: 15292.6387\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7931.2021 - val_loss: 15292.6396\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7711.3199 - val_loss: 15292.5996\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7755.0304 - val_loss: 15292.6006\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7674.6386 - val_loss: 15292.5986\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7156.9521 - val_loss: 15292.5889\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7409.0098 - val_loss: 15292.5850\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7635.7426 - val_loss: 15292.5762\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7364.6913 - val_loss: 15292.5781\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 7515.4959 - val_loss: 15292.6768\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 7740.8755 - val_loss: 15292.5732\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7665.4605 - val_loss: 15292.5703\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 7389.5501 - val_loss: 15292.5596\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7567.2000 - val_loss: 15292.5576\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7380.2729 - val_loss: 15292.5635\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 6s 28ms/step - loss: 8010.6485 - val_loss: 15292.5615\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7487.6696 - val_loss: 15292.5576\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7642.0053 - val_loss: 15292.5557\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7647.6233 - val_loss: 15292.5576\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7805.6767 - val_loss: 15292.5527\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7644.4731 - val_loss: 15292.5518\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7691.4719 - val_loss: 15292.5469\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7563.0637 - val_loss: 15292.5615\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7685.2663 - val_loss: 15292.5527\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7279.7540 - val_loss: 15292.5449\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 7481.5722 - val_loss: 15292.5469\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7536.6202 - val_loss: 15292.5430\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7987.3072 - val_loss: 15292.5400\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7791.3606 - val_loss: 15292.5381\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7725.5467 - val_loss: 15292.5430\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 7865.7150 - val_loss: 15292.5381\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7755.6644 - val_loss: 15292.5381\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7576.8694 - val_loss: 15292.5381\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 7381.8173 - val_loss: 15292.5391\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7659.3673 - val_loss: 15292.5352\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 7441.5882 - val_loss: 15292.5352\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 8179.6111 - val_loss: 15292.5361\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7632.0042 - val_loss: 15292.5332\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7800.3864 - val_loss: 15292.5332\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7570.7820 - val_loss: 15292.5352\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7666.5197 - val_loss: 15292.5342\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 7550.6346 - val_loss: 15292.5352\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7689.0919 - val_loss: 15292.5352\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7496.7180 - val_loss: 15292.5312\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 30ms/step - loss: 4.3258 - val_loss: 4.9315\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.1255 - val_loss: 4.9019\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0963 - val_loss: 4.8901\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.1126 - val_loss: 4.8649\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0744 - val_loss: 4.8528\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0613 - val_loss: 4.8392\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0568 - val_loss: 4.8316\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0560 - val_loss: 4.8214\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.0709 - val_loss: 4.8201\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0420 - val_loss: 4.8087\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0111 - val_loss: 4.8066\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0133 - val_loss: 4.7991\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0265 - val_loss: 4.7934\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.0253 - val_loss: 4.7928\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0333 - val_loss: 4.7959\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0032 - val_loss: 4.7827\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0064 - val_loss: 4.7806\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0038 - val_loss: 4.7781\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.0009 - val_loss: 4.7751\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 3.9975 - val_loss: 4.7745\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 4.0213 - val_loss: 4.7709\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0185 - val_loss: 4.7704\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0178 - val_loss: 4.7682\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.0198 - val_loss: 4.7706\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9720 - val_loss: 4.7660\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0058 - val_loss: 4.7710\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9558 - val_loss: 4.7654\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9596 - val_loss: 4.7594\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 3.9966 - val_loss: 4.7592\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 3.9825 - val_loss: 4.7577\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9946 - val_loss: 4.7549\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9749 - val_loss: 4.7697\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0061 - val_loss: 4.7533\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 3.9680 - val_loss: 4.7544\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9839 - val_loss: 4.7549\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 3.9698 - val_loss: 4.7510\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9913 - val_loss: 4.7525\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9663 - val_loss: 4.7493\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 3.9819 - val_loss: 4.7505\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9558 - val_loss: 4.7482\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 3.9785 - val_loss: 4.7489\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 3.9672 - val_loss: 4.7537\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9708 - val_loss: 4.7461\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 3.9660 - val_loss: 4.7463\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9622 - val_loss: 4.7458\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9698 - val_loss: 4.7515\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9708 - val_loss: 4.7435\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 3.9657 - val_loss: 4.7450\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: 3.9633 - val_loss: 4.8146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9819 - val_loss: 4.7426\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 30ms/step - loss: 7297.7425 - val_loss: 15292.9688\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7353.7985 - val_loss: 15292.8955\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7766.8808 - val_loss: 15292.8574\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7588.0673 - val_loss: 15292.8027\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7442.8208 - val_loss: 15292.7676\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 7701.5022 - val_loss: 15292.7520\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7520.7158 - val_loss: 15292.7227\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7355.0646 - val_loss: 15292.6973\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7268.9103 - val_loss: 15292.6924\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7335.1306 - val_loss: 15292.6729\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 7420.8317 - val_loss: 15292.6611\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 8088.3452 - val_loss: 15292.6953\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7709.0750 - val_loss: 15292.6357\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7313.1036 - val_loss: 15292.6299\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7564.6790 - val_loss: 15292.6201\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 7707.5653 - val_loss: 15292.6143\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7660.5841 - val_loss: 15292.6006\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7481.4114 - val_loss: 15292.5938\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 7609.8170 - val_loss: 15292.5908\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 7549.4695 - val_loss: 15292.5850\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7895.1519 - val_loss: 15292.5850\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7474.8877 - val_loss: 15292.5830\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 7914.8220 - val_loss: 15292.5713\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7882.5400 - val_loss: 15292.5703\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 8122.9122 - val_loss: 15292.5654\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 8310.5606 - val_loss: 15292.5635\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 8349.9191 - val_loss: 15292.5576\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7664.9185 - val_loss: 15292.5615\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7183.7335 - val_loss: 15292.5654\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7638.5547 - val_loss: 15292.5576\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7559.1972 - val_loss: 15292.5459\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 7864.2489 - val_loss: 15292.5537\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7908.6339 - val_loss: 15292.5488\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7639.4288 - val_loss: 15292.5410\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7099.3812 - val_loss: 15292.5469\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7443.6256 - val_loss: 15292.5391\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7361.3716 - val_loss: 15292.5371\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7544.9688 - val_loss: 15292.5352\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 7748.8206 - val_loss: 15292.5371\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7571.3395 - val_loss: 15292.5293\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 7544.5396 - val_loss: 15292.5283\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 7926.2935 - val_loss: 15292.5283\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7440.6701 - val_loss: 15292.5244\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7548.2881 - val_loss: 15292.5283\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 8020.1420 - val_loss: 15292.5244\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7697.5390 - val_loss: 15292.5254\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7739.2690 - val_loss: 15292.5186\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7513.3042 - val_loss: 15292.5225\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7666.4919 - val_loss: 15292.5215\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 7407.5684 - val_loss: 15292.5215\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 31ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 7s 34ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: 30.6241\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 25.6250 - val_loss: 25.3900\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 21.6458 - val_loss: 22.4295\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 19.1197 - val_loss: 20.3944\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 17.5712 - val_loss: 18.8706\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 16.2045 - val_loss: 17.6693\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 15.2402 - val_loss: 16.6944\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 14.3577 - val_loss: 15.8862\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 13.7070 - val_loss: 15.2041\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 13.1810 - val_loss: 14.6191\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: 28493.6777\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 14986.9857 - val_loss: 26011.7617\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13442.8650 - val_loss: 24579.8496\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 12674.7378 - val_loss: 23589.1406\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11523.6583 - val_loss: 22841.2988\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11354.0469 - val_loss: 22256.2988\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11235.1766 - val_loss: 21781.3105\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 10753.3055 - val_loss: 21388.3672\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 10938.4432 - val_loss: 21056.2891\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11222.8581 - val_loss: 20771.8086\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 18.7554 - val_loss: 3.0366\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.7951 - val_loss: 2.8423\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.4246 - val_loss: 6.5548\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.3988 - val_loss: 3.0217\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.5229 - val_loss: 2.9477\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.3953 - val_loss: 8.4214\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.5680 - val_loss: 3.0677\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.9189 - val_loss: 3.9172\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.6719 - val_loss: 3.4422\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.3499 - val_loss: 3.0336\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.7864 - val_loss: 2.9872\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0890 - val_loss: 3.2752\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.4855 - val_loss: 2.5174\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.2102 - val_loss: 2.5551\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1663 - val_loss: 2.5001\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1186 - val_loss: 2.5056\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1211 - val_loss: 2.4785\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1060 - val_loss: 2.8110\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1335 - val_loss: 2.5625\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1083 - val_loss: 2.6200\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1387 - val_loss: 2.5056\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1147 - val_loss: 2.7035\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1127 - val_loss: 2.5561\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1097 - val_loss: 2.4842\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1212 - val_loss: 2.4694\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1006 - val_loss: 2.5917\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1139 - val_loss: 2.5006\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1062 - val_loss: 2.4845\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0999 - val_loss: 2.5114\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0989 - val_loss: 2.5047\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1162 - val_loss: 2.5222\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0945 - val_loss: 2.4728\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0992 - val_loss: 2.6381\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0937 - val_loss: 2.5213\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1009 - val_loss: 2.5308\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0718 - val_loss: 2.4687\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0607 - val_loss: 2.4647\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0625 - val_loss: 2.4616\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0570 - val_loss: 2.4620\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0571 - val_loss: 2.4656\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0576 - val_loss: 2.4672\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0580 - val_loss: 2.4689\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0611 - val_loss: 2.4661\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0624 - val_loss: 2.4676\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0517 - val_loss: 2.4602\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0587 - val_loss: 2.4660\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0624 - val_loss: 2.4580\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0650 - val_loss: 2.4612\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0583 - val_loss: 2.4613\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0527 - val_loss: 2.4655\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 1617.3222 - val_loss: 1852.6208\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 955.6684 - val_loss: 1898.4523\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 931.5511 - val_loss: 1898.8257\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 907.0698 - val_loss: 1942.2925\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 893.5458 - val_loss: 1921.8423\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 889.3271 - val_loss: 1937.6162\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 883.5444 - val_loss: 1808.6844\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 875.6546 - val_loss: 1803.4910\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 906.7295 - val_loss: 1935.9089\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 848.0223 - val_loss: 2250.1775\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 904.7324 - val_loss: 1855.4856\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 899.6936 - val_loss: 1873.1106\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 923.5229 - val_loss: 1824.9729\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 936.4035 - val_loss: 1762.4482\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 907.1420 - val_loss: 1715.9886\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 905.1390 - val_loss: 1861.8225\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 886.5246 - val_loss: 2063.9031\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 856.2057 - val_loss: 2001.5129\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 875.0783 - val_loss: 1891.4915\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 883.5324 - val_loss: 1927.3774\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 940.9235 - val_loss: 2030.9006\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 895.9236 - val_loss: 1724.3969\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 883.3976 - val_loss: 1856.9193\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 911.1684 - val_loss: 2177.7500\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 868.6394 - val_loss: 1857.1777\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 861.3746 - val_loss: 1839.7688\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 839.1019 - val_loss: 1829.0968\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 897.1794 - val_loss: 1872.3267\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 900.8187 - val_loss: 1853.2063\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 831.8693 - val_loss: 1823.1134\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 893.5925 - val_loss: 1868.7058\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 881.4669 - val_loss: 1835.9163\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 923.5337 - val_loss: 1859.6571\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 877.2899 - val_loss: 1824.9049\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 850.4457 - val_loss: 1836.4014\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 834.0669 - val_loss: 1833.9364\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 858.3016 - val_loss: 1834.5002\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 832.9044 - val_loss: 1832.4314\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 810.3858 - val_loss: 1832.3649\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 856.2809 - val_loss: 1832.7661\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 843.8170 - val_loss: 1833.2212\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 818.7706 - val_loss: 1832.7792\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 813.2301 - val_loss: 1832.8544\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 831.0908 - val_loss: 1832.5707\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 847.6393 - val_loss: 1834.3892\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 860.5160 - val_loss: 1834.4534\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 875.3527 - val_loss: 1834.7155\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.9104 - val_loss: 1834.6978\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.6306 - val_loss: 1834.6436\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 872.2786 - val_loss: 1834.6935\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 3.7826 - val_loss: 2.5811\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1303 - val_loss: 2.4839\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1061 - val_loss: 2.4938\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0965 - val_loss: 2.4769\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0927 - val_loss: 2.4742\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0954 - val_loss: 2.4903\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2.0787 - val_loss: 2.4821\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0891 - val_loss: 2.4713\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0871 - val_loss: 2.5103\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0876 - val_loss: 2.4828\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0874 - val_loss: 2.4839\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0852 - val_loss: 2.4795\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0870 - val_loss: 2.5290\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0856 - val_loss: 2.4694\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0949 - val_loss: 2.5693\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0848 - val_loss: 2.5080\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1027 - val_loss: 2.5257\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0821 - val_loss: 2.4818\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0856 - val_loss: 2.4745\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0910 - val_loss: 2.4809\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0897 - val_loss: 3.8056\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1317 - val_loss: 2.4893\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0972 - val_loss: 2.4781\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1012 - val_loss: 2.4924\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0643 - val_loss: 2.4669\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0630 - val_loss: 2.4573\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0644 - val_loss: 2.4635\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0628 - val_loss: 2.4641\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0579 - val_loss: 2.4562\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0624 - val_loss: 2.4626\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0579 - val_loss: 2.4636\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0549 - val_loss: 2.4613\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0510 - val_loss: 2.4671\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0584 - val_loss: 2.4551\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0523 - val_loss: 2.4536\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0542 - val_loss: 2.4581\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0581 - val_loss: 2.4585\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0634 - val_loss: 2.4623\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0653 - val_loss: 2.4671\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0559 - val_loss: 2.4645\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0593 - val_loss: 2.4557\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0618 - val_loss: 2.4667\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0570 - val_loss: 2.4661\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0575 - val_loss: 2.4604\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0590 - val_loss: 2.4672\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0494 - val_loss: 2.4609\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0581 - val_loss: 2.4606\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0500 - val_loss: 2.4599\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0606 - val_loss: 2.4611\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0515 - val_loss: 2.4592\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 3147.9750 - val_loss: 2388.5103\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 1080.3787 - val_loss: 2004.7567\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 939.2715 - val_loss: 1917.9491\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 895.7668 - val_loss: 1926.4575\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 906.0871 - val_loss: 1892.9536\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 847.1057 - val_loss: 1869.3646\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 879.5139 - val_loss: 1835.4128\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 922.7424 - val_loss: 1888.5881\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 897.9018 - val_loss: 1926.5546\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 889.5158 - val_loss: 1800.9283\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 881.1807 - val_loss: 1839.1228\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 835.8432 - val_loss: 1888.9713\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 869.7806 - val_loss: 1812.9122\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 852.7560 - val_loss: 1897.9769\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 834.0553 - val_loss: 1856.1311\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 866.1461 - val_loss: 1829.6952\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 859.5363 - val_loss: 1736.2183\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 878.1226 - val_loss: 1847.4791\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 833.3657 - val_loss: 1923.4910\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 873.8830 - val_loss: 1886.3223\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.6223 - val_loss: 1813.9963\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 863.5216 - val_loss: 1809.4886\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.1185 - val_loss: 1703.4021\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 845.3248 - val_loss: 1923.7400\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 899.7207 - val_loss: 1853.1893\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 877.9510 - val_loss: 1897.6311\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 876.4826 - val_loss: 1753.2395\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 841.4929 - val_loss: 1925.3237\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 897.1101 - val_loss: 1943.5170\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 860.6379 - val_loss: 1786.4371\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 808.1663 - val_loss: 1813.9320\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 865.3656 - val_loss: 1806.7886\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 843.4344 - val_loss: 1857.3069\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 818.5632 - val_loss: 1826.6199\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 877.6811 - val_loss: 1849.6132\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 812.1240 - val_loss: 1830.0603\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 847.2074 - val_loss: 1842.7782\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.3652 - val_loss: 1837.3533\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 920.2839 - val_loss: 1851.8757\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 842.5618 - val_loss: 1841.3459\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 824.4466 - val_loss: 1843.3074\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 840.5223 - val_loss: 1840.4136\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 830.5174 - val_loss: 1855.4569\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 860.8601 - val_loss: 1850.9209\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 841.3690 - val_loss: 1848.9974\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 825.3049 - val_loss: 1848.1885\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 869.4190 - val_loss: 1846.2184\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 847.6338 - val_loss: 1844.9327\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 808.4410 - val_loss: 1844.3693\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 869.9144 - val_loss: 1843.4336\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9978 - val_loss: 4.7006\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9380 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9457 - val_loss: 4.7006\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9466 - val_loss: 4.7007\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9343 - val_loss: 4.7021\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: 3.9454 - val_loss: 4.7007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9294 - val_loss: 4.7007\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9299 - val_loss: 4.7007\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9295 - val_loss: 4.7013\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9463 - val_loss: 4.7008\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9276 - val_loss: 4.7007\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9335 - val_loss: 4.7008\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9374 - val_loss: 4.7010\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9510 - val_loss: 4.7011\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9346 - val_loss: 4.7010\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9192 - val_loss: 4.7009\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9526 - val_loss: 4.7011\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9154 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9486 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9312 - val_loss: 4.7009\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9281 - val_loss: 4.7009\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9367 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9076 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9314 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9268 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9370 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9220 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9221 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9178 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9272 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9056 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9395 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9338 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9622 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9354 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9249 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9068 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9254 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9038 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9205 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9157 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9128 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9277 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9355 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9296 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9436 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9010 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9480 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9352 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9185 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7725.7710 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7020.5690 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7931.3352 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7195.1938 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7849.2671 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7711.5569 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7890.1224 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7545.0742 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7445.1522 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7511.5031 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7646.6515 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7524.6411 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7279.8296 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7422.5308 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7596.9434 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7202.0379 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7620.5317 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7266.7334 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7605.6972 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7819.9017 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7734.9288 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7366.2699 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7660.9840 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7718.8077 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8020.5369 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7405.3488 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7728.8204 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7374.9090 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7557.7278 - val_loss: 15292.4346\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: 7697.4471 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7605.7632 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7413.9897 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7304.6926 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7883.0041 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7748.3248 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7508.1475 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7499.4025 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7874.5716 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7638.4689 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8021.8672 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7824.1810 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7641.9263 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7831.0160 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7215.0043 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7451.2082 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7752.7503 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7672.3459 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7523.5250 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7374.3725 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8038.0729 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0904 - val_loss: 4.7017\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9338 - val_loss: 4.7009\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9126 - val_loss: 4.7008\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9433 - val_loss: 4.7008\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.8967 - val_loss: 4.7009\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9249 - val_loss: 4.7008\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9303 - val_loss: 4.7008\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9408 - val_loss: 4.7008\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9473 - val_loss: 4.7009\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9217 - val_loss: 4.7007\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9487 - val_loss: 4.7010\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9529 - val_loss: 4.7009\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8969 - val_loss: 4.7008\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9325 - val_loss: 4.7017\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9247 - val_loss: 4.7013\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9312 - val_loss: 4.7008\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9266 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9274 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9040 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9221 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9266 - val_loss: 4.7010\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9114 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9622 - val_loss: 4.7011\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9418 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9401 - val_loss: 4.7011\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9428 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9397 - val_loss: 4.7011\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9317 - val_loss: 4.7011\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9130 - val_loss: 4.7011\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9351 - val_loss: 4.7011\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9328 - val_loss: 4.7011\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9267 - val_loss: 4.7011\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9410 - val_loss: 4.7011\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9153 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9230 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9377 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9447 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9109 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9013 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9395 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9116 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9324 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9340 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9357 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9368 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9273 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9266 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9438 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9189 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9513 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7630.4820 - val_loss: 15292.4404\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7430.0215 - val_loss: 15292.4346\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 7867.5436 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7634.0222 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7511.7111 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7026.9873 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7396.7398 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7887.5936 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7662.7215 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8040.6577 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7723.5508 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7341.2566 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7416.0208 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7259.1932 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7391.8013 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7436.5494 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7653.9770 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7917.1202 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7808.1020 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7490.5236 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8008.1656 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7764.3863 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7563.7744 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7713.2138 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7776.6952 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7672.8169 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7428.8382 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7424.8918 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7847.9966 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7470.3383 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7636.4546 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7851.9023 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7900.6122 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7526.8275 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7796.8168 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7687.4267 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7628.3423 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7567.9435 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7312.4914 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7865.1926 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7279.4567 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7404.7899 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7588.6231 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7864.4232 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7838.9810 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7417.2761 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7772.9869 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7544.8974 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7388.0927 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7857.7806 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 19.7239 - val_loss: 2.6997\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 9.1354 - val_loss: 12.1620\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 17.7849 - val_loss: 11.2785\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 23.0529 - val_loss: 29.4704\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 22.4206 - val_loss: 51.0927\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 25.8168 - val_loss: 39.6261\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 23.7757 - val_loss: 52.9415\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 24.6738 - val_loss: 36.2074\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 25.1287 - val_loss: 17.2468\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 22.2467 - val_loss: 63.7997\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 24.5798 - val_loss: 9.3405\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7.5419 - val_loss: 7.0896\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.7841 - val_loss: 6.2393\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 5.7763 - val_loss: 6.1585\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3832 - val_loss: 5.7346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.3659 - val_loss: 5.6865\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.7340 - val_loss: 5.4711\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.7157 - val_loss: 5.4520\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.6464 - val_loss: 5.4260\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9579 - val_loss: 5.9842\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.5110 - val_loss: 6.1957\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 4.2033 - val_loss: 5.2492\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.1419 - val_loss: 5.4589\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.1441 - val_loss: 5.2490\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.1746 - val_loss: 5.2389\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.1292 - val_loss: 5.2567\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.1901 - val_loss: 5.2849\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.1461 - val_loss: 5.3753\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 4.1716 - val_loss: 5.2829\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.2108 - val_loss: 5.3263\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.2289 - val_loss: 5.2439\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.1762 - val_loss: 5.2427\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.1238 - val_loss: 5.2364\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.1303 - val_loss: 5.2368\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.1435 - val_loss: 5.2392\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0916 - val_loss: 5.2361\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.1371 - val_loss: 5.2359\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.1884 - val_loss: 5.2412\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.1916 - val_loss: 5.2418\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.1632 - val_loss: 5.2384\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.1753 - val_loss: 5.2410\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.1558 - val_loss: 5.2404\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.1992 - val_loss: 5.2399\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0778 - val_loss: 5.2395\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.1203 - val_loss: 5.2393\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.1478 - val_loss: 5.2391\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.1368 - val_loss: 5.2390\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.1134 - val_loss: 5.2389\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.1531 - val_loss: 5.2388\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.1447 - val_loss: 5.2387\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 1791.2567 - val_loss: 2362.1667\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 970.8199 - val_loss: 2008.4532\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 882.3350 - val_loss: 1910.6525\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 909.4415 - val_loss: 2013.4193\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 860.5399 - val_loss: 1859.2380\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 850.9854 - val_loss: 1725.6161\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 881.9330 - val_loss: 1718.3336\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 929.3676 - val_loss: 1904.6782\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 903.3796 - val_loss: 1977.5151\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 922.6586 - val_loss: 1973.6636\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 888.7169 - val_loss: 1814.5848\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 839.7375 - val_loss: 1785.3933\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 909.3230 - val_loss: 1971.1201\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 928.9614 - val_loss: 1882.9628\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 920.6668 - val_loss: 1888.1433\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 857.6997 - val_loss: 1834.7233\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 856.0127 - val_loss: 2486.4487\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 909.2051 - val_loss: 1838.1713\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 855.5403 - val_loss: 1857.5684\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 868.3900 - val_loss: 1850.0415\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 869.7582 - val_loss: 1846.8822\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 878.2229 - val_loss: 1826.8398\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 942.6895 - val_loss: 1864.2557\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 923.0253 - val_loss: 1850.0626\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 905.8862 - val_loss: 1835.6138\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 873.8276 - val_loss: 1823.5316\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 855.1661 - val_loss: 1836.2533\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 842.2224 - val_loss: 1835.5682\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 852.9062 - val_loss: 1834.6124\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 849.3811 - val_loss: 1834.7511\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 810.5990 - val_loss: 1835.1146\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 863.9840 - val_loss: 1835.6597\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 812.9182 - val_loss: 1835.8840\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 840.1606 - val_loss: 1836.4349\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 875.2275 - val_loss: 1838.0989\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 841.7534 - val_loss: 1835.7593\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 846.4700 - val_loss: 1836.0098\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 844.1932 - val_loss: 1836.0470\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 845.1198 - val_loss: 1836.3927\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 824.9773 - val_loss: 1836.4478\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 803.4295 - val_loss: 1836.4546\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 896.0083 - val_loss: 1836.5869\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 906.0275 - val_loss: 1836.7520\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 897.0485 - val_loss: 1836.8693\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 852.0626 - val_loss: 1836.9475\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 841.0253 - val_loss: 1836.9227\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 848.8257 - val_loss: 1837.0107\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 865.8987 - val_loss: 1837.0001\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 846.3623 - val_loss: 1836.9954\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 885.0543 - val_loss: 1836.9934\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 22ms/step - loss: 3.9375 - val_loss: 2.4881\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1251 - val_loss: 2.5454\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.1233 - val_loss: 2.4774\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1024 - val_loss: 2.5341\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1037 - val_loss: 2.4965\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.1037 - val_loss: 2.4841\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0918 - val_loss: 2.4649\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0952 - val_loss: 2.4890\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0925 - val_loss: 2.5052\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0840 - val_loss: 2.4886\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0836 - val_loss: 2.4764\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0813 - val_loss: 2.4757\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0796 - val_loss: 2.4769\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0850 - val_loss: 2.4580\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1011 - val_loss: 2.4749\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1218 - val_loss: 2.4673\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1025 - val_loss: 2.4814\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0810 - val_loss: 2.6547\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0969 - val_loss: 2.4762\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0857 - val_loss: 2.4859\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1050 - val_loss: 2.5005\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0860 - val_loss: 2.4661\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1022 - val_loss: 2.4974\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0874 - val_loss: 2.4787\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0508 - val_loss: 2.4565\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.0494 - val_loss: 2.4547\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0564 - val_loss: 2.4549\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0415 - val_loss: 2.4534\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0529 - val_loss: 2.4563\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0420 - val_loss: 2.4553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0391 - val_loss: 2.4494\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0521 - val_loss: 2.4529\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0485 - val_loss: 2.4526\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0397 - val_loss: 2.4521\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0398 - val_loss: 2.4511\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0447 - val_loss: 2.4596\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0409 - val_loss: 2.4588\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0288 - val_loss: 2.4507\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0333 - val_loss: 2.4467\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0390 - val_loss: 2.4484\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0331 - val_loss: 2.4520\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0319 - val_loss: 2.4473\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0372 - val_loss: 2.4506\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0274 - val_loss: 2.4477\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0323 - val_loss: 2.4478\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0350 - val_loss: 2.4505\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0378 - val_loss: 2.4444\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0276 - val_loss: 2.4458\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.0279 - val_loss: 2.4532\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0314 - val_loss: 2.4456\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2946.0567 - val_loss: 2322.5403\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 1091.5351 - val_loss: 2041.2374\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 952.4344 - val_loss: 2018.5468\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 933.2937 - val_loss: 1955.3665\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 899.7499 - val_loss: 1769.2081\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 821.9886 - val_loss: 1749.9808\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 863.9878 - val_loss: 1922.9557\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 841.4510 - val_loss: 1788.8711\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 880.1313 - val_loss: 1876.6746\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 913.6331 - val_loss: 1879.0697\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 910.1496 - val_loss: 1909.7317\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 855.1346 - val_loss: 1884.7391\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 857.0314 - val_loss: 1837.9556\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 801.4097 - val_loss: 1720.7499\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 873.9190 - val_loss: 1834.7299\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 828.6121 - val_loss: 1781.5054\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 888.4435 - val_loss: 1878.4287\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 881.9171 - val_loss: 1794.7799\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 873.0821 - val_loss: 1817.8502\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 899.2307 - val_loss: 1903.2130\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 910.6911 - val_loss: 1875.5494\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 878.8679 - val_loss: 1777.6597\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 872.1870 - val_loss: 1875.0157\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 867.0040 - val_loss: 1774.4785\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 852.3149 - val_loss: 1821.2399\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 849.3747 - val_loss: 1823.2883\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 808.3457 - val_loss: 1856.7435\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 835.9003 - val_loss: 1858.1465\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 875.3487 - val_loss: 1845.1611\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 849.9137 - val_loss: 1828.8259\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 833.0286 - val_loss: 1838.4169\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 823.3962 - val_loss: 1837.7151\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 909.9360 - val_loss: 1851.6373\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 892.5247 - val_loss: 1852.6375\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 875.7159 - val_loss: 1850.0028\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 866.4315 - val_loss: 1849.0457\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 907.2854 - val_loss: 1849.3656\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 826.1201 - val_loss: 1847.4639\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 880.1922 - val_loss: 1847.4053\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 904.7172 - val_loss: 1849.0992\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 898.6666 - val_loss: 1850.6838\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 840.5446 - val_loss: 1849.5884\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 851.5170 - val_loss: 1849.7001\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 782.2018 - val_loss: 1846.8381\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 842.9846 - val_loss: 1846.7858\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 843.6402 - val_loss: 1846.7169\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 828.7343 - val_loss: 1846.6527\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 820.6516 - val_loss: 1846.5947\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 862.2608 - val_loss: 1846.5876\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 869.9249 - val_loss: 1846.4790\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 4.2422 - val_loss: 4.9316\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1468 - val_loss: 4.9115\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1091 - val_loss: 4.8788\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0848 - val_loss: 4.8479\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0727 - val_loss: 4.8338\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0811 - val_loss: 4.8249\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0296 - val_loss: 4.8138\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0453 - val_loss: 4.8306\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0328 - val_loss: 4.8127\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0041 - val_loss: 4.8061\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0371 - val_loss: 4.8047\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0588 - val_loss: 4.8006\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0360 - val_loss: 4.8094\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0529 - val_loss: 4.7971\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0717 - val_loss: 4.7998\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0187 - val_loss: 4.7986\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0330 - val_loss: 4.8173\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0119 - val_loss: 4.7952\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0095 - val_loss: 4.7942\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0158 - val_loss: 4.7929\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0360 - val_loss: 4.7967\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0161 - val_loss: 4.7932\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9993 - val_loss: 4.7938\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0094 - val_loss: 4.7946\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0250 - val_loss: 4.7974\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0576 - val_loss: 4.7927\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0203 - val_loss: 4.7914\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0270 - val_loss: 4.7965\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0358 - val_loss: 4.8014\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0021 - val_loss: 4.7967\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0200 - val_loss: 4.7958\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0123 - val_loss: 4.7902\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0318 - val_loss: 4.7954\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0139 - val_loss: 4.7901\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0344 - val_loss: 4.7989\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0321 - val_loss: 4.7870\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0166 - val_loss: 4.7870\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0065 - val_loss: 4.7889\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0115 - val_loss: 4.7894\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9825 - val_loss: 4.7876\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0333 - val_loss: 4.7963\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0212 - val_loss: 4.7924\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0262 - val_loss: 4.7885\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0031 - val_loss: 4.7890\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0180 - val_loss: 4.7905\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0120 - val_loss: 4.7882\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9600 - val_loss: 4.7150\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9410 - val_loss: 4.7148\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9418 - val_loss: 4.7146\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9487 - val_loss: 4.7147\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7483.6515 - val_loss: 15292.8066\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7689.0684 - val_loss: 15292.7529\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7936.0575 - val_loss: 15292.7490\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7840.2779 - val_loss: 15292.7139\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7369.5536 - val_loss: 15292.6875\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7669.6913 - val_loss: 15292.6602\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7757.0567 - val_loss: 15292.6465\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7904.8416 - val_loss: 15292.6377\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7506.9473 - val_loss: 15292.6309\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7457.3299 - val_loss: 15292.6172\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7259.5794 - val_loss: 15292.6074\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7807.5605 - val_loss: 15292.5898\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7567.2022 - val_loss: 15292.5859\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7487.9044 - val_loss: 15292.5869\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7335.8645 - val_loss: 15292.5898\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7372.5719 - val_loss: 15292.5947\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7645.6218 - val_loss: 15292.5723\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7590.1164 - val_loss: 15292.5654\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7645.9596 - val_loss: 15292.5625\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7631.1864 - val_loss: 15292.5615\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7565.7729 - val_loss: 15292.5635\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7774.8727 - val_loss: 15292.5596\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7906.7893 - val_loss: 15292.5576\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7855.1950 - val_loss: 15292.5566\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7323.6768 - val_loss: 15292.5527\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7858.2643 - val_loss: 15292.5498\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7893.1540 - val_loss: 15292.5527\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7837.7765 - val_loss: 15292.5488\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7559.9373 - val_loss: 15292.5449\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7751.8050 - val_loss: 15292.5449\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7841.4703 - val_loss: 15292.5439\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 7599.0132 - val_loss: 15292.5449\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7700.4121 - val_loss: 15292.5439\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7775.3993 - val_loss: 15292.5469\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7388.5127 - val_loss: 15292.5400\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7898.4823 - val_loss: 15292.5391\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7268.3115 - val_loss: 15292.5381\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7462.4507 - val_loss: 15292.5381\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7530.7815 - val_loss: 15292.5371\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7704.4228 - val_loss: 15292.5381\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7429.1499 - val_loss: 15292.5352\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7790.9216 - val_loss: 15292.5361\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7635.5026 - val_loss: 15292.5352\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7245.1676 - val_loss: 15292.5352\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7472.2166 - val_loss: 15292.5332\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7549.9669 - val_loss: 15292.5332\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7581.0582 - val_loss: 15292.5332\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7588.8663 - val_loss: 15292.5332\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7921.5210 - val_loss: 15292.5312\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7339.4535 - val_loss: 15292.5312\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 4.3343 - val_loss: 4.9281\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.1236 - val_loss: 4.8996\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0944 - val_loss: 4.8801\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0828 - val_loss: 4.8703\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0931 - val_loss: 4.8573\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0735 - val_loss: 4.8420\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0826 - val_loss: 4.8310\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0622 - val_loss: 4.8238\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0724 - val_loss: 4.8191\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0188 - val_loss: 4.8095\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0401 - val_loss: 4.8067\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0195 - val_loss: 4.7995\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0148 - val_loss: 4.7952\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9891 - val_loss: 4.8263\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0259 - val_loss: 4.7886\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9981 - val_loss: 4.7839\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0077 - val_loss: 4.7822\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9984 - val_loss: 4.7805\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9946 - val_loss: 4.7767\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0154 - val_loss: 4.7743\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0106 - val_loss: 4.7722\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9852 - val_loss: 4.7695\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9852 - val_loss: 4.7733\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0142 - val_loss: 4.7677\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9914 - val_loss: 4.7643\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0238 - val_loss: 4.7636\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9693 - val_loss: 4.7604\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9884 - val_loss: 4.7638\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0179 - val_loss: 4.7623\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9943 - val_loss: 4.7582\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0058 - val_loss: 4.7580\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9871 - val_loss: 4.7555\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9740 - val_loss: 4.7549\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9959 - val_loss: 4.7530\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9646 - val_loss: 4.7536\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9798 - val_loss: 4.7546\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9841 - val_loss: 4.7522\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9866 - val_loss: 4.7491\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9728 - val_loss: 4.7492\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9757 - val_loss: 4.7483\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9896 - val_loss: 4.7468\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9511 - val_loss: 4.7465\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9721 - val_loss: 4.7457\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9365 - val_loss: 4.7511\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9875 - val_loss: 4.7443\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9600 - val_loss: 4.7498\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9583 - val_loss: 4.7448\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9977 - val_loss: 4.7508\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9938 - val_loss: 4.7438\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9601 - val_loss: 4.7463\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7777.7822 - val_loss: 15292.9668\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7662.2357 - val_loss: 15292.8896\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7408.5379 - val_loss: 15292.8516\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7822.2763 - val_loss: 15292.8174\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7929.3718 - val_loss: 15292.7705\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7471.2649 - val_loss: 15292.7578\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7869.9344 - val_loss: 15292.7236\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7583.9218 - val_loss: 15292.7061\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7717.2942 - val_loss: 15292.6885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8007.7699 - val_loss: 15292.6816\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8115.4629 - val_loss: 15292.6543\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7467.0500 - val_loss: 15292.6504\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7723.4922 - val_loss: 15292.6377\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7233.3318 - val_loss: 15292.6230\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7786.8650 - val_loss: 15292.6152\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7729.3469 - val_loss: 15292.6309\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7617.0880 - val_loss: 15292.6055\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7392.3924 - val_loss: 15292.5967\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7439.4658 - val_loss: 15292.5908\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7380.6568 - val_loss: 15292.5938\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7337.8455 - val_loss: 15292.5820\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7293.8808 - val_loss: 15292.5752\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7502.4844 - val_loss: 15292.5713\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7772.7870 - val_loss: 15292.5674\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7897.5945 - val_loss: 15292.5664\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7375.8265 - val_loss: 15292.5645\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7545.8209 - val_loss: 15292.5635\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7665.4986 - val_loss: 15292.5566\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7572.6714 - val_loss: 15292.5518\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7447.1954 - val_loss: 15292.5488\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7478.2696 - val_loss: 15292.5469\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7588.5976 - val_loss: 15292.5586\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7476.2891 - val_loss: 15292.5566\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7794.7215 - val_loss: 15292.5449\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7841.8125 - val_loss: 15292.5586\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7060.0827 - val_loss: 15292.5518\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7704.3636 - val_loss: 15292.5352\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7516.2373 - val_loss: 15292.5371\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7852.5517 - val_loss: 15292.5312\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7470.9243 - val_loss: 15292.5264\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7152.9496 - val_loss: 15292.5303\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7521.6357 - val_loss: 15292.5283\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7688.3969 - val_loss: 15292.5293\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7573.3372 - val_loss: 15292.5332\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7646.3900 - val_loss: 15292.5215\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7548.1243 - val_loss: 15292.5195\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7381.3512 - val_loss: 15292.5195\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7618.9537 - val_loss: 15292.5195\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7535.8424 - val_loss: 15292.5176\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7494.9082 - val_loss: 15292.5225\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 30.6298\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 25.3696 - val_loss: 25.3883\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 21.6438 - val_loss: 22.4311\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 19.2702 - val_loss: 20.3976\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 17.6633 - val_loss: 18.8722\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 16.3438 - val_loss: 17.6710\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 15.3734 - val_loss: 16.6958\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 14.4465 - val_loss: 15.8872\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13.6678 - val_loss: 15.2035\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13.0909 - val_loss: 14.6196\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 28493.8887\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 14242.1524 - val_loss: 26011.8516\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13543.6197 - val_loss: 24579.9316\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 12395.1939 - val_loss: 23587.2969\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11496.0953 - val_loss: 22840.7285\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11741.2924 - val_loss: 22255.8730\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11541.3346 - val_loss: 21780.2988\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11612.6609 - val_loss: 21387.2012\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 10629.6675 - val_loss: 21056.6641\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 10902.1849 - val_loss: 20771.6699\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 19.0446 - val_loss: 3.0915\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.8228 - val_loss: 2.9181\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.0273 - val_loss: 3.2888\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.0697 - val_loss: 14.7262\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.1288 - val_loss: 3.5059\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.1326 - val_loss: 12.0037\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 8.0067 - val_loss: 2.9421\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.0963 - val_loss: 2.9491\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.9036 - val_loss: 3.8329\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.7252 - val_loss: 12.5892\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.7061 - val_loss: 3.9003\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.7683 - val_loss: 5.5416\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.5374 - val_loss: 2.5140\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1755 - val_loss: 2.5127\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1925 - val_loss: 2.5553\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1351 - val_loss: 2.5035\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1471 - val_loss: 2.5090\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1180 - val_loss: 2.4960\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1240 - val_loss: 2.5142\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1150 - val_loss: 2.7437\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1258 - val_loss: 2.5146\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1125 - val_loss: 2.4846\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1138 - val_loss: 2.5171\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1016 - val_loss: 2.4926\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1027 - val_loss: 2.4902\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1054 - val_loss: 2.4813\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0990 - val_loss: 2.5029\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0974 - val_loss: 2.5101\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0985 - val_loss: 2.5209\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1199 - val_loss: 2.4898\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1045 - val_loss: 2.4808\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1057 - val_loss: 2.4845\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1022 - val_loss: 2.5008\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1001 - val_loss: 2.4762\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1081 - val_loss: 2.4826\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0929 - val_loss: 2.5222\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1076 - val_loss: 2.4869\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1002 - val_loss: 2.4820\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0978 - val_loss: 2.4735\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1042 - val_loss: 2.4876\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0906 - val_loss: 2.4843\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1004 - val_loss: 2.4868\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0931 - val_loss: 2.5048\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1006 - val_loss: 2.5146\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0991 - val_loss: 2.4968\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0984 - val_loss: 2.4958\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1040 - val_loss: 2.5047\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1040 - val_loss: 2.4912\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1015 - val_loss: 2.4949\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0749 - val_loss: 2.4608\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 1600.9000 - val_loss: 2379.4714\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 971.9566 - val_loss: 1959.1732\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 916.8905 - val_loss: 2018.0894\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 892.7105 - val_loss: 1982.1321\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.8441 - val_loss: 2255.2522\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 916.0757 - val_loss: 1900.2273\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 916.9131 - val_loss: 1987.6959\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 899.0356 - val_loss: 1796.8390\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 933.7015 - val_loss: 2094.0745\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 941.4079 - val_loss: 1864.3365\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.5691 - val_loss: 1959.3325\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 839.8413 - val_loss: 1837.9569\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 867.3599 - val_loss: 1811.9407\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 933.8738 - val_loss: 1985.8771\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 884.3773 - val_loss: 1804.9700\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 894.6855 - val_loss: 1886.4999\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 850.5890 - val_loss: 1754.3470\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 873.9016 - val_loss: 2045.2030\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 841.1128 - val_loss: 2000.6549\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 912.9981 - val_loss: 2062.7241\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 872.8109 - val_loss: 1867.4777\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 846.1961 - val_loss: 1772.5753\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 907.9495 - val_loss: 1855.2937\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 849.9204 - val_loss: 1914.2277\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 868.0861 - val_loss: 1853.8511\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 874.6000 - val_loss: 1795.2533\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 883.8693 - val_loss: 1861.4137\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 943.2004 - val_loss: 1883.5858\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 828.7663 - val_loss: 1829.2883\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 824.2455 - val_loss: 1819.8105\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 837.9923 - val_loss: 1827.7133\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 899.8350 - val_loss: 1826.7618\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 855.0231 - val_loss: 1851.3673\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 819.8802 - val_loss: 1821.0793\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 853.2553 - val_loss: 1829.1483\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.8312 - val_loss: 1839.7256\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 851.6190 - val_loss: 1835.5012\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 872.1408 - val_loss: 1839.4417\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 871.5540 - val_loss: 1839.3960\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 825.3648 - val_loss: 1839.5796\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.8869 - val_loss: 1840.1169\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 845.9084 - val_loss: 1839.3113\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 846.9586 - val_loss: 1840.2076\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 876.6609 - val_loss: 1839.6115\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 883.0265 - val_loss: 1840.8981\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 846.5367 - val_loss: 1840.1597\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 895.2808 - val_loss: 1840.9886\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 841.8708 - val_loss: 1841.0205\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 830.1750 - val_loss: 1841.0756\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 802.2654 - val_loss: 1841.0096\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 3.7976 - val_loss: 2.7923\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1921 - val_loss: 2.6237\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1226 - val_loss: 2.4772\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0943 - val_loss: 2.4739\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0871 - val_loss: 2.4771\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0880 - val_loss: 2.4803\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0883 - val_loss: 2.4743\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0944 - val_loss: 2.4796\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0868 - val_loss: 2.5041\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0855 - val_loss: 2.4940\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0877 - val_loss: 2.4783\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0882 - val_loss: 2.4802\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0845 - val_loss: 2.4696\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0866 - val_loss: 2.4964\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0954 - val_loss: 2.4947\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0865 - val_loss: 2.4984\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0957 - val_loss: 2.4911\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1016 - val_loss: 2.4762\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0917 - val_loss: 2.4857\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0968 - val_loss: 2.4875\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0883 - val_loss: 2.4998\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0917 - val_loss: 2.4817\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0926 - val_loss: 2.4751\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0604 - val_loss: 2.4632\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0574 - val_loss: 2.4625\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0655 - val_loss: 2.4634\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0533 - val_loss: 2.4646\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0554 - val_loss: 2.4655\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0598 - val_loss: 2.4636\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0612 - val_loss: 2.4640\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0574 - val_loss: 2.4647\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0625 - val_loss: 2.4624\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0535 - val_loss: 2.4605\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0577 - val_loss: 2.4679\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0533 - val_loss: 2.4634\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0528 - val_loss: 2.4580\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0598 - val_loss: 2.4618\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0554 - val_loss: 2.4601\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0629 - val_loss: 2.4574\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0575 - val_loss: 2.4609\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0618 - val_loss: 2.4625\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0556 - val_loss: 2.4594\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0537 - val_loss: 2.4605\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0601 - val_loss: 2.4613\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0528 - val_loss: 2.4686\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0601 - val_loss: 2.4588\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0570 - val_loss: 2.4675\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0560 - val_loss: 2.4624\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0545 - val_loss: 2.4634\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0548 - val_loss: 2.4609\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 3198.5686 - val_loss: 2483.8894\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 994.2897 - val_loss: 2042.4225\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 859.9745 - val_loss: 1932.6520\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.4987 - val_loss: 1878.6619\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 896.4554 - val_loss: 1939.1758\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 886.4860 - val_loss: 1907.2942\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 883.8555 - val_loss: 1776.3229\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 851.3042 - val_loss: 1788.7638\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 894.0421 - val_loss: 1862.5383\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 827.0996 - val_loss: 1822.0195\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 919.3165 - val_loss: 1787.9424\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 827.9975 - val_loss: 1797.4386\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 901.2298 - val_loss: 1763.1969\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 857.6342 - val_loss: 1755.6240\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 851.4412 - val_loss: 1901.6736\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 847.5973 - val_loss: 1788.7715\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 875.0811 - val_loss: 1883.5739\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 895.7368 - val_loss: 1802.3754\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 882.3411 - val_loss: 2024.0033\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 877.4895 - val_loss: 1759.5406\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 877.2905 - val_loss: 1904.0752\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 832.3164 - val_loss: 1746.8164\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 900.5883 - val_loss: 1865.5771\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 890.3957 - val_loss: 1869.2018\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 810.4857 - val_loss: 1870.4950\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 846.9750 - val_loss: 1849.5669\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 868.0007 - val_loss: 1841.1082\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 838.9722 - val_loss: 1918.4961\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 897.8480 - val_loss: 1829.8759\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 907.5699 - val_loss: 1889.7001\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 819.0058 - val_loss: 1912.5554\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 839.3330 - val_loss: 1850.9625\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 849.7770 - val_loss: 1832.3508\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 821.9790 - val_loss: 1823.4139\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 851.7526 - val_loss: 1841.7825\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 924.8216 - val_loss: 1854.0267\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 841.8911 - val_loss: 1831.5364\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 863.3139 - val_loss: 1838.0553\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 827.9171 - val_loss: 1833.3060\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.1929 - val_loss: 1838.2495\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.0513 - val_loss: 1845.4840\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 836.5712 - val_loss: 1836.2893\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 845.4994 - val_loss: 1838.8232\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 894.1758 - val_loss: 1839.8597\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 841.2306 - val_loss: 1840.2474\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 846.6400 - val_loss: 1841.2601\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 878.6355 - val_loss: 1841.0315\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 842.1907 - val_loss: 1841.0428\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 824.1765 - val_loss: 1840.3782\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 830.3140 - val_loss: 1840.3147\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9514 - val_loss: 4.7006\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9269 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9150 - val_loss: 4.7006\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9060 - val_loss: 4.7006\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9250 - val_loss: 4.7008\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9349 - val_loss: 4.7006\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9361 - val_loss: 4.7007\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9270 - val_loss: 4.7007\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9075 - val_loss: 4.7013\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9384 - val_loss: 4.7007\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9222 - val_loss: 4.7007\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9134 - val_loss: 4.7012\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9294 - val_loss: 4.7009\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9286 - val_loss: 4.7009\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9471 - val_loss: 4.7010\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9272 - val_loss: 4.7009\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9185 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9320 - val_loss: 4.7009\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9232 - val_loss: 4.7009\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9380 - val_loss: 4.7009\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9322 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9139 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9339 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9182 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9500 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9302 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9339 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9268 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9431 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9262 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9263 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9255 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9035 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9645 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9373 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9238 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9168 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9232 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9216 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9362 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9087 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9194 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9477 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9447 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9517 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9348 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9354 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9494 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9350 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9313 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7176.3550 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7303.9224 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7373.8986 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7646.3509 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7421.2970 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7296.2751 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7690.0435 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7692.3366 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7637.8715 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7952.7538 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7437.1926 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7415.4362 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7353.8309 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7276.7923 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7558.4596 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7695.7842 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7923.8914 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7389.6547 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7345.6821 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7331.9667 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7829.4633 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7678.8532 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7649.7689 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8171.3962 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7638.7951 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7856.9269 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7405.3969 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8162.4504 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7699.6862 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7476.4351 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7946.1866 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7537.4477 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8138.2252 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7762.9259 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7593.5106 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 8013.8902 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7643.9255 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7792.8146 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7497.8336 - val_loss: 15292.4346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8111.0097 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7664.4580 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7678.4121 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7837.6585 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7627.6403 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7476.6832 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7735.5359 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7637.8149 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7555.5033 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7320.7834 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8131.6690 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0985 - val_loss: 4.7017\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9309 - val_loss: 4.7011\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9168 - val_loss: 4.7011\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9352 - val_loss: 4.7018\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9182 - val_loss: 4.7009\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9432 - val_loss: 4.7012\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9438 - val_loss: 4.7010\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9474 - val_loss: 4.7012\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9620 - val_loss: 4.7013\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9132 - val_loss: 4.7008\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9159 - val_loss: 4.7009\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9365 - val_loss: 4.7009\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9285 - val_loss: 4.7009\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9131 - val_loss: 4.7008\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9233 - val_loss: 4.7009\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9359 - val_loss: 4.7007\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9292 - val_loss: 4.7009\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9347 - val_loss: 4.7015\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9243 - val_loss: 4.7008\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9224 - val_loss: 4.7007\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9130 - val_loss: 4.7011\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9279 - val_loss: 4.7011\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9335 - val_loss: 4.7009\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9209 - val_loss: 4.7014\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9348 - val_loss: 4.7011\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9266 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9470 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9153 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9337 - val_loss: 4.7011\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9332 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9123 - val_loss: 4.7010\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9368 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9384 - val_loss: 4.7011\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9520 - val_loss: 4.7011\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9018 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9182 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9501 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9472 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9035 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9539 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8867 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9447 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9402 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9143 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9381 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9460 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9342 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9005 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9161 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9424 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7969.9513 - val_loss: 15292.4414\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7398.8758 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8005.7353 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8022.7800 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7598.5848 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7735.4391 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7724.2706 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7890.3025 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7536.8492 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7589.3326 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7375.5959 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7912.4264 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7643.9947 - val_loss: 15292.4346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7552.2230 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7858.4789 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7742.2296 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7501.2415 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7524.3359 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7548.9999 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7881.1221 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8028.7169 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7420.5071 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7686.6913 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7799.5013 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 8044.9348 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7855.8981 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7499.5641 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7794.5587 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7696.7465 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7438.8424 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7439.5076 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7693.3966 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7621.3874 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7793.4244 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7559.7422 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7744.4356 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7454.6947 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7785.7379 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7533.3743 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7873.4559 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7381.9455 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7920.2812 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7781.3548 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7506.1701 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7639.2555 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7713.8422 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7893.6140 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7475.9129 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7809.0048 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7695.6734 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: 27.6681 - val_loss: 2.5262\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.7333 - val_loss: 2.7830\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 10.2500 - val_loss: 7.2006\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 18.1133 - val_loss: 9.0366\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 8.9570 - val_loss: 15.9602\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 14.7424 - val_loss: 18.7527\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 21.1006 - val_loss: 11.3436\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 21.6528 - val_loss: 28.4086\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 23.3070 - val_loss: 10.3200\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 22.5984 - val_loss: 33.1808\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 21.9012 - val_loss: 9.8097\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 6.7673 - val_loss: 7.5744\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.6877 - val_loss: 6.7608\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.2927 - val_loss: 10.7998\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3583 - val_loss: 9.8363\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3572 - val_loss: 6.1631\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 5.2797 - val_loss: 7.4755\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.6450 - val_loss: 7.6466\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 5.3994 - val_loss: 5.9857\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.5371 - val_loss: 6.0864\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.2744 - val_loss: 5.9006\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.6978 - val_loss: 6.6887\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.5652 - val_loss: 5.9895\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.6067 - val_loss: 5.9565\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.6056 - val_loss: 6.0661\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.6446 - val_loss: 6.0521\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.6308 - val_loss: 6.0311\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.6200 - val_loss: 6.0177\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.6354 - val_loss: 5.9159\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.5882 - val_loss: 5.9124\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.5679 - val_loss: 5.9735\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.5734 - val_loss: 5.9400\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.5415 - val_loss: 5.9327\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.5880 - val_loss: 5.9385\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.5806 - val_loss: 5.9379\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.5615 - val_loss: 5.9395\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.6211 - val_loss: 5.9748\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.6048 - val_loss: 5.9768\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.5248 - val_loss: 5.9469\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.5439 - val_loss: 5.9481\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.6222 - val_loss: 5.9619\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.5519 - val_loss: 5.9623\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.6155 - val_loss: 5.9624\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.6227 - val_loss: 5.9631\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.5148 - val_loss: 5.9630\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.5921 - val_loss: 5.9631\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.5631 - val_loss: 5.9635\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.5656 - val_loss: 5.9631\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.5687 - val_loss: 5.9625\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.5711 - val_loss: 5.9628\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 1915.3190 - val_loss: 1909.7145\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 943.9831 - val_loss: 1913.8260\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 962.0782 - val_loss: 2062.4514\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 938.8345 - val_loss: 1904.5071\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 870.8758 - val_loss: 1777.4883\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 921.1905 - val_loss: 1797.5715\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 874.4855 - val_loss: 1778.7085\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 922.6790 - val_loss: 1915.6588\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 876.5396 - val_loss: 1990.0906\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 851.6227 - val_loss: 1953.2220\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 902.3893 - val_loss: 1707.3918\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 879.7249 - val_loss: 1719.9524\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 898.6021 - val_loss: 1989.8098\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 902.6128 - val_loss: 1943.4847\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 867.3262 - val_loss: 1837.5385\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 857.2694 - val_loss: 1898.5629\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 894.6602 - val_loss: 1901.5459\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 933.2609 - val_loss: 2004.1239\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 903.3532 - val_loss: 2544.9167\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 889.9908 - val_loss: 1933.9672\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 910.1801 - val_loss: 1892.2743\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 855.3854 - val_loss: 1813.4495\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 818.7691 - val_loss: 1832.7803\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 857.8327 - val_loss: 1853.9800\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 878.8259 - val_loss: 1828.9733\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 826.0050 - val_loss: 1839.1611\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 870.6232 - val_loss: 1861.5692\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 895.9151 - val_loss: 1853.4835\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 806.8793 - val_loss: 1817.5098\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 832.3557 - val_loss: 1825.4871\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 919.6311 - val_loss: 1840.0586\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 773.0969 - val_loss: 1839.8237\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 804.2186 - val_loss: 1842.4918\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 846.9153 - val_loss: 1841.5341\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 841.5798 - val_loss: 1844.2617\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 924.5626 - val_loss: 1844.1029\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 902.7962 - val_loss: 1845.5690\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 848.1166 - val_loss: 1842.3187\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 808.5864 - val_loss: 1840.6681\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 839.7525 - val_loss: 1841.8047\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 856.3889 - val_loss: 1841.9989\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 879.1665 - val_loss: 1841.9199\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 845.3960 - val_loss: 1841.8251\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 824.1867 - val_loss: 1841.9078\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 880.0894 - val_loss: 1841.8204\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 833.1688 - val_loss: 1841.6921\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 822.5346 - val_loss: 1841.7189\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 878.8132 - val_loss: 1841.7495\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 858.2334 - val_loss: 1842.0703\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 835.2339 - val_loss: 1841.9016\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.8327 - val_loss: 2.5727\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.1896 - val_loss: 2.6071\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1550 - val_loss: 2.5234\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1537 - val_loss: 2.5240\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1480 - val_loss: 2.5176\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1493 - val_loss: 2.5224\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1413 - val_loss: 2.5682\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1290 - val_loss: 2.5210\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1267 - val_loss: 2.5062\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1523 - val_loss: 2.5163\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1839 - val_loss: 2.5106\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2143 - val_loss: 2.5073\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1585 - val_loss: 2.5383\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.5045 - val_loss: 2.5373\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1355 - val_loss: 2.5181\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1437 - val_loss: 2.5971\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1847 - val_loss: 2.5443\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1173 - val_loss: 2.5062\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5014 - val_loss: 2.5272\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1013 - val_loss: 2.5051\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0920 - val_loss: 2.5001\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0840 - val_loss: 2.5047\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0945 - val_loss: 2.5075\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0923 - val_loss: 2.5095\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0945 - val_loss: 2.5018\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0834 - val_loss: 2.4964\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0924 - val_loss: 2.5024\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0882 - val_loss: 2.5012\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0873 - val_loss: 2.4949\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.0922 - val_loss: 2.5003\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0844 - val_loss: 2.5028\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0784 - val_loss: 2.4989\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0808 - val_loss: 2.5011\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.0872 - val_loss: 2.5023\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0761 - val_loss: 2.4953\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0813 - val_loss: 2.4945\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0889 - val_loss: 2.5006\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0803 - val_loss: 2.4967\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0828 - val_loss: 2.4904\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0790 - val_loss: 2.4920\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0815 - val_loss: 2.4901\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0741 - val_loss: 2.4939\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.0802 - val_loss: 2.4961\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0766 - val_loss: 2.4940\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0811 - val_loss: 2.4900\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0774 - val_loss: 2.4931\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0717 - val_loss: 2.4889\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0761 - val_loss: 2.4873\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0820 - val_loss: 2.4931\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0683 - val_loss: 2.4921\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3081.7419 - val_loss: 2325.2915\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 983.2980 - val_loss: 2026.9869\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 919.0145 - val_loss: 1916.8395\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 944.2644 - val_loss: 1903.0138\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 879.9738 - val_loss: 1981.5491\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 858.3725 - val_loss: 1899.1964\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 851.5597 - val_loss: 1918.1284\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 860.1783 - val_loss: 1901.8928\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 855.7059 - val_loss: 1822.5754\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 903.5049 - val_loss: 1906.5934\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 877.4724 - val_loss: 1954.0195\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 940.8539 - val_loss: 1808.5294\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 926.7386 - val_loss: 1915.2079\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 900.3284 - val_loss: 1838.3806\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 860.7511 - val_loss: 1729.8615\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 908.8639 - val_loss: 1968.8668\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 864.3467 - val_loss: 2013.6691\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 848.2785 - val_loss: 1797.3809\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 909.7507 - val_loss: 1819.1884\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 852.9324 - val_loss: 1792.7672\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 839.8228 - val_loss: 1791.5367\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 825.8048 - val_loss: 1794.4166\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 847.3085 - val_loss: 1897.2913\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 898.2440 - val_loss: 1964.3080\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 907.3178 - val_loss: 1829.6625\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 879.6077 - val_loss: 1847.0690\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 871.8373 - val_loss: 1856.0662\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 858.8805 - val_loss: 1854.8969\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 860.1391 - val_loss: 1852.6995\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 879.0259 - val_loss: 1866.5448\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 864.6395 - val_loss: 1859.0690\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 923.3273 - val_loss: 1884.1072\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 816.1908 - val_loss: 1840.5438\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 861.4723 - val_loss: 1849.4762\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 845.4409 - val_loss: 1849.3927\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 834.6141 - val_loss: 1846.7524\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 820.1862 - val_loss: 1844.9651\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 855.6339 - val_loss: 1845.3439\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 870.6252 - val_loss: 1845.6045\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 807.6410 - val_loss: 1844.5312\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 843.3042 - val_loss: 1844.6185\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 858.9623 - val_loss: 1844.3960\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 823.1233 - val_loss: 1843.0853\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 862.2687 - val_loss: 1844.0840\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 841.6141 - val_loss: 1843.5620\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 822.2630 - val_loss: 1843.4718\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 831.8368 - val_loss: 1843.4697\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 869.1981 - val_loss: 1843.5015\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 882.1425 - val_loss: 1843.4639\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 835.6603 - val_loss: 1843.4891\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 4.2408 - val_loss: 4.9386\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1768 - val_loss: 4.9147\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1654 - val_loss: 4.8679\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1210 - val_loss: 4.8478\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0406 - val_loss: 4.8271\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0534 - val_loss: 4.8497\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0349 - val_loss: 4.8145\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0624 - val_loss: 4.8196\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0174 - val_loss: 4.8072\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0242 - val_loss: 4.8027\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0362 - val_loss: 4.8084\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0549 - val_loss: 4.8015\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0329 - val_loss: 4.8107\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0185 - val_loss: 4.8005\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0166 - val_loss: 4.7976\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0290 - val_loss: 4.7975\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0110 - val_loss: 4.8020\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0222 - val_loss: 4.7975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0424 - val_loss: 4.7947\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0021 - val_loss: 4.7942\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0227 - val_loss: 4.7928\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0451 - val_loss: 4.7941\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0420 - val_loss: 4.7953\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0270 - val_loss: 4.7965\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9867 - val_loss: 4.7969\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0280 - val_loss: 4.7951\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0209 - val_loss: 4.7941\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0003 - val_loss: 4.7941\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0251 - val_loss: 4.7895\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0306 - val_loss: 4.7899\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0227 - val_loss: 4.7990\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9996 - val_loss: 4.7899\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0310 - val_loss: 4.7898\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0354 - val_loss: 4.7925\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0225 - val_loss: 4.7944\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0101 - val_loss: 4.7903\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0171 - val_loss: 4.7892\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0330 - val_loss: 4.7945\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0006 - val_loss: 4.7893\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0206 - val_loss: 4.7905\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0348 - val_loss: 4.7905\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0229 - val_loss: 4.7920\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0268 - val_loss: 4.7902\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0120 - val_loss: 4.8209\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0279 - val_loss: 4.7913\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0255 - val_loss: 4.7935\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9956 - val_loss: 4.7899\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9562 - val_loss: 4.7149\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9646 - val_loss: 4.7145\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9412 - val_loss: 4.7148\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7467.9575 - val_loss: 15292.8066\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7716.9320 - val_loss: 15292.7490\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7951.7863 - val_loss: 15292.7285\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7638.1598 - val_loss: 15292.7012\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7721.1614 - val_loss: 15292.6875\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7381.0118 - val_loss: 15292.6631\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7419.7455 - val_loss: 15292.6426\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8184.6041 - val_loss: 15292.6240\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7310.6680 - val_loss: 15292.6084\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7944.1019 - val_loss: 15292.5986\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7597.7572 - val_loss: 15292.6025\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7824.8695 - val_loss: 15292.5996\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7286.3386 - val_loss: 15292.5898\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7902.2472 - val_loss: 15292.5996\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7589.7374 - val_loss: 15292.5811\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7670.0314 - val_loss: 15292.5752\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7737.9805 - val_loss: 15292.5713\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7886.1513 - val_loss: 15292.5684\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7500.0497 - val_loss: 15292.5732\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7594.2386 - val_loss: 15292.5615\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7762.6872 - val_loss: 15292.5596\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7952.2145 - val_loss: 15292.5596\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7712.9612 - val_loss: 15292.5527\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7472.3597 - val_loss: 15292.5566\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7469.9034 - val_loss: 15292.5518\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7636.1553 - val_loss: 15292.5518\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7582.7434 - val_loss: 15292.5518\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7581.0274 - val_loss: 15292.5488\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7554.6284 - val_loss: 15292.5498\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7454.4808 - val_loss: 15292.5488\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7692.1285 - val_loss: 15292.5469\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 8327.4334 - val_loss: 15292.5430\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8030.0802 - val_loss: 15292.5400\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7692.4005 - val_loss: 15292.5439\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7434.9223 - val_loss: 15292.5430\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7219.9955 - val_loss: 15292.5381\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7269.0066 - val_loss: 15292.5410\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7870.3408 - val_loss: 15292.5391\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7456.9891 - val_loss: 15292.5381\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7792.2266 - val_loss: 15292.5361\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7488.3530 - val_loss: 15292.5381\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7815.7646 - val_loss: 15292.5361\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7441.7096 - val_loss: 15292.5361\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7342.1096 - val_loss: 15292.5352\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 7124.1975 - val_loss: 15292.5352\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8073.3106 - val_loss: 15292.5361\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7673.6214 - val_loss: 15292.5361\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7853.7424 - val_loss: 15292.5332\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7727.2078 - val_loss: 15292.5312\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7566.0481 - val_loss: 15292.5312\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 4.2680 - val_loss: 4.9288\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1295 - val_loss: 4.9042\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1099 - val_loss: 4.8847\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1003 - val_loss: 4.8663\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0670 - val_loss: 4.8561\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0682 - val_loss: 4.8402\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0483 - val_loss: 4.8359\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0712 - val_loss: 4.8242\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0438 - val_loss: 4.8147\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0226 - val_loss: 4.8092\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0388 - val_loss: 4.8041\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0391 - val_loss: 4.7983\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9898 - val_loss: 4.8018\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0403 - val_loss: 4.7941\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0002 - val_loss: 4.7874\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0065 - val_loss: 4.7827\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0388 - val_loss: 4.7802\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0002 - val_loss: 4.7812\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0117 - val_loss: 4.7802\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9903 - val_loss: 4.7746\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9971 - val_loss: 4.7707\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0055 - val_loss: 4.7694\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9876 - val_loss: 4.7677\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9916 - val_loss: 4.7661\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9882 - val_loss: 4.7648\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9874 - val_loss: 4.7686\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9641 - val_loss: 4.7606\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9902 - val_loss: 4.7596\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9602 - val_loss: 4.7578\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9802 - val_loss: 4.7566\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9533 - val_loss: 4.7553\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9748 - val_loss: 4.7552\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9775 - val_loss: 4.7546\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9826 - val_loss: 4.7528\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9981 - val_loss: 4.7544\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9695 - val_loss: 4.7522\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0260 - val_loss: 4.7545\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0011 - val_loss: 4.7525\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9817 - val_loss: 4.7501\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9737 - val_loss: 4.7469\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9655 - val_loss: 4.7475\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9744 - val_loss: 4.7530\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9970 - val_loss: 4.7446\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9657 - val_loss: 4.7488\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9727 - val_loss: 4.7491\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9593 - val_loss: 4.7445\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9719 - val_loss: 4.7438\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9719 - val_loss: 4.7439\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9520 - val_loss: 4.7441\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9711 - val_loss: 4.7414\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 7616.3341 - val_loss: 15292.9805\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7595.1807 - val_loss: 15292.9033\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7780.2039 - val_loss: 15292.8535\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7661.0255 - val_loss: 15292.8242\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7290.6908 - val_loss: 15292.7832\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7908.7827 - val_loss: 15292.7422\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7871.5858 - val_loss: 15292.7305\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7605.6316 - val_loss: 15292.7031\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7593.1782 - val_loss: 15292.6865\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7820.0742 - val_loss: 15292.6777\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7403.9077 - val_loss: 15292.6562\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7565.2632 - val_loss: 15292.6475\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7760.4581 - val_loss: 15292.6367\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7785.4185 - val_loss: 15292.6318\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8034.7121 - val_loss: 15292.6172\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7650.1908 - val_loss: 15292.6133\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7443.0450 - val_loss: 15292.6016\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7269.0500 - val_loss: 15292.5967\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7570.5263 - val_loss: 15292.5957\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7308.4266 - val_loss: 15292.5889\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7485.4585 - val_loss: 15292.5869\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 7554.6873 - val_loss: 15292.5752\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7235.7504 - val_loss: 15292.5732\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7637.9645 - val_loss: 15292.5723\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7553.6484 - val_loss: 15292.5771\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7660.5600 - val_loss: 15292.5664\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7384.1144 - val_loss: 15292.5586\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7212.3591 - val_loss: 15292.5566\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7439.0483 - val_loss: 15292.5498\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7648.7922 - val_loss: 15292.5498\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7874.7149 - val_loss: 15292.5469\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7825.4227 - val_loss: 15292.5488\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7951.6859 - val_loss: 15292.5488\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7553.5745 - val_loss: 15292.5391\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7919.2777 - val_loss: 15292.5381\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7599.1891 - val_loss: 15292.5361\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7566.9859 - val_loss: 15292.5381\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7529.3212 - val_loss: 15292.5371\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7751.0336 - val_loss: 15292.5332\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7886.3475 - val_loss: 15292.5293\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7592.2608 - val_loss: 15292.5303\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7584.4240 - val_loss: 15292.5264\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7553.8870 - val_loss: 15292.5273\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7763.8874 - val_loss: 15292.5205\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7303.8341 - val_loss: 15292.5215\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7532.4446 - val_loss: 15292.5283\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7751.8572 - val_loss: 15292.5264\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7871.9817 - val_loss: 15292.5156\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7531.2132 - val_loss: 15292.5186\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8058.1635 - val_loss: 15292.5156\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 30.6143\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 25.5096 - val_loss: 25.3871\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 21.6875 - val_loss: 22.4269\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 19.2061 - val_loss: 20.3934\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 17.6227 - val_loss: 18.8712\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 16.3152 - val_loss: 17.6708\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 15.3514 - val_loss: 16.6954\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 14.4002 - val_loss: 15.8861\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 13.6767 - val_loss: 15.2033\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 13.1192 - val_loss: 14.6187\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: 28497.3184\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13903.9525 - val_loss: 26013.7207\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 12806.0498 - val_loss: 24581.1465\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 12016.4031 - val_loss: 23588.9277\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11880.3507 - val_loss: 22843.3652\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11408.9678 - val_loss: 22256.8770\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 11986.4482 - val_loss: 21781.8516\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 11091.9633 - val_loss: 21387.8887\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 11339.1517 - val_loss: 21057.0332\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11012.4174 - val_loss: 20772.5508\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 27.3702 - val_loss: 3.7499\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.8497 - val_loss: 2.9351\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7.9789 - val_loss: 19.1176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.5437 - val_loss: 2.9090\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.5173 - val_loss: 3.0028\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0784 - val_loss: 20.4885\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7.1781 - val_loss: 2.9179\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.2466 - val_loss: 2.9236\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.4171 - val_loss: 2.9761\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1962 - val_loss: 22.0780\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.5552 - val_loss: 2.9450\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.8150 - val_loss: 3.0001\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9388 - val_loss: 2.8554\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.1059 - val_loss: 2.9503\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.4878 - val_loss: 15.3692\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.8147 - val_loss: 2.8742\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.4104 - val_loss: 3.1920\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0398 - val_loss: 24.5254\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.7941 - val_loss: 5.6478\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.3936 - val_loss: 2.9786\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.3681 - val_loss: 35.1635\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.9090 - val_loss: 4.8863\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.1679 - val_loss: 3.2921\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.5334 - val_loss: 2.5080\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1512 - val_loss: 2.5225\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1552 - val_loss: 2.4972\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1631 - val_loss: 2.5763\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1345 - val_loss: 2.5511\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1205 - val_loss: 2.5330\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1237 - val_loss: 2.5348\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1272 - val_loss: 2.6223\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1387 - val_loss: 2.6106\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1175 - val_loss: 2.4891\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1025 - val_loss: 2.5019\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1034 - val_loss: 2.4846\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.1044 - val_loss: 2.4978\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1089 - val_loss: 2.4957\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1065 - val_loss: 2.4843\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1045 - val_loss: 2.5419\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1194 - val_loss: 2.4866\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1070 - val_loss: 2.4859\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1254 - val_loss: 2.5401\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1172 - val_loss: 2.5748\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1085 - val_loss: 2.5187\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1048 - val_loss: 2.5250\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1058 - val_loss: 2.4784\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1152 - val_loss: 2.4819\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0973 - val_loss: 2.4756\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1087 - val_loss: 2.4734\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.1119 - val_loss: 2.5022\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 1549.6919 - val_loss: 2053.5515\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 1001.3142 - val_loss: 1913.2833\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 928.2160 - val_loss: 2010.3135\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 933.6032 - val_loss: 2012.6053\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 892.7163 - val_loss: 1864.7627\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 884.0791 - val_loss: 1886.8873\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 872.5280 - val_loss: 1784.7379\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 931.9225 - val_loss: 2000.0791\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 858.4593 - val_loss: 1829.1143\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 893.9383 - val_loss: 1953.4158\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 876.9408 - val_loss: 1808.2625\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 912.4869 - val_loss: 1790.1357\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 906.9515 - val_loss: 1888.3285\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 921.4367 - val_loss: 1900.3027\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 853.6361 - val_loss: 1790.5192\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 878.4096 - val_loss: 2027.2465\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 944.5573 - val_loss: 1879.2332\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 844.5635 - val_loss: 1843.5829\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 905.5038 - val_loss: 1820.0292\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.9251 - val_loss: 1846.9811\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 851.1197 - val_loss: 1825.1310\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 898.0233 - val_loss: 1835.8226\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 848.6756 - val_loss: 1826.4254\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 853.4347 - val_loss: 1833.2644\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 845.4138 - val_loss: 1848.6597\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 875.2990 - val_loss: 1861.5551\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 828.4141 - val_loss: 1838.0317\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 839.7807 - val_loss: 1843.5544\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 849.4758 - val_loss: 1843.7368\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 829.3900 - val_loss: 1843.9918\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 902.1015 - val_loss: 1843.5812\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 850.5334 - val_loss: 1843.5524\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 867.0754 - val_loss: 1843.0880\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 876.1012 - val_loss: 1846.1118\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 864.6303 - val_loss: 1844.8203\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.6070 - val_loss: 1844.1495\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 874.4923 - val_loss: 1844.5367\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 864.9947 - val_loss: 1844.6921\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 793.5148 - val_loss: 1844.7887\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 883.1912 - val_loss: 1844.7614\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 847.8166 - val_loss: 1844.9375\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 886.4388 - val_loss: 1844.8771\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 939.6060 - val_loss: 1845.0822\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 838.6125 - val_loss: 1845.0681\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 878.9691 - val_loss: 1845.0234\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 822.8161 - val_loss: 1845.0234\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 859.7264 - val_loss: 1845.0282\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 830.0394 - val_loss: 1845.0195\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 822.6731 - val_loss: 1845.0132\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.4713 - val_loss: 1844.9984\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.2747 - val_loss: 2.5862\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1633 - val_loss: 2.4996\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.1119 - val_loss: 2.5058\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1035 - val_loss: 2.4904\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0864 - val_loss: 2.4849\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0833 - val_loss: 2.4856\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0910 - val_loss: 2.5101\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0929 - val_loss: 2.4874\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0884 - val_loss: 2.4837\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0843 - val_loss: 2.4788\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0904 - val_loss: 2.4951\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0982 - val_loss: 2.4731\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1018 - val_loss: 2.4743\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0925 - val_loss: 2.5202\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0865 - val_loss: 2.4718\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0984 - val_loss: 2.5073\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0920 - val_loss: 2.4878\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0890 - val_loss: 2.4780\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0909 - val_loss: 2.4641\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0905 - val_loss: 2.4642\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0921 - val_loss: 2.5010\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0995 - val_loss: 2.4893\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0866 - val_loss: 2.5251\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0929 - val_loss: 2.4888\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1047 - val_loss: 2.4774\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0903 - val_loss: 2.4849\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0853 - val_loss: 2.6428\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0984 - val_loss: 2.4686\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0924 - val_loss: 2.5090\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0663 - val_loss: 2.4629\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0558 - val_loss: 2.4581\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0608 - val_loss: 2.4548\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0612 - val_loss: 2.4535\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0572 - val_loss: 2.4557\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0539 - val_loss: 2.4547\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0573 - val_loss: 2.4562\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0510 - val_loss: 2.4542\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0546 - val_loss: 2.4537\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0556 - val_loss: 2.4619\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0517 - val_loss: 2.4544\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0490 - val_loss: 2.4572\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0528 - val_loss: 2.4562\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0582 - val_loss: 2.4585\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0605 - val_loss: 2.4584\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0526 - val_loss: 2.4586\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0479 - val_loss: 2.4547\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0543 - val_loss: 2.4563\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0502 - val_loss: 2.4554\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0550 - val_loss: 2.4569\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0513 - val_loss: 2.4565\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 3598.6030 - val_loss: 2394.1272\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 1046.0162 - val_loss: 2359.7437\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 924.5848 - val_loss: 1860.6689\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 901.1291 - val_loss: 2016.5895\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 903.6780 - val_loss: 1818.9524\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 831.9689 - val_loss: 1973.8601\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 880.1013 - val_loss: 1850.7075\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 862.4678 - val_loss: 1788.0065\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 843.9701 - val_loss: 1860.7069\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 853.3510 - val_loss: 1872.2874\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 856.4624 - val_loss: 1834.5806\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.0862 - val_loss: 1840.6304\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 847.3768 - val_loss: 1860.5690\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 837.8470 - val_loss: 1805.6633\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 905.4352 - val_loss: 2003.9453\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 919.1437 - val_loss: 2026.5636\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 882.6678 - val_loss: 1836.3538\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 862.5077 - val_loss: 1726.5005\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 868.2337 - val_loss: 1846.0182\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 908.9541 - val_loss: 1956.8961\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 876.3969 - val_loss: 1875.4644\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 866.5536 - val_loss: 1891.5461\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 907.0235 - val_loss: 1961.5865\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 866.6943 - val_loss: 1830.7731\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 825.1985 - val_loss: 1755.6575\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.6117 - val_loss: 1879.1278\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 842.7299 - val_loss: 1887.4932\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 878.0618 - val_loss: 1897.5493\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 846.9093 - val_loss: 1843.7273\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 873.5806 - val_loss: 1849.4569\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 827.0756 - val_loss: 1829.3827\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 820.1031 - val_loss: 1826.5822\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.5676 - val_loss: 1845.7950\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 842.5641 - val_loss: 1824.1317\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 860.7415 - val_loss: 1831.8475\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 887.7841 - val_loss: 1851.0752\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 808.0759 - val_loss: 1830.4766\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 874.5827 - val_loss: 1842.7882\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 824.3544 - val_loss: 1841.9725\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 852.0550 - val_loss: 1841.0022\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 902.1402 - val_loss: 1840.7363\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.5810 - val_loss: 1841.1492\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 795.8139 - val_loss: 1840.6732\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 832.5976 - val_loss: 1840.9512\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 855.9929 - val_loss: 1840.8433\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 853.2711 - val_loss: 1842.5657\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 858.1958 - val_loss: 1843.1473\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 866.3916 - val_loss: 1842.8209\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 897.1820 - val_loss: 1842.8480\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 905.0160 - val_loss: 1842.8242\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 4.0158 - val_loss: 4.7011\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9446 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9181 - val_loss: 4.7006\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9264 - val_loss: 4.7006\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9243 - val_loss: 4.7007\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9170 - val_loss: 4.7007\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9287 - val_loss: 4.7007\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9313 - val_loss: 4.7008\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9269 - val_loss: 4.7007\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9194 - val_loss: 4.7013\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9329 - val_loss: 4.7011\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9427 - val_loss: 4.7007\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9392 - val_loss: 4.7010\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9307 - val_loss: 4.7011\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9398 - val_loss: 4.7010\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9061 - val_loss: 4.7009\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9240 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9443 - val_loss: 4.7011\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9349 - val_loss: 4.7009\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9311 - val_loss: 4.7011\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9139 - val_loss: 4.7009\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9261 - val_loss: 4.7009\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9466 - val_loss: 4.7009\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9374 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9544 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9240 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9297 - val_loss: 4.7010\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9245 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9516 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9191 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9029 - val_loss: 4.7010\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9325 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9073 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9420 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9791 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9219 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9341 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9194 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9296 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9169 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9174 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9085 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9606 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9149 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9554 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9201 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9504 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9548 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9202 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9465 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 7609.0523 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7379.6595 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7677.9280 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7980.5265 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7616.4554 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7402.5919 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7016.3591 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7423.7279 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7689.6954 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7775.3151 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7432.7733 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7916.8257 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7635.5323 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7579.9618 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7884.7835 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7482.9517 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7869.9721 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7844.2064 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 7792.0636 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7227.7119 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7671.7807 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7412.5510 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7773.9825 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7450.4376 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7577.5536 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7126.0789 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7520.0147 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7893.6254 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7831.8432 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7300.6949 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7523.6208 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7905.0427 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7742.0810 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7708.5374 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7633.6017 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7639.2156 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7257.0368 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7825.5356 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7655.9316 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7502.1138 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7374.8686 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7524.8969 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7771.0041 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7717.8974 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7228.6086 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7472.9761 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7676.0864 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7381.8367 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7632.2428 - val_loss: 15292.4346\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: 7524.2899 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 4.0831 - val_loss: 4.7011\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9434 - val_loss: 4.7010\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9280 - val_loss: 4.7009\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9545 - val_loss: 4.7009\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9111 - val_loss: 4.7009\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9237 - val_loss: 4.7008\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9478 - val_loss: 4.7010\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9457 - val_loss: 4.7011\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9160 - val_loss: 4.7011\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9145 - val_loss: 4.7009\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9635 - val_loss: 4.7009\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9168 - val_loss: 4.7010\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9651 - val_loss: 4.7009\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9379 - val_loss: 4.7008\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9408 - val_loss: 4.7020\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9172 - val_loss: 4.7009\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9399 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9333 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9346 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9155 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9175 - val_loss: 4.7010\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9361 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9439 - val_loss: 4.7011\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9222 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9352 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9195 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9060 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.8925 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9237 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9412 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9412 - val_loss: 4.7010\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9392 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9184 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9398 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9247 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9071 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9401 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9284 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9384 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9309 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9359 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9349 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9515 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9491 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9367 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9465 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9063 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9402 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9297 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9588 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7348.4853 - val_loss: 15292.4414\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7778.8055 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7534.2803 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7454.0433 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7259.3190 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7388.0114 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7881.2921 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7646.4170 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7781.5003 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8214.5819 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7428.8676 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7667.9367 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7812.8588 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7428.3568 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7589.5160 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7447.5451 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7673.3216 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7811.6760 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7446.9436 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7501.3426 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7735.8097 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7849.1307 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 7370.8504 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: 7672.4118 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7358.5396 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7864.0485 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7571.1903 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7517.6146 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7793.3858 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7758.8637 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7755.5120 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7615.6269 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7667.5718 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7560.7963 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7564.9177 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7748.0584 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7578.8477 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7663.0088 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8054.8387 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7581.4162 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7533.6002 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7542.4007 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7269.5444 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7561.1812 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7788.9093 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7767.7205 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7432.4465 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7517.6825 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7589.2725 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7596.1088 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 22.4626 - val_loss: 3.1622\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.3945 - val_loss: 24.2793\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 11.9762 - val_loss: 6.0325\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 12.6814 - val_loss: 8.1481\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 19.3791 - val_loss: 7.0612\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 15.5137 - val_loss: 10.7287\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 18.1325 - val_loss: 9.2133\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 18.4008 - val_loss: 7.2100\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 18.3649 - val_loss: 22.3408\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 20.7082 - val_loss: 6.8314\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 19.8224 - val_loss: 22.2193\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 10.7611 - val_loss: 7.1295\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.5351 - val_loss: 8.8458\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.4257 - val_loss: 4.5301\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9625 - val_loss: 10.0693\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.7735 - val_loss: 5.1952\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9381 - val_loss: 7.7339\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7440 - val_loss: 4.5460\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.6383 - val_loss: 7.3636\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.8066 - val_loss: 4.2073\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.5470 - val_loss: 6.5155\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.5350 - val_loss: 4.2636\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.2762 - val_loss: 4.1957\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.2961 - val_loss: 4.1977\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.2910 - val_loss: 4.1963\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.2761 - val_loss: 4.1933\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.2478 - val_loss: 4.1927\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.2942 - val_loss: 4.1910\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.2753 - val_loss: 4.2028\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.3235 - val_loss: 4.1991\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.3053 - val_loss: 4.2050\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.2732 - val_loss: 4.1988\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.2673 - val_loss: 4.1970\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.2983 - val_loss: 4.1993\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.2752 - val_loss: 4.1982\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.2937 - val_loss: 4.1975\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.2765 - val_loss: 4.1967\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.2843 - val_loss: 4.1946\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.2808 - val_loss: 4.1970\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.3048 - val_loss: 4.1977\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.2731 - val_loss: 4.1971\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.2744 - val_loss: 4.1971\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.2814 - val_loss: 4.1971\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.2873 - val_loss: 4.1971\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.2830 - val_loss: 4.1972\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.2973 - val_loss: 4.1972\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.2894 - val_loss: 4.1972\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.2659 - val_loss: 4.1973\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.3444 - val_loss: 4.1974\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.2844 - val_loss: 4.1974\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 1729.3336 - val_loss: 2059.2009\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 954.2664 - val_loss: 2018.5198\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 925.5705 - val_loss: 1838.5834\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 860.5501 - val_loss: 1897.8737\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 849.4890 - val_loss: 1896.9193\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 931.3758 - val_loss: 2092.9951\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 874.3508 - val_loss: 1921.1409\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 891.5403 - val_loss: 2002.0846\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 927.1478 - val_loss: 1892.7712\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 882.6393 - val_loss: 1874.0970\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 904.7830 - val_loss: 2048.3650\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 896.8733 - val_loss: 1869.7900\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 903.7181 - val_loss: 1794.0691\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 883.6687 - val_loss: 1894.2114\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 878.0066 - val_loss: 3341.2654\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 900.8831 - val_loss: 1894.0125\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 868.4989 - val_loss: 1910.6049\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 876.8974 - val_loss: 1794.3817\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 873.4881 - val_loss: 1792.9017\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 872.8696 - val_loss: 1889.7404\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 869.9527 - val_loss: 1695.5389\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 890.0844 - val_loss: 1843.4376\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 943.2770 - val_loss: 1786.7803\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 893.2405 - val_loss: 1870.1605\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 917.6199 - val_loss: 1975.1587\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 870.9968 - val_loss: 1828.9198\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 887.9249 - val_loss: 1839.8223\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 850.1046 - val_loss: 1924.0951\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 856.1759 - val_loss: 1877.2792\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 868.5343 - val_loss: 1846.7396\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 905.1686 - val_loss: 1811.2279\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 868.4533 - val_loss: 1871.5729\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 872.3526 - val_loss: 1836.2341\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 827.8969 - val_loss: 1805.4041\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 844.9695 - val_loss: 1812.0188\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 862.3871 - val_loss: 1843.9663\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 821.9073 - val_loss: 1813.0280\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 830.3930 - val_loss: 1813.5120\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 832.4493 - val_loss: 1818.3751\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 855.9773 - val_loss: 1836.7800\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 834.8884 - val_loss: 1839.5143\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 859.5163 - val_loss: 1830.7307\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 854.2507 - val_loss: 1827.3274\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 821.2458 - val_loss: 1828.5162\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 810.3753 - val_loss: 1825.9071\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 839.9862 - val_loss: 1827.2532\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 839.3665 - val_loss: 1831.7052\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 824.8693 - val_loss: 1828.4342\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 819.3377 - val_loss: 1827.5479\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 887.5293 - val_loss: 1829.3607\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9675 - val_loss: 3.0564\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3479 - val_loss: 2.6151\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2219 - val_loss: 2.5765\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2110 - val_loss: 2.5639\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1794 - val_loss: 2.5529\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1697 - val_loss: 2.5736\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1711 - val_loss: 2.5360\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1561 - val_loss: 2.5459\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1500 - val_loss: 2.5766\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1495 - val_loss: 2.5392\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1666 - val_loss: 2.5481\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1390 - val_loss: 2.5474\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1351 - val_loss: 2.5124\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1360 - val_loss: 2.5216\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2893 - val_loss: 2.8055\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1568 - val_loss: 2.5715\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3227 - val_loss: 2.5295\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2498 - val_loss: 2.5395\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1375 - val_loss: 2.5418\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.1385 - val_loss: 2.5492\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.1682 - val_loss: 2.5167\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2381 - val_loss: 2.5515\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2484 - val_loss: 2.5470\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0998 - val_loss: 2.5093\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1093 - val_loss: 2.5098\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1005 - val_loss: 2.5051\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0989 - val_loss: 2.5048\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1032 - val_loss: 2.5122\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0996 - val_loss: 2.5057\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1016 - val_loss: 2.5080\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1034 - val_loss: 2.5036\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1018 - val_loss: 2.5134\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0953 - val_loss: 2.5092\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0954 - val_loss: 2.5077\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1031 - val_loss: 2.5113\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0960 - val_loss: 2.5040\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0947 - val_loss: 2.5032\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0894 - val_loss: 2.5068\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0943 - val_loss: 2.5053\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0859 - val_loss: 2.5041\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1020 - val_loss: 2.5047\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1061 - val_loss: 2.5040\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0989 - val_loss: 2.5050\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0892 - val_loss: 2.4978\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0939 - val_loss: 2.5034\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0945 - val_loss: 2.5033\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0943 - val_loss: 2.5024\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.0900 - val_loss: 2.4981\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0894 - val_loss: 2.4953\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.0869 - val_loss: 2.4969\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 2908.3294 - val_loss: 2502.9050\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 1124.6985 - val_loss: 2195.1050\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 943.1188 - val_loss: 2023.4332\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 876.0693 - val_loss: 2040.6495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 851.0327 - val_loss: 1820.2855\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 872.2908 - val_loss: 1982.7704\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 888.5772 - val_loss: 1857.2848\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 865.2266 - val_loss: 2002.3759\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 885.6519 - val_loss: 1988.7139\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 864.2860 - val_loss: 1801.5206\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 886.4909 - val_loss: 2186.3835\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 851.7678 - val_loss: 1865.5811\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 897.4632 - val_loss: 1848.1715\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 916.1170 - val_loss: 1985.1478\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 867.0468 - val_loss: 1913.1559\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 870.9070 - val_loss: 1884.2588\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 830.8397 - val_loss: 1790.6777\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 863.4899 - val_loss: 1789.8910\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 868.9097 - val_loss: 1853.0116\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 890.6577 - val_loss: 1804.3414\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 905.5109 - val_loss: 1912.6741\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 921.3505 - val_loss: 1837.1050\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 840.2812 - val_loss: 1798.8027\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 899.1663 - val_loss: 1950.6639\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 882.8922 - val_loss: 1838.8181\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 855.5659 - val_loss: 1895.9769\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 939.7428 - val_loss: 1867.2047\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 894.8505 - val_loss: 1851.9236\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 830.8846 - val_loss: 1845.5560\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 821.1742 - val_loss: 1829.0900\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 873.5456 - val_loss: 1845.6578\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 868.1745 - val_loss: 1819.9277\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 861.2876 - val_loss: 1856.1605\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 853.8241 - val_loss: 1836.8207\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 842.1133 - val_loss: 1828.8689\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 804.4158 - val_loss: 1834.8043\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 848.7046 - val_loss: 1829.4146\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 887.5556 - val_loss: 1860.1193\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 856.3165 - val_loss: 1854.0939\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 840.1349 - val_loss: 1851.5496\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 855.9135 - val_loss: 1849.8459\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 850.0206 - val_loss: 1848.8259\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 869.1441 - val_loss: 1848.9856\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 873.6257 - val_loss: 1847.8145\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 856.3673 - val_loss: 1846.7377\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 888.5648 - val_loss: 1846.3983\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 860.8730 - val_loss: 1845.2697\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 814.0652 - val_loss: 1844.5802\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 821.7244 - val_loss: 1844.5647\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 809.8605 - val_loss: 1844.5623\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.2181 - val_loss: 4.9340\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.1599 - val_loss: 4.9340\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1563 - val_loss: 4.8707\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0944 - val_loss: 4.8580\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0556 - val_loss: 4.8474\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0436 - val_loss: 4.8212\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0409 - val_loss: 4.8150\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0386 - val_loss: 4.8085\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0381 - val_loss: 4.8178\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0433 - val_loss: 4.8100\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0892 - val_loss: 4.8108\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0264 - val_loss: 4.8074\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0149 - val_loss: 4.7997\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0326 - val_loss: 4.8035\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0251 - val_loss: 4.7951\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0216 - val_loss: 4.7963\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0230 - val_loss: 4.8010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0098 - val_loss: 4.7946\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0400 - val_loss: 4.7933\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0081 - val_loss: 4.7939\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0118 - val_loss: 4.7922\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0096 - val_loss: 4.7924\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0151 - val_loss: 4.7932\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0333 - val_loss: 4.7919\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0181 - val_loss: 4.7922\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0025 - val_loss: 4.8004\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0289 - val_loss: 4.7919\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0350 - val_loss: 4.7937\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0160 - val_loss: 4.7919\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9965 - val_loss: 4.7896\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9932 - val_loss: 4.7954\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0173 - val_loss: 4.7942\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0115 - val_loss: 4.8163\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0237 - val_loss: 4.7896\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0213 - val_loss: 4.7888\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0237 - val_loss: 4.7911\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0303 - val_loss: 4.7942\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0150 - val_loss: 4.7902\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0392 - val_loss: 4.7895\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0287 - val_loss: 4.7884\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9952 - val_loss: 4.7895\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0113 - val_loss: 4.7886\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0133 - val_loss: 4.7904\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9978 - val_loss: 4.7899\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0282 - val_loss: 4.7903\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0260 - val_loss: 4.7924\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0296 - val_loss: 4.7903\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0098 - val_loss: 4.7878\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0057 - val_loss: 4.7904\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0106 - val_loss: 4.7899\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7701.1242 - val_loss: 15292.8672\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7529.0303 - val_loss: 15292.7529\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7334.4165 - val_loss: 15292.7393\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7756.8148 - val_loss: 15292.7236\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7516.0708 - val_loss: 15292.6982\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7743.8504 - val_loss: 15292.6758\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7578.6433 - val_loss: 15292.6611\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7843.5889 - val_loss: 15292.6396\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7966.7145 - val_loss: 15292.6221\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 8157.7182 - val_loss: 15292.6182\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7536.4323 - val_loss: 15292.6113\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7811.9292 - val_loss: 15292.6055\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7787.3592 - val_loss: 15292.5928\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7862.6009 - val_loss: 15292.5918\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7583.0471 - val_loss: 15292.5850\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7597.4002 - val_loss: 15292.5820\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7844.6247 - val_loss: 15292.5801\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7658.7098 - val_loss: 15292.5654\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7932.9930 - val_loss: 15292.5674\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8012.9780 - val_loss: 15292.5664\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7396.5957 - val_loss: 15292.5635\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7767.6381 - val_loss: 15292.5596\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7810.4868 - val_loss: 15292.5596\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7518.3066 - val_loss: 15292.5596\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7594.8850 - val_loss: 15292.5596\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7691.6127 - val_loss: 15292.5566\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7454.2652 - val_loss: 15292.5566\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7637.2561 - val_loss: 15292.5547\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7409.4946 - val_loss: 15292.5527\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8042.1182 - val_loss: 15292.5488\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8214.5531 - val_loss: 15292.5488\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8137.1190 - val_loss: 15292.5488\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7302.5488 - val_loss: 15292.5439\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7583.2590 - val_loss: 15292.5439\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7270.5315 - val_loss: 15292.5391\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7523.0677 - val_loss: 15292.5410\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7295.5542 - val_loss: 15292.5430\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7437.2961 - val_loss: 15292.5391\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7476.0813 - val_loss: 15292.5439\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7780.4079 - val_loss: 15292.5381\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7751.7138 - val_loss: 15292.5381\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7861.2863 - val_loss: 15292.5381\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7895.9725 - val_loss: 15292.5381\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7456.4180 - val_loss: 15292.5361\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7781.2446 - val_loss: 15292.5352\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7601.3472 - val_loss: 15292.5352\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7916.8534 - val_loss: 15292.5352\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7811.8402 - val_loss: 15292.5342\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7785.6220 - val_loss: 15292.5352\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7708.6417 - val_loss: 15292.5332\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.2886 - val_loss: 4.9303\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.1412 - val_loss: 4.9027\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1023 - val_loss: 4.8879\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0948 - val_loss: 4.8747\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0738 - val_loss: 4.8575\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0921 - val_loss: 4.8421\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0602 - val_loss: 4.8343\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0769 - val_loss: 4.8217\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0406 - val_loss: 4.8146\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0366 - val_loss: 4.8104\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0402 - val_loss: 4.8039\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0197 - val_loss: 4.7983\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0245 - val_loss: 4.8021\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0383 - val_loss: 4.7936\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0425 - val_loss: 4.7879\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0210 - val_loss: 4.7839\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0038 - val_loss: 4.7819\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9950 - val_loss: 4.7899\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0269 - val_loss: 4.7765\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0042 - val_loss: 4.7734\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9620 - val_loss: 4.7743\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9987 - val_loss: 4.7683\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0001 - val_loss: 4.7663\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9848 - val_loss: 4.7670\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9686 - val_loss: 4.7636\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0026 - val_loss: 4.7640\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9804 - val_loss: 4.7613\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9967 - val_loss: 4.7595\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0138 - val_loss: 4.7587\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0048 - val_loss: 4.7594\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9901 - val_loss: 4.7563\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9808 - val_loss: 4.7561\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9705 - val_loss: 4.7531\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9689 - val_loss: 4.7548\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9525 - val_loss: 4.7521\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9871 - val_loss: 4.7566\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9780 - val_loss: 4.7496\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0089 - val_loss: 4.7495\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9878 - val_loss: 4.7491\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9639 - val_loss: 4.7469\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9733 - val_loss: 4.7490\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9766 - val_loss: 4.7474\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9559 - val_loss: 4.7470\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9998 - val_loss: 4.7491\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9654 - val_loss: 4.7444\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9763 - val_loss: 4.7465\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9562 - val_loss: 4.7436\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9812 - val_loss: 4.7439\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9762 - val_loss: 4.7450\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9631 - val_loss: 4.7425\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 7581.4552 - val_loss: 15292.9785\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7757.9726 - val_loss: 15292.8975\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7303.0290 - val_loss: 15292.8525\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7613.2637 - val_loss: 15292.8086\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7905.2279 - val_loss: 15292.7842\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7410.5056 - val_loss: 15292.7432\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7717.5297 - val_loss: 15292.7324\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7766.9385 - val_loss: 15292.7051\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7591.8325 - val_loss: 15292.6885\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7668.9713 - val_loss: 15292.6699\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7272.9880 - val_loss: 15292.6689\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7612.3446 - val_loss: 15292.6455\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7799.8147 - val_loss: 15292.6387\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7564.5810 - val_loss: 15292.6377\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7860.9864 - val_loss: 15292.6201\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7706.4092 - val_loss: 15292.6221\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7931.0382 - val_loss: 15292.6045\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7521.4897 - val_loss: 15292.6006\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7724.7553 - val_loss: 15292.6016\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7755.9550 - val_loss: 15292.5889\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7843.8310 - val_loss: 15292.5918\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7479.0399 - val_loss: 15292.5752\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7373.7000 - val_loss: 15292.5771\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7856.1395 - val_loss: 15292.5723\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7545.0527 - val_loss: 15292.5674\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7522.7238 - val_loss: 15292.5645\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7660.6169 - val_loss: 15292.5615\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7704.5143 - val_loss: 15292.5537\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7924.2436 - val_loss: 15292.5518\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7389.1661 - val_loss: 15292.5469\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7576.5795 - val_loss: 15292.5518\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7249.4919 - val_loss: 15292.5469\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7944.2756 - val_loss: 15292.5430\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 7478.4764 - val_loss: 15292.5400\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7379.7824 - val_loss: 15292.5371\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7899.5739 - val_loss: 15292.5400\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7363.6225 - val_loss: 15292.5449\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7574.5584 - val_loss: 15292.5342\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7388.0508 - val_loss: 15292.5293\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7629.9089 - val_loss: 15292.5312\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7683.3351 - val_loss: 15292.5332\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7702.8631 - val_loss: 15292.5283\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7435.4330 - val_loss: 15292.5332\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7557.6118 - val_loss: 15292.5352\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7534.4880 - val_loss: 15292.5215\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7779.9817 - val_loss: 15292.5225\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7382.5754 - val_loss: 15292.5176\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7813.3148 - val_loss: 15292.5205\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7583.8397 - val_loss: 15292.5166\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7875.4536 - val_loss: 15292.5166\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 30.6202\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 25.6369 - val_loss: 25.3913\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 21.7516 - val_loss: 22.4301\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 19.2078 - val_loss: 20.3973\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 17.4666 - val_loss: 18.8718\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 16.3038 - val_loss: 17.6705\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 15.1914 - val_loss: 16.6952\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 14.3982 - val_loss: 15.8863\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 13.6574 - val_loss: 15.2048\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13.1704 - val_loss: 14.6198\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 28495.3379\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13889.3103 - val_loss: 26012.6699\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13624.0500 - val_loss: 24580.4238\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 12403.2069 - val_loss: 23588.3672\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11970.8814 - val_loss: 22842.7617\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11677.6150 - val_loss: 22257.4395\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11296.9247 - val_loss: 21782.0078\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 10945.3604 - val_loss: 21389.9570\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 10695.4440 - val_loss: 21057.0195\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 10756.0686 - val_loss: 20773.0039\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 18.5693 - val_loss: 4.9730\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 7.1472 - val_loss: 2.9156\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.2856 - val_loss: 13.8955\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.8688 - val_loss: 2.9682\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.2580 - val_loss: 4.0601\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.1719 - val_loss: 4.9740\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.8218 - val_loss: 5.9481\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 5.0119 - val_loss: 2.9126\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.5803 - val_loss: 3.0191\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.8839 - val_loss: 8.4912\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.5557 - val_loss: 2.8973\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.1825 - val_loss: 3.2581\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.3943 - val_loss: 2.9226\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.4520 - val_loss: 2.8941\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.3672 - val_loss: 10.6386\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.7740 - val_loss: 2.8777\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9656 - val_loss: 2.9965\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.5548 - val_loss: 2.8883\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 5.5236 - val_loss: 2.8750\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.2429 - val_loss: 16.8880\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8.7061 - val_loss: 3.5529\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.4367 - val_loss: 3.1657\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.9197 - val_loss: 3.3528\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.8180 - val_loss: 3.0403\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.3078 - val_loss: 2.9590\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.5390 - val_loss: 4.2437\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.6367 - val_loss: 37.9770\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.6239 - val_loss: 3.0436\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.9453 - val_loss: 25.8605\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.8276 - val_loss: 2.5251\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.2011 - val_loss: 2.4913\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1497 - val_loss: 2.5231\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1545 - val_loss: 2.7401\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1525 - val_loss: 2.5373\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1374 - val_loss: 2.6038\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1426 - val_loss: 2.4886\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1431 - val_loss: 2.4823\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.1238 - val_loss: 2.4872\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1223 - val_loss: 2.6117\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1305 - val_loss: 2.5116\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1212 - val_loss: 2.4970\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1064 - val_loss: 2.5114\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1114 - val_loss: 2.4978\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.1043 - val_loss: 2.5021\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1081 - val_loss: 2.5048\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1175 - val_loss: 2.4872\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1197 - val_loss: 2.4882\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0722 - val_loss: 2.4655\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0623 - val_loss: 2.4652\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0691 - val_loss: 2.4654\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 1775.8389 - val_loss: 2028.0452\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 976.8824 - val_loss: 1959.7081\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 914.4088 - val_loss: 1750.7231\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 925.2505 - val_loss: 1766.3215\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 881.5381 - val_loss: 2252.6289\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 918.3511 - val_loss: 2078.9368\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 931.9022 - val_loss: 1840.4648\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 896.1400 - val_loss: 2186.4446\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 876.7224 - val_loss: 1925.8271\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 923.1194 - val_loss: 1824.7206\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 885.1251 - val_loss: 1680.2242\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 932.2385 - val_loss: 1820.1421\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 887.1080 - val_loss: 1749.2549\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 852.9214 - val_loss: 1982.9908\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 911.5819 - val_loss: 2182.6890\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 898.9171 - val_loss: 1826.7103\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 894.0449 - val_loss: 1942.2484\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 925.3579 - val_loss: 1832.4320\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 843.8273 - val_loss: 1705.7075\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 900.3606 - val_loss: 1852.6112\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 859.4951 - val_loss: 1927.1144\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 849.0516 - val_loss: 1848.7219\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 840.4050 - val_loss: 1833.1720\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 868.8961 - val_loss: 1860.0193\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 923.8399 - val_loss: 1908.4042\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 907.6486 - val_loss: 1839.3231\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 871.5002 - val_loss: 1852.2173\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 880.0208 - val_loss: 1871.6038\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 887.6496 - val_loss: 1872.8153\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 874.1779 - val_loss: 1856.8778\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 847.2812 - val_loss: 1834.9648\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 828.9559 - val_loss: 1834.1157\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 841.4936 - val_loss: 1834.1064\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 872.1098 - val_loss: 1834.5111\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 813.5701 - val_loss: 1833.7018\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 913.6205 - val_loss: 1834.4583\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 869.4696 - val_loss: 1834.8433\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 877.9213 - val_loss: 1835.5714\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 846.8508 - val_loss: 1834.8054\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 837.9946 - val_loss: 1836.0813\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.5500 - val_loss: 1836.3341\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 848.6019 - val_loss: 1836.7770\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 832.5829 - val_loss: 1837.0042\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 858.3759 - val_loss: 1836.9773\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 846.2433 - val_loss: 1837.0778\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 862.4895 - val_loss: 1836.9882\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 836.3871 - val_loss: 1836.9954\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 830.8883 - val_loss: 1837.0193\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 892.6048 - val_loss: 1836.9764\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 871.2705 - val_loss: 1836.9957\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 3.7618 - val_loss: 2.5393\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1588 - val_loss: 2.5107\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1045 - val_loss: 2.5331\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0862 - val_loss: 2.4875\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0855 - val_loss: 2.4773\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0858 - val_loss: 2.4693\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0882 - val_loss: 2.4820\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0834 - val_loss: 2.5217\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0789 - val_loss: 2.5008\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0858 - val_loss: 2.5053\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0786 - val_loss: 2.4938\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0933 - val_loss: 2.4856\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0852 - val_loss: 2.5406\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0801 - val_loss: 2.4902\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0945 - val_loss: 2.4833\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0836 - val_loss: 2.5054\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0628 - val_loss: 2.4633\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0542 - val_loss: 2.4603\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0547 - val_loss: 2.4632\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0519 - val_loss: 2.4581\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0575 - val_loss: 2.4624\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0579 - val_loss: 2.4629\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0540 - val_loss: 2.4574\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0561 - val_loss: 2.4578\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0546 - val_loss: 2.4565\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0578 - val_loss: 2.4653\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0591 - val_loss: 2.4597\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0597 - val_loss: 2.4631\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0632 - val_loss: 2.4671\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0555 - val_loss: 2.4619\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0536 - val_loss: 2.4637\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0550 - val_loss: 2.4594\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0562 - val_loss: 2.4572\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0522 - val_loss: 2.4554\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0545 - val_loss: 2.4598\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0552 - val_loss: 2.4582\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0630 - val_loss: 2.4629\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0577 - val_loss: 2.4614\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0597 - val_loss: 2.4575\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0556 - val_loss: 2.4554\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0617 - val_loss: 2.4610\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0507 - val_loss: 2.4541\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0568 - val_loss: 2.4588\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0619 - val_loss: 2.4621\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0571 - val_loss: 2.4611\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0457 - val_loss: 2.4561\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0553 - val_loss: 2.4599\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 2.0550 - val_loss: 2.4657\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0543 - val_loss: 2.4556\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0557 - val_loss: 2.4550\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 3126.3086 - val_loss: 2349.4636\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 1034.8991 - val_loss: 2012.0146\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 944.4459 - val_loss: 1994.0522\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 882.7610 - val_loss: 1867.2153\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 863.4652 - val_loss: 1880.7778\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 885.1146 - val_loss: 1776.0560\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 890.7228 - val_loss: 1825.3009\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 907.5572 - val_loss: 1780.4307\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 872.9787 - val_loss: 1796.0100\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 857.9471 - val_loss: 1884.3109\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 876.0253 - val_loss: 1881.0570\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 869.5264 - val_loss: 1761.1677\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 848.7004 - val_loss: 1823.8361\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 838.9001 - val_loss: 1818.5729\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 842.3400 - val_loss: 1872.5697\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 861.9263 - val_loss: 1860.7880\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 859.9934 - val_loss: 1891.5872\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 871.7817 - val_loss: 1817.8761\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 862.0078 - val_loss: 1772.5189\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 835.9404 - val_loss: 1736.6688\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.4419 - val_loss: 1873.3296\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 844.3740 - val_loss: 1821.5402\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 878.8779 - val_loss: 1986.8302\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 829.6978 - val_loss: 1857.8829\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 845.4401 - val_loss: 1838.9280\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 896.9802 - val_loss: 1781.9642\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 903.9729 - val_loss: 1765.8356\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 858.9341 - val_loss: 1771.0001\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 899.4170 - val_loss: 1798.7731\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 857.0244 - val_loss: 1796.1296\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 851.0980 - val_loss: 1831.7697\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 891.0003 - val_loss: 1851.8823\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 866.9810 - val_loss: 1846.9009\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.8144 - val_loss: 1842.0359\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 802.7088 - val_loss: 1811.8228\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 894.1012 - val_loss: 1854.9310\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 901.6784 - val_loss: 1853.5681\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 868.5452 - val_loss: 1833.8512\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 849.0321 - val_loss: 1844.2340\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 863.7408 - val_loss: 1841.0159\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 852.9425 - val_loss: 1840.5974\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.2795 - val_loss: 1840.3777\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.0105 - val_loss: 1841.0504\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 845.7729 - val_loss: 1840.8820\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 855.4608 - val_loss: 1841.5370\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 866.5173 - val_loss: 1841.3403\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 822.5055 - val_loss: 1840.6733\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 899.5032 - val_loss: 1841.5159\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 844.8066 - val_loss: 1841.3346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.9365 - val_loss: 1840.6392\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9799 - val_loss: 4.7007\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9329 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9152 - val_loss: 4.7006\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9239 - val_loss: 4.7006\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9408 - val_loss: 4.7008\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9319 - val_loss: 4.7007\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9010 - val_loss: 4.7006\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9360 - val_loss: 4.7012\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9457 - val_loss: 4.7008\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9472 - val_loss: 4.7007\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9626 - val_loss: 4.7008\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9588 - val_loss: 4.7013\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9439 - val_loss: 4.7009\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9284 - val_loss: 4.7010\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9180 - val_loss: 4.7012\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9495 - val_loss: 4.7010\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9385 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9387 - val_loss: 4.7012\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9138 - val_loss: 4.7011\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9119 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9236 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9306 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9371 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9317 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9359 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9476 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9345 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9293 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9456 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9199 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9257 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9362 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9549 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9334 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9509 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9191 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9252 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9554 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9072 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9495 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9179 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9352 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9470 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9210 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9282 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9426 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9087 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9270 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9367 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9204 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 7165.7122 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7572.6809 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7387.7449 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7643.1922 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7833.3690 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: 7557.0104 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7763.0585 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7680.5773 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7378.2630 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7660.6995 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7610.3305 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7341.8013 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7657.1910 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8055.8978 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7629.8336 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7659.5091 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7520.9598 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7651.3806 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7459.2352 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7910.5203 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7904.3240 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7801.9038 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7665.4639 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7480.0847 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 6993.4467 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7450.6577 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8166.1465 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7468.3746 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7435.1327 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7590.7405 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7617.5891 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8117.2795 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7559.8600 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7449.8397 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7684.9926 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7424.0544 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7866.2959 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7628.0185 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7642.2018 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7818.3555 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7693.1275 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7723.8018 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8029.7878 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7509.8946 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7928.7014 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7651.3058 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7345.9442 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7719.2151 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7166.6059 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7502.6874 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 4.0843 - val_loss: 4.7011\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9352 - val_loss: 4.7009\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9327 - val_loss: 4.7009\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9151 - val_loss: 4.7013\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9290 - val_loss: 4.7009\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9111 - val_loss: 4.7010\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9479 - val_loss: 4.7009\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9276 - val_loss: 4.7008\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9207 - val_loss: 4.7013\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9281 - val_loss: 4.7008\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9474 - val_loss: 4.7009\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9241 - val_loss: 4.7012\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9535 - val_loss: 4.7010\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9415 - val_loss: 4.7010\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9412 - val_loss: 4.7010\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9541 - val_loss: 4.7010\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9293 - val_loss: 4.7011\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9089 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9232 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9310 - val_loss: 4.7011\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9277 - val_loss: 4.7011\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9283 - val_loss: 4.7011\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9209 - val_loss: 4.7011\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9195 - val_loss: 4.7011\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9184 - val_loss: 4.7011\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9408 - val_loss: 4.7011\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9138 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9260 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9307 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9398 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9316 - val_loss: 4.7010\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9490 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9378 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9340 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9437 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9397 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9070 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9529 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9240 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9300 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9457 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9436 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9411 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9417 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9292 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9102 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9107 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9269 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9478 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9218 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7957.9419 - val_loss: 15292.4414\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7118.0335 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7705.4925 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7845.4182 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7692.8995 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7261.1610 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7997.8403 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7482.4193 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7359.4126 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7533.3072 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7486.6682 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7703.7372 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7670.1879 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7535.1601 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7965.2599 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7415.1301 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7483.0148 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7481.6250 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7789.4359 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7191.9482 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7966.1286 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7758.1887 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7922.0698 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7482.3430 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7880.0910 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7786.1189 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8160.7528 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7650.4901 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7716.0938 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7504.5830 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7954.9609 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7715.3983 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7715.2374 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7783.1195 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7762.0882 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7568.3273 - val_loss: 15292.4346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7835.7074 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7532.4650 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7637.0237 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7620.2819 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7735.0898 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7907.8946 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7921.9531 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8288.4943 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7640.4861 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7629.1560 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7461.2366 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7348.2372 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7852.2564 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7477.3221 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 22.3416 - val_loss: 2.8762\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.2145 - val_loss: 3.0293\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.9376 - val_loss: 10.3470\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 9.3435 - val_loss: 10.0056\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8.5073 - val_loss: 4.8375\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7.2806 - val_loss: 4.2142\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 5.9835 - val_loss: 23.6276\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 9.6032 - val_loss: 4.8780\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 6.3668 - val_loss: 3.8096\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.3060 - val_loss: 4.7032\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3870 - val_loss: 3.2689\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.6702 - val_loss: 3.1119\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5385 - val_loss: 2.9532\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4332 - val_loss: 2.8792\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4177 - val_loss: 2.8340\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3766 - val_loss: 2.7853\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3560 - val_loss: 2.7731\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3637 - val_loss: 2.7539\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 22ms/step - loss: 2.6511 - val_loss: 2.7409\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4103 - val_loss: 2.7268\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.4143 - val_loss: 2.7231\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4476 - val_loss: 2.7403\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4643 - val_loss: 2.7401\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.4333 - val_loss: 2.7136\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4406 - val_loss: 2.7249\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6249 - val_loss: 2.8083\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3765 - val_loss: 2.7058\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5035 - val_loss: 2.7033\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3092 - val_loss: 2.7001\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3188 - val_loss: 2.7071\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3846 - val_loss: 2.7281\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3234 - val_loss: 2.6983\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4152 - val_loss: 2.7112\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3222 - val_loss: 2.7086\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3170 - val_loss: 2.7136\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.3007 - val_loss: 2.7032\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2664 - val_loss: 2.6964\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.6362 - val_loss: 2.7148\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4414 - val_loss: 2.6963\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3166 - val_loss: 2.7083\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2893 - val_loss: 2.6800\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3505 - val_loss: 2.6800\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4530 - val_loss: 2.6828\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3567 - val_loss: 2.6993\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3003 - val_loss: 2.6829\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3600 - val_loss: 2.6853\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3152 - val_loss: 2.6791\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3873 - val_loss: 2.7117\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.3299 - val_loss: 2.6773\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3592 - val_loss: 2.6737\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 1640.9967 - val_loss: 2099.2747\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 972.1296 - val_loss: 2343.8967\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 901.5896 - val_loss: 2097.4043\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 919.2949 - val_loss: 1833.2142\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 926.5810 - val_loss: 1762.8337\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 917.0752 - val_loss: 1885.7252\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 874.8764 - val_loss: 1913.5566\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 874.9904 - val_loss: 1878.2681\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 853.3436 - val_loss: 1780.4385\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 930.2036 - val_loss: 1985.2418\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 896.5070 - val_loss: 1817.0557\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 935.8027 - val_loss: 2047.7107\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 890.8029 - val_loss: 1799.1456\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 891.3558 - val_loss: 1834.0959\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 895.4995 - val_loss: 1802.3704\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 875.6968 - val_loss: 1843.6245\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 889.3843 - val_loss: 1868.1735\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 846.8531 - val_loss: 1850.2651\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 849.6616 - val_loss: 1862.2878\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 839.6282 - val_loss: 1828.0917\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 848.4479 - val_loss: 1826.5280\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 920.0797 - val_loss: 1844.8938\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 895.7314 - val_loss: 1861.1456\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 834.6214 - val_loss: 1835.6488\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 853.0566 - val_loss: 1825.2748\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 869.6317 - val_loss: 1841.4099\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 806.9535 - val_loss: 1837.6968\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 851.5186 - val_loss: 1833.7834\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 817.0749 - val_loss: 1836.1322\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 849.4580 - val_loss: 1835.7007\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 836.3356 - val_loss: 1837.0837\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 832.1113 - val_loss: 1837.0002\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 824.3346 - val_loss: 1834.0233\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 843.1052 - val_loss: 1835.4330\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 913.1622 - val_loss: 1836.9677\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 872.4863 - val_loss: 1837.1741\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 857.6088 - val_loss: 1837.2755\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 846.4603 - val_loss: 1837.5603\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 845.3036 - val_loss: 1837.8009\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 876.7919 - val_loss: 1837.6312\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 860.7868 - val_loss: 1837.7009\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 873.9352 - val_loss: 1837.6014\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 814.9447 - val_loss: 1837.5417\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 22ms/step - loss: 895.0741 - val_loss: 1837.8077\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 857.0439 - val_loss: 1837.9293\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 851.0351 - val_loss: 1837.9163\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 847.1525 - val_loss: 1837.9255\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 854.8546 - val_loss: 1837.9176\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 798.6271 - val_loss: 1837.9380\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 814.4546 - val_loss: 1837.9243\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.8576 - val_loss: 2.7044\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2988 - val_loss: 2.6493\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2004 - val_loss: 2.5629\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1729 - val_loss: 2.5312\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1423 - val_loss: 2.5810\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1702 - val_loss: 2.5354\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1474 - val_loss: 2.5490\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1356 - val_loss: 2.5392\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1368 - val_loss: 2.6896\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1500 - val_loss: 2.5742\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1560 - val_loss: 2.5546\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1539 - val_loss: 2.5467\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.2243 - val_loss: 2.5390\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.2138 - val_loss: 2.5203\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2196 - val_loss: 2.5186\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2282 - val_loss: 2.5196\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.2617 - val_loss: 2.5527\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2197 - val_loss: 4.4920\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2915 - val_loss: 2.5380\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1389 - val_loss: 2.5808\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2250 - val_loss: 5.6349\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2159 - val_loss: 2.5813\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8385 - val_loss: 2.5116\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1361 - val_loss: 2.5313\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2493 - val_loss: 6.9472\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5495 - val_loss: 2.4881\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2319 - val_loss: 2.5297\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6152 - val_loss: 2.5098\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3399 - val_loss: 2.4888\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1530 - val_loss: 5.3162\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8804 - val_loss: 2.4920\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2960 - val_loss: 2.5347\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3399 - val_loss: 2.4599\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3008 - val_loss: 2.5621\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5027 - val_loss: 2.5114\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3851 - val_loss: 2.4880\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4066 - val_loss: 2.5139\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2824 - val_loss: 2.4874\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3241 - val_loss: 2.5784\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5770 - val_loss: 2.4594\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.3307 - val_loss: 2.5056\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.0040 - val_loss: 2.4942\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4101 - val_loss: 2.4900\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2167 - val_loss: 2.4742\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4520 - val_loss: 2.5194\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3183 - val_loss: 2.4917\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4108 - val_loss: 2.4808\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.4542 - val_loss: 2.9695\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5555 - val_loss: 2.5014\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3952 - val_loss: 2.4826\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3070.0264 - val_loss: 2334.5557\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 972.0303 - val_loss: 1962.1107\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 938.6226 - val_loss: 2042.0035\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 946.1956 - val_loss: 1983.8331\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 829.0150 - val_loss: 1890.2800\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 901.0748 - val_loss: 1845.7365\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 947.9978 - val_loss: 1931.2606\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 908.0249 - val_loss: 1962.6149\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 905.7463 - val_loss: 1860.7478\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 915.6969 - val_loss: 1859.7079\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 872.2643 - val_loss: 1839.5681\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 861.6148 - val_loss: 1851.0239\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 888.0306 - val_loss: 1852.3959\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 873.0314 - val_loss: 1807.7617\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 842.3045 - val_loss: 1849.1549\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 824.2474 - val_loss: 1739.4845\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 889.0209 - val_loss: 1839.8711\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 861.9317 - val_loss: 1863.1145\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 917.9009 - val_loss: 1847.7769\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 882.2210 - val_loss: 1896.8844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 917.2952 - val_loss: 1834.0664\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 866.9160 - val_loss: 1832.7898\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 849.4268 - val_loss: 1878.4435\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 879.6743 - val_loss: 1802.5812\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 838.9500 - val_loss: 1955.1990\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 856.5712 - val_loss: 1966.2762\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 861.8608 - val_loss: 1850.2733\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 824.0874 - val_loss: 1836.0708\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 831.9470 - val_loss: 1845.2583\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 810.0077 - val_loss: 1833.2981\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 878.1131 - val_loss: 1847.5714\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 871.5877 - val_loss: 1835.3602\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 890.9915 - val_loss: 1842.2255\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 838.9143 - val_loss: 1823.4851\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 847.8140 - val_loss: 1851.7098\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 892.2858 - val_loss: 1855.9270\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 895.7300 - val_loss: 1852.3711\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 828.1586 - val_loss: 1848.6913\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 896.7822 - val_loss: 1848.4316\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 838.1338 - val_loss: 1847.5359\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 830.7968 - val_loss: 1846.9139\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 894.3903 - val_loss: 1847.5983\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 855.2616 - val_loss: 1847.4741\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 840.8159 - val_loss: 1846.1591\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 862.6928 - val_loss: 1844.9769\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 840.0854 - val_loss: 1843.8317\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 868.1570 - val_loss: 1843.8058\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 850.1043 - val_loss: 1843.7640\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 837.2036 - val_loss: 1843.7085\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 819.8711 - val_loss: 1843.8401\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.2631 - val_loss: 4.9317\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1843 - val_loss: 4.9148\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1611 - val_loss: 4.8801\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1040 - val_loss: 4.8595\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0874 - val_loss: 4.8319\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0566 - val_loss: 4.8539\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0624 - val_loss: 4.8180\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0467 - val_loss: 4.8081\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0481 - val_loss: 4.8063\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0295 - val_loss: 4.8077\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0239 - val_loss: 4.8067\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0433 - val_loss: 4.8049\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0353 - val_loss: 4.7986\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0134 - val_loss: 4.7973\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0150 - val_loss: 4.8000\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0372 - val_loss: 4.7977\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0193 - val_loss: 4.7986\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0249 - val_loss: 4.7954\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0354 - val_loss: 4.7947\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0248 - val_loss: 4.8034\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0205 - val_loss: 4.7953\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0205 - val_loss: 4.7956\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0280 - val_loss: 4.7924\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0207 - val_loss: 4.7922\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0473 - val_loss: 4.7961\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0528 - val_loss: 4.7956\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0127 - val_loss: 4.7903\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0341 - val_loss: 4.7928\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0234 - val_loss: 4.7953\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0341 - val_loss: 4.7906\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0119 - val_loss: 4.7938\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0197 - val_loss: 4.8157\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0313 - val_loss: 4.7921\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0636 - val_loss: 4.7914\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0091 - val_loss: 4.7890\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0179 - val_loss: 4.7973\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0171 - val_loss: 4.7917\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0228 - val_loss: 4.7907\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0360 - val_loss: 4.7909\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0038 - val_loss: 4.7909\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0155 - val_loss: 4.7927\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0017 - val_loss: 4.7900\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0363 - val_loss: 4.7894\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9988 - val_loss: 4.7910\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0015 - val_loss: 4.7911\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9407 - val_loss: 4.7150\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9462 - val_loss: 4.7149\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9334 - val_loss: 4.7149\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9458 - val_loss: 4.7147\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9363 - val_loss: 4.7149\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7406.3359 - val_loss: 15292.8037\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7829.7202 - val_loss: 15292.7520\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7536.3411 - val_loss: 15292.7324\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7381.7255 - val_loss: 15292.7275\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7363.7392 - val_loss: 15292.6914\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7674.7773 - val_loss: 15292.6631\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7627.8948 - val_loss: 15292.6406\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7595.1458 - val_loss: 15292.6357\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7180.5836 - val_loss: 15292.6201\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7455.7677 - val_loss: 15292.6113\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7505.1847 - val_loss: 15292.6104\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7908.2677 - val_loss: 15292.6016\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7702.0392 - val_loss: 15292.5908\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7433.5264 - val_loss: 15292.5850\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7401.6811 - val_loss: 15292.5850\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7520.6404 - val_loss: 15292.5801\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7621.5479 - val_loss: 15292.5674\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7810.0900 - val_loss: 15292.5723\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7692.1575 - val_loss: 15292.5654\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7443.1031 - val_loss: 15292.5674\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7808.5678 - val_loss: 15292.5635\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7573.7190 - val_loss: 15292.5566\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7611.0698 - val_loss: 15292.5566\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7462.0215 - val_loss: 15292.5527\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7767.5676 - val_loss: 15292.5566\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7596.6474 - val_loss: 15292.5566\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7363.1426 - val_loss: 15292.5527\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7601.5249 - val_loss: 15292.5479\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7679.1274 - val_loss: 15292.5488\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7554.8218 - val_loss: 15292.5488\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7858.5495 - val_loss: 15292.5430\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7742.0472 - val_loss: 15292.5439\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7827.5958 - val_loss: 15292.5439\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7761.7389 - val_loss: 15292.5488\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7826.5042 - val_loss: 15292.5430\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7653.6628 - val_loss: 15292.5469\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7629.2248 - val_loss: 15292.5381\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7776.2503 - val_loss: 15292.5391\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7304.8231 - val_loss: 15292.5391\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7529.4183 - val_loss: 15292.5381\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7681.0512 - val_loss: 15292.5469\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7444.1585 - val_loss: 15292.5361\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7526.4475 - val_loss: 15292.5361\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7745.2053 - val_loss: 15292.5352\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7435.1702 - val_loss: 15292.5352\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7839.9821 - val_loss: 15292.5332\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7351.6477 - val_loss: 15292.5352\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7771.6093 - val_loss: 15292.5352\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7702.6341 - val_loss: 15292.5312\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7661.1760 - val_loss: 15292.5312\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.3005 - val_loss: 4.9276\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1333 - val_loss: 4.9008\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.1220 - val_loss: 4.8822\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0761 - val_loss: 4.8640\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0700 - val_loss: 4.8504\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0715 - val_loss: 4.8414\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0543 - val_loss: 4.8398\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0402 - val_loss: 4.8293\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0156 - val_loss: 4.8144\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0414 - val_loss: 4.8093\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0376 - val_loss: 4.8033\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0358 - val_loss: 4.8016\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0241 - val_loss: 4.7944\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9985 - val_loss: 4.7909\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0042 - val_loss: 4.7864\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0076 - val_loss: 4.7879\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9759 - val_loss: 4.7815\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0330 - val_loss: 4.7800\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9728 - val_loss: 4.7773\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0007 - val_loss: 4.7796\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0113 - val_loss: 4.7772\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9906 - val_loss: 4.7925\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0222 - val_loss: 4.7674\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0051 - val_loss: 4.7700\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0028 - val_loss: 4.7653\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0158 - val_loss: 4.7630\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9969 - val_loss: 4.7631\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9684 - val_loss: 4.7610\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9820 - val_loss: 4.7588\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9842 - val_loss: 4.7592\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9767 - val_loss: 4.7580\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9757 - val_loss: 4.7540\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9927 - val_loss: 4.7546\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9769 - val_loss: 4.7548\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9644 - val_loss: 4.7519\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9755 - val_loss: 4.7524\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9957 - val_loss: 4.7566\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9863 - val_loss: 4.7491\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9686 - val_loss: 4.7522\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9935 - val_loss: 4.7486\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9819 - val_loss: 4.7475\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9657 - val_loss: 4.7477\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9453 - val_loss: 4.7460\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9558 - val_loss: 4.7514\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9770 - val_loss: 4.7489\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9575 - val_loss: 4.7465\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9802 - val_loss: 4.7440\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9515 - val_loss: 4.7426\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9673 - val_loss: 4.7642\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9879 - val_loss: 4.7418\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7794.5645 - val_loss: 15292.9697\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7477.1840 - val_loss: 15292.8975\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7485.4377 - val_loss: 15292.8457\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7488.7500 - val_loss: 15292.8281\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7659.0338 - val_loss: 15292.7852\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7510.9133 - val_loss: 15292.7432\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7259.6903 - val_loss: 15292.7275\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7986.5849 - val_loss: 15292.7051\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7932.8421 - val_loss: 15292.6865\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7728.7040 - val_loss: 15292.6719\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7529.3680 - val_loss: 15292.6562\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7381.2939 - val_loss: 15292.6465\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7517.2235 - val_loss: 15292.6367\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7436.8119 - val_loss: 15292.6270\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7470.5096 - val_loss: 15292.6221\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7365.4201 - val_loss: 15292.6152\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7423.3828 - val_loss: 15292.6035\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7786.5130 - val_loss: 15292.6016\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7445.2630 - val_loss: 15292.6006\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7285.8535 - val_loss: 15292.5869\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7537.1325 - val_loss: 15292.5869\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7895.0073 - val_loss: 15292.5752\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8075.1233 - val_loss: 15292.5723\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7592.5575 - val_loss: 15292.5723\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7891.2908 - val_loss: 15292.5664\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7788.3422 - val_loss: 15292.5674\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7422.4097 - val_loss: 15292.5615\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7726.7655 - val_loss: 15292.5635\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7405.4970 - val_loss: 15292.5498\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7390.2771 - val_loss: 15292.5498\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7598.8210 - val_loss: 15292.5488\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7447.3589 - val_loss: 15292.5439\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7401.4531 - val_loss: 15292.5410\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7548.0986 - val_loss: 15292.5439\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7791.3462 - val_loss: 15292.5449\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7674.6575 - val_loss: 15292.5381\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7244.8225 - val_loss: 15292.5391\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7626.7139 - val_loss: 15292.5332\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7715.9602 - val_loss: 15292.5303\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7567.2915 - val_loss: 15292.5312\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7444.4185 - val_loss: 15292.5332\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8062.5760 - val_loss: 15292.5303\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7547.2054 - val_loss: 15292.5215\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7784.3552 - val_loss: 15292.5293\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7368.3811 - val_loss: 15292.5215\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7262.7494 - val_loss: 15292.5195\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7378.5660 - val_loss: 15292.5205\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7758.9083 - val_loss: 15292.5205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7540.5081 - val_loss: 15292.5195\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7733.4635 - val_loss: 15292.5156\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: 30.6166\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 25.4275 - val_loss: 25.3876\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 21.6708 - val_loss: 22.4258\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 19.1774 - val_loss: 20.3953\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 17.4439 - val_loss: 18.8702\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 16.2129 - val_loss: 17.6703\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 15.2783 - val_loss: 16.6947\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 14.4466 - val_loss: 15.8863\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 13.6570 - val_loss: 15.2039\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 13.1013 - val_loss: 14.6195\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 28495.3145\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 14162.5579 - val_loss: 26012.6348\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 13346.0812 - val_loss: 24580.4375\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 12614.1108 - val_loss: 23587.8887\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11919.1018 - val_loss: 22842.5039\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11872.2910 - val_loss: 22254.7578\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11330.1835 - val_loss: 21780.0801\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11344.5004 - val_loss: 21386.7090\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11003.5298 - val_loss: 21056.2266\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 10211.2881 - val_loss: 20772.4141\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 23.0186 - val_loss: 13.1339\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.4284 - val_loss: 2.9870\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.9768 - val_loss: 2.9703\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8.9258 - val_loss: 3.1398\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.2582 - val_loss: 2.8842\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.1914 - val_loss: 3.1724\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.2583 - val_loss: 3.2518\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.0279 - val_loss: 34.9923\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.4430 - val_loss: 2.9534\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.8202 - val_loss: 3.0201\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.0515 - val_loss: 8.9492\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.2736 - val_loss: 3.4182\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8.1941 - val_loss: 17.0505\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 10.0000 - val_loss: 7.1066\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8.4489 - val_loss: 3.6523\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.6146 - val_loss: 2.5602\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1928 - val_loss: 2.6321\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1658 - val_loss: 2.5006\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1559 - val_loss: 2.6156\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1381 - val_loss: 2.4925\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1302 - val_loss: 2.5024\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1141 - val_loss: 2.5415\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1319 - val_loss: 2.5411\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1125 - val_loss: 2.4871\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1188 - val_loss: 2.5057\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1273 - val_loss: 2.4955\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1349 - val_loss: 2.6207\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1167 - val_loss: 2.5145\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1139 - val_loss: 2.5221\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1211 - val_loss: 2.9318\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1299 - val_loss: 2.5162\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1053 - val_loss: 2.4890\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1112 - val_loss: 2.5082\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1061 - val_loss: 2.5175\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0785 - val_loss: 2.4683\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0614 - val_loss: 2.4643\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 2.0674 - val_loss: 2.4685\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0615 - val_loss: 2.4701\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0642 - val_loss: 2.4613\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0551 - val_loss: 2.4628\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0562 - val_loss: 2.4659\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0615 - val_loss: 2.4603\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0554 - val_loss: 2.4671\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0545 - val_loss: 2.4605\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0608 - val_loss: 2.4693\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0560 - val_loss: 2.4626\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0529 - val_loss: 2.4554\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0569 - val_loss: 2.4614\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0497 - val_loss: 2.4556\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0517 - val_loss: 2.4571\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 28ms/step - loss: 1720.8357 - val_loss: 1864.0359\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 965.5212 - val_loss: 1960.3451\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 922.8633 - val_loss: 2079.1936\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 891.7353 - val_loss: 1906.6108\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 854.9048 - val_loss: 1746.5448\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 878.3844 - val_loss: 2078.8704\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 834.4616 - val_loss: 1861.6111\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 913.5733 - val_loss: 2100.6907\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 870.2186 - val_loss: 1787.4706\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 915.9463 - val_loss: 1829.1454\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 885.6720 - val_loss: 1844.3988\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 861.2247 - val_loss: 1749.0045\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 874.4158 - val_loss: 1948.1929\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 936.1310 - val_loss: 2037.4434\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 905.6631 - val_loss: 2131.5901\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 876.9923 - val_loss: 1851.6407\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 836.7462 - val_loss: 1856.2010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 851.3868 - val_loss: 1831.6652\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 849.5074 - val_loss: 1859.6143\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 819.2326 - val_loss: 1798.6040\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 836.0130 - val_loss: 1825.7006\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 839.6713 - val_loss: 1830.1804\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 890.0124 - val_loss: 1855.1304\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.1279 - val_loss: 1817.1466\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 857.3686 - val_loss: 1850.7877\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 841.9899 - val_loss: 1845.9403\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 834.4863 - val_loss: 1844.0751\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 878.3755 - val_loss: 1843.6058\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 871.9470 - val_loss: 1841.1379\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 831.7463 - val_loss: 1840.7905\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 854.5562 - val_loss: 1839.3123\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.3542 - val_loss: 1839.0621\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 838.9291 - val_loss: 1838.4814\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 836.9010 - val_loss: 1837.1703\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.0407 - val_loss: 1836.6011\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 844.3660 - val_loss: 1836.6973\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 813.8971 - val_loss: 1836.8070\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 859.5479 - val_loss: 1836.9263\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 858.0230 - val_loss: 1837.0442\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 862.1929 - val_loss: 1837.1658\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 868.9294 - val_loss: 1837.2727\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 868.4167 - val_loss: 1837.1122\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 926.9671 - val_loss: 1837.2754\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 837.5290 - val_loss: 1837.2136\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 853.3611 - val_loss: 1837.4159\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 861.1652 - val_loss: 1837.3965\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 801.9631 - val_loss: 1837.4089\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 848.4494 - val_loss: 1837.3970\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 845.6006 - val_loss: 1837.3942\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 873.4058 - val_loss: 1837.3933\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 3.8494 - val_loss: 2.6093\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1759 - val_loss: 2.5148\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1218 - val_loss: 2.5283\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1056 - val_loss: 2.5784\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1017 - val_loss: 2.4931\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1059 - val_loss: 2.5002\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1038 - val_loss: 2.4846\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0978 - val_loss: 2.5315\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1005 - val_loss: 2.4798\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1032 - val_loss: 2.4855\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0987 - val_loss: 2.5125\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0991 - val_loss: 2.4841\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1111 - val_loss: 2.5158\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0944 - val_loss: 2.4988\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0939 - val_loss: 2.5286\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1045 - val_loss: 2.4973\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1126 - val_loss: 2.4977\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1069 - val_loss: 2.5002\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1142 - val_loss: 2.5002\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0762 - val_loss: 2.4766\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0709 - val_loss: 2.4799\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0719 - val_loss: 2.4818\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0713 - val_loss: 2.4805\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0630 - val_loss: 2.4777\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0695 - val_loss: 2.4740\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0718 - val_loss: 2.4800\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0727 - val_loss: 2.4802\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0656 - val_loss: 2.4776\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0700 - val_loss: 2.4700\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0679 - val_loss: 2.4757\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0726 - val_loss: 2.4795\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0700 - val_loss: 2.4761\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0697 - val_loss: 2.4730\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0677 - val_loss: 2.4711\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0690 - val_loss: 2.4756\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0695 - val_loss: 2.4738\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0640 - val_loss: 2.4720\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0622 - val_loss: 2.4697\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0648 - val_loss: 2.4696\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0715 - val_loss: 2.4713\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0632 - val_loss: 2.4718\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0694 - val_loss: 2.4771\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0651 - val_loss: 2.4794\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0668 - val_loss: 2.4704\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0611 - val_loss: 2.4755\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0660 - val_loss: 2.4746\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0570 - val_loss: 2.4661\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0589 - val_loss: 2.4558\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0543 - val_loss: 2.4642\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0561 - val_loss: 2.4603\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 2959.0894 - val_loss: 2387.8896\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 1062.6074 - val_loss: 1946.3759\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 899.0567 - val_loss: 1848.5754\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 877.2101 - val_loss: 2070.1843\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.6670 - val_loss: 1921.4260\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 886.5171 - val_loss: 2011.0349\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 856.7522 - val_loss: 1872.7405\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 865.5665 - val_loss: 1898.4641\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 919.8441 - val_loss: 1798.0243\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 865.9925 - val_loss: 1939.0410\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 844.2531 - val_loss: 1927.9407\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 899.9426 - val_loss: 1904.9460\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 856.4496 - val_loss: 1904.5604\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 889.5660 - val_loss: 1883.3383\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 874.6146 - val_loss: 1778.9650\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 830.3015 - val_loss: 1864.6146\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 900.9968 - val_loss: 1884.2761\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 844.5260 - val_loss: 1829.6591\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 909.5185 - val_loss: 1875.3601\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 825.4134 - val_loss: 1811.8912\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 888.5426 - val_loss: 1870.1050\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 894.7089 - val_loss: 1978.6818\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.4613 - val_loss: 1897.9702\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 873.2198 - val_loss: 1817.2246\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 886.5224 - val_loss: 1865.3185\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 828.2664 - val_loss: 1840.2615\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 824.8843 - val_loss: 1844.8619\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 843.5382 - val_loss: 1846.2949\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 882.6219 - val_loss: 1850.0518\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.9761 - val_loss: 1841.8124\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 839.6819 - val_loss: 1847.6655\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 863.2358 - val_loss: 1853.9698\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 825.2571 - val_loss: 1832.8153\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 863.2923 - val_loss: 1822.9930\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 901.6726 - val_loss: 1844.6995\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 842.2401 - val_loss: 1844.6942\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 880.0390 - val_loss: 1845.3510\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 858.7127 - val_loss: 1843.4583\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 839.2606 - val_loss: 1842.6884\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 898.2673 - val_loss: 1843.0240\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 873.4847 - val_loss: 1842.4175\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 855.8445 - val_loss: 1841.8619\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 831.5588 - val_loss: 1842.1952\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 833.8868 - val_loss: 1841.2061\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 839.9872 - val_loss: 1840.6260\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 846.3621 - val_loss: 1840.6438\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 868.6360 - val_loss: 1840.7208\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 883.8253 - val_loss: 1840.8914\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 834.2990 - val_loss: 1840.7467\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.3046 - val_loss: 1840.6716\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9660 - val_loss: 4.7018\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9425 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9153 - val_loss: 4.7006\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9350 - val_loss: 4.7006\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9270 - val_loss: 4.7018\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9276 - val_loss: 4.7007\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9494 - val_loss: 4.7006\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9301 - val_loss: 4.7024\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9328 - val_loss: 4.7010\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9053 - val_loss: 4.7007\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9415 - val_loss: 4.7011\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9205 - val_loss: 4.7007\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9519 - val_loss: 4.7010\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9136 - val_loss: 4.7008\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9441 - val_loss: 4.7009\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9404 - val_loss: 4.7008\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9226 - val_loss: 4.7011\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9228 - val_loss: 4.7011\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9299 - val_loss: 4.7009\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9371 - val_loss: 4.7012\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9293 - val_loss: 4.7010\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9016 - val_loss: 4.7009\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9536 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9340 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9192 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9403 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9486 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9206 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9310 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9322 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.8979 - val_loss: 4.7010\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9422 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9720 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9114 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.8898 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9123 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9373 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9301 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9522 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9408 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9384 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9055 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9238 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9240 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9588 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9031 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9469 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9276 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9250 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8912 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 7444.5927 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7371.1026 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7336.4721 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8023.6319 - val_loss: 15292.4346\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 7551.8201 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7177.2115 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7657.8490 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7944.6866 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7691.3223 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7806.1494 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7613.4586 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7687.3824 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7552.1186 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7378.6615 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7970.4878 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7624.1595 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7499.4649 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7855.6105 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7591.4564 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7579.2490 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7632.5522 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7555.9896 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7448.2019 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7501.0230 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7453.3837 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7788.9225 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7436.5339 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7426.9724 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7620.8508 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7671.5789 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7392.1221 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7643.7745 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7813.1158 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7670.9093 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7875.5137 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7711.0449 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7728.1096 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7391.6383 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7512.9264 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7586.8089 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7131.4168 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7516.0076 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7639.5852 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7269.0906 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7910.0870 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7328.9604 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7413.4822 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7637.5946 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7307.5464 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7924.8899 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 4.0733 - val_loss: 4.7009\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9312 - val_loss: 4.7010\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9450 - val_loss: 4.7016\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9181 - val_loss: 4.7007\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9236 - val_loss: 4.7010\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9194 - val_loss: 4.7009\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9220 - val_loss: 4.7008\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9322 - val_loss: 4.7008\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9173 - val_loss: 4.7008\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9626 - val_loss: 4.7012\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9222 - val_loss: 4.7008\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9349 - val_loss: 4.7011\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9298 - val_loss: 4.7007\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9295 - val_loss: 4.7008\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9442 - val_loss: 4.7009\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9369 - val_loss: 4.7010\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9171 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9197 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9325 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9469 - val_loss: 4.7011\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8996 - val_loss: 4.7010\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9160 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9434 - val_loss: 4.7011\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9530 - val_loss: 4.7011\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9076 - val_loss: 4.7011\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9334 - val_loss: 4.7011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9276 - val_loss: 4.7011\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9403 - val_loss: 4.7011\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9346 - val_loss: 4.7011\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9344 - val_loss: 4.7011\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9145 - val_loss: 4.7011\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8975 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9325 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9438 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9136 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9487 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9008 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9489 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9547 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9245 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9363 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9048 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9338 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9118 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9177 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9024 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9385 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9383 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9318 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9323 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 7459.5194 - val_loss: 15292.4424\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7629.7843 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7831.0935 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7146.1122 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7567.5758 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7759.0645 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7655.2689 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7304.4987 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7584.7465 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7547.8619 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7270.2809 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: 7263.4591 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7486.3550 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7403.8681 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7665.6579 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7311.1755 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7244.2272 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7450.4848 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7498.5187 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7811.7117 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 7612.8158 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7476.1697 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7346.2350 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7333.4847 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8049.0710 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7276.2965 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7824.3454 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7727.2801 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7793.1527 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7676.0555 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7923.0546 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7754.4518 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7399.1907 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7673.1952 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7826.0978 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7315.1904 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8073.6270 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7467.8810 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7672.5201 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7506.7877 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7979.6593 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7577.6674 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7640.8652 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7491.8834 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7593.4477 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7362.8913 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7746.6699 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7212.2187 - val_loss: 15292.4346\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: 7374.4328 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7119.2021 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 61.8432 - val_loss: 5.2500\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.9248 - val_loss: 10.8971\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.2379 - val_loss: 2.6272\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.4693 - val_loss: 2.8771\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.3686 - val_loss: 3.3530\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6109 - val_loss: 2.8590\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.7581 - val_loss: 7.6817\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 10.8615 - val_loss: 3.0827\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.0376 - val_loss: 5.6655\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.9301 - val_loss: 8.0657\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7.2457 - val_loss: 7.3105\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 6.6201 - val_loss: 7.3786\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.8420 - val_loss: 9.5653\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 6.8388 - val_loss: 7.4143\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.7792 - val_loss: 7.2398\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.5484 - val_loss: 6.9691\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.3806 - val_loss: 6.9584\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.4109 - val_loss: 6.9549\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.3859 - val_loss: 6.9453\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.4465 - val_loss: 6.9572\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3605 - val_loss: 6.9265\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.3837 - val_loss: 6.9181\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.4250 - val_loss: 6.9161\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.3394 - val_loss: 6.9170\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.4359 - val_loss: 6.9162\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3425 - val_loss: 6.9155\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3755 - val_loss: 6.9147\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3407 - val_loss: 6.9138\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3389 - val_loss: 6.9122\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3778 - val_loss: 6.9141\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3306 - val_loss: 6.9130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.3733 - val_loss: 6.9151\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 5.4619 - val_loss: 6.9127\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 5.3883 - val_loss: 6.9134\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 5.3551 - val_loss: 6.9138\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 5.3438 - val_loss: 6.9140\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3243 - val_loss: 6.9142\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3552 - val_loss: 6.9141\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.3540 - val_loss: 6.9143\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3773 - val_loss: 6.9143\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 5.3078 - val_loss: 6.9142\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3601 - val_loss: 6.9143\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.4014 - val_loss: 6.9144\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3653 - val_loss: 6.9144\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.3567 - val_loss: 6.9144\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3641 - val_loss: 6.9144\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3740 - val_loss: 6.9144\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3614 - val_loss: 6.9144\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3942 - val_loss: 6.9144\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.4463 - val_loss: 6.9144\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 1750.3312 - val_loss: 1861.5084\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 960.9188 - val_loss: 1987.2316\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 858.7723 - val_loss: 1789.4298\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 931.8385 - val_loss: 2107.7495\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 917.4254 - val_loss: 1868.6353\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 806.7909 - val_loss: 1739.1053\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 890.1008 - val_loss: 2062.9424\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 921.9723 - val_loss: 1671.1810\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 916.1711 - val_loss: 1784.7758\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 904.9383 - val_loss: 2131.1619\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 867.3194 - val_loss: 1904.4978\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 858.8214 - val_loss: 1877.7073\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 915.2476 - val_loss: 1810.0590\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 940.3303 - val_loss: 1937.5809\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 823.7897 - val_loss: 1880.3798\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 860.8189 - val_loss: 1950.0681\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 883.0098 - val_loss: 1786.3633\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 908.2688 - val_loss: 1840.4321\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 872.4560 - val_loss: 1861.5997\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 883.2558 - val_loss: 1842.3021\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 813.3622 - val_loss: 1823.0624\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 838.6437 - val_loss: 1808.9988\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 846.6973 - val_loss: 1835.2427\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 839.5600 - val_loss: 1837.9417\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 847.1246 - val_loss: 1834.4337\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 848.4912 - val_loss: 1832.9812\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 885.4546 - val_loss: 1845.0046\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 802.4864 - val_loss: 1853.1617\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 892.2920 - val_loss: 1839.0660\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 843.1835 - val_loss: 1838.1622\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 914.5463 - val_loss: 1839.7365\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 880.1918 - val_loss: 1840.6821\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 811.5129 - val_loss: 1838.7402\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 863.2704 - val_loss: 1840.7469\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 898.2614 - val_loss: 1842.4006\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 852.1293 - val_loss: 1840.3267\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 909.2316 - val_loss: 1842.5073\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 828.1234 - val_loss: 1839.3810\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 826.2203 - val_loss: 1839.3787\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 831.8791 - val_loss: 1839.3835\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 832.1374 - val_loss: 1839.2850\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 861.7550 - val_loss: 1839.2560\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 851.1230 - val_loss: 1839.1028\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 878.9840 - val_loss: 1838.9553\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 870.7809 - val_loss: 1838.8828\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 854.2148 - val_loss: 1838.9165\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 854.8875 - val_loss: 1839.0303\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 843.2398 - val_loss: 1838.8396\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 819.0127 - val_loss: 1838.8274\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 885.9504 - val_loss: 1838.8256\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 3.7153 - val_loss: 2.5557\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1709 - val_loss: 2.5367\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1425 - val_loss: 2.5110\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1363 - val_loss: 2.5603\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1302 - val_loss: 2.5265\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1924 - val_loss: 2.5076\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0923 - val_loss: 2.4812\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1009 - val_loss: 2.4972\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1355 - val_loss: 2.5218\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1384 - val_loss: 2.5105\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1161 - val_loss: 2.4848\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1458 - val_loss: 2.4767\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2485 - val_loss: 2.4909\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2018 - val_loss: 2.5135\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1107 - val_loss: 2.4858\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0955 - val_loss: 2.5076\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1130 - val_loss: 2.4923\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1519 - val_loss: 2.4686\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1064 - val_loss: 2.4731\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.2320 - val_loss: 2.4958\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4558 - val_loss: 2.4779\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2045 - val_loss: 2.4704\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2790 - val_loss: 4.2044\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2243 - val_loss: 2.4653\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.3565 - val_loss: 2.4531\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1444 - val_loss: 2.4534\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3216 - val_loss: 5.0605\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3271 - val_loss: 2.4625\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2807 - val_loss: 2.4672\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4706 - val_loss: 2.4809\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.3056 - val_loss: 2.4799\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.5109 - val_loss: 2.4414\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5443 - val_loss: 3.1274\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.5250 - val_loss: 2.9382\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3427 - val_loss: 2.4483\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2740 - val_loss: 2.4867\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1734 - val_loss: 2.4435\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.5303 - val_loss: 2.4537\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3630 - val_loss: 2.4829\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.2888 - val_loss: 2.4700\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2361 - val_loss: 2.4552\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.6582 - val_loss: 2.4705\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0512 - val_loss: 2.4377\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0433 - val_loss: 2.4419\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0226 - val_loss: 2.4367\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0188 - val_loss: 2.4351\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0610 - val_loss: 2.4309\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0283 - val_loss: 2.4332\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0213 - val_loss: 2.4482\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0269 - val_loss: 2.4385\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3034.4322 - val_loss: 2509.0950\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 1143.0411 - val_loss: 2068.3159\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 947.0747 - val_loss: 2064.2190\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 935.2705 - val_loss: 1961.0963\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 864.5050 - val_loss: 1938.4839\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 879.2576 - val_loss: 1854.8879\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 871.1048 - val_loss: 1921.6936\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 879.2240 - val_loss: 1919.3619\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 862.1387 - val_loss: 1938.3257\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 887.0309 - val_loss: 1977.0315\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 870.0078 - val_loss: 1946.1028\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 847.2510 - val_loss: 1898.8081\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 837.5016 - val_loss: 1802.9094\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 826.0314 - val_loss: 1773.5310\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 872.2740 - val_loss: 1855.4725\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 817.7520 - val_loss: 1827.9240\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 842.6673 - val_loss: 1769.8146\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 890.4070 - val_loss: 1829.7151\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 893.5999 - val_loss: 1912.7120\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 864.2229 - val_loss: 1871.5743\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 857.8079 - val_loss: 1881.5508\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 892.7832 - val_loss: 1817.2125\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 804.9652 - val_loss: 1904.3885\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 845.1251 - val_loss: 1801.9943\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 874.1054 - val_loss: 1885.3046\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 872.8015 - val_loss: 1871.7250\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 919.9031 - val_loss: 1901.5953\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 827.5294 - val_loss: 1845.2457\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 887.8215 - val_loss: 1870.4978\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 853.5686 - val_loss: 1851.6931\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 835.6604 - val_loss: 1843.5045\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 834.6874 - val_loss: 1835.2445\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 843.1429 - val_loss: 1851.3577\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 908.4367 - val_loss: 1858.1161\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 875.3005 - val_loss: 1833.8157\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 837.5082 - val_loss: 1836.9543\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 849.1378 - val_loss: 1844.4583\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 823.8915 - val_loss: 1843.1896\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 840.5400 - val_loss: 1842.0956\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 869.4869 - val_loss: 1843.0583\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 884.0702 - val_loss: 1843.8606\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 865.7542 - val_loss: 1845.3575\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 857.4900 - val_loss: 1845.4254\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 870.8466 - val_loss: 1845.4198\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 880.6235 - val_loss: 1845.8457\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 843.6902 - val_loss: 1844.3494\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 858.6320 - val_loss: 1843.5189\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 816.7306 - val_loss: 1843.5776\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 863.7831 - val_loss: 1843.5730\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 863.3037 - val_loss: 1843.5204\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 4.2263 - val_loss: 4.9657\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1476 - val_loss: 4.9157\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1152 - val_loss: 4.8696\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1027 - val_loss: 4.8687\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0918 - val_loss: 4.8301\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0620 - val_loss: 4.8258\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0621 - val_loss: 4.8163\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0455 - val_loss: 4.8078\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0294 - val_loss: 4.8139\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0404 - val_loss: 4.8334\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0271 - val_loss: 4.8044\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0204 - val_loss: 4.8084\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0473 - val_loss: 4.8023\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0422 - val_loss: 4.8002\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0277 - val_loss: 4.7976\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0308 - val_loss: 4.7945\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0290 - val_loss: 4.7979\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0335 - val_loss: 4.8004\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0365 - val_loss: 4.7938\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0356 - val_loss: 4.8024\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0157 - val_loss: 4.7947\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0418 - val_loss: 4.7953\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0167 - val_loss: 4.7932\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0178 - val_loss: 4.8134\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0141 - val_loss: 4.7903\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0127 - val_loss: 4.7953\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0169 - val_loss: 4.7927\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0373 - val_loss: 4.7876\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9993 - val_loss: 4.7932\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0180 - val_loss: 4.7913\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0424 - val_loss: 4.7913\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0089 - val_loss: 4.7884\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0425 - val_loss: 4.7919\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0407 - val_loss: 4.7953\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0106 - val_loss: 4.7979\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0176 - val_loss: 4.7946\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9961 - val_loss: 4.7865\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0451 - val_loss: 4.7865\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0312 - val_loss: 4.7880\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0288 - val_loss: 4.7958\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0103 - val_loss: 4.7928\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0142 - val_loss: 4.7905\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0265 - val_loss: 4.7881\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0360 - val_loss: 4.7865\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0253 - val_loss: 4.7883\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0301 - val_loss: 4.7869\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0210 - val_loss: 4.7994\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9483 - val_loss: 4.7144\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9615 - val_loss: 4.7150\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9499 - val_loss: 4.7145\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 26ms/step - loss: 7535.4677 - val_loss: 15292.8164\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7307.9533 - val_loss: 15292.7588\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7711.8434 - val_loss: 15292.7295\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7610.9023 - val_loss: 15292.7021\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7753.6008 - val_loss: 15292.6660\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7632.4047 - val_loss: 15292.6494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8031.7568 - val_loss: 15292.6289\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7613.3094 - val_loss: 15292.6309\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7302.6616 - val_loss: 15292.6201\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7626.8312 - val_loss: 15292.6025\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7760.2511 - val_loss: 15292.5967\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7596.0163 - val_loss: 15292.5898\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7301.4214 - val_loss: 15292.5830\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7482.1143 - val_loss: 15292.5811\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7395.4183 - val_loss: 15292.5898\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7752.6847 - val_loss: 15292.5723\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7524.4627 - val_loss: 15292.5654\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7549.5792 - val_loss: 15292.5654\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7612.9026 - val_loss: 15292.5625\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7953.6621 - val_loss: 15292.5674\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7927.5158 - val_loss: 15292.5615\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7574.7149 - val_loss: 15292.5527\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7662.5263 - val_loss: 15292.5566\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7523.5094 - val_loss: 15292.5566\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7747.9429 - val_loss: 15292.5527\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7649.5198 - val_loss: 15292.5488\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8049.5411 - val_loss: 15292.5557\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7768.9037 - val_loss: 15292.5439\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7415.7031 - val_loss: 15292.5430\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7864.8769 - val_loss: 15292.5488\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7892.2745 - val_loss: 15292.5430\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7914.3029 - val_loss: 15292.5439\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7990.4174 - val_loss: 15292.5430\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7950.3588 - val_loss: 15292.5439\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7501.5991 - val_loss: 15292.5381\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7481.5533 - val_loss: 15292.5400\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7760.5428 - val_loss: 15292.5381\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7549.2733 - val_loss: 15292.5400\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7758.5261 - val_loss: 15292.5381\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7680.9840 - val_loss: 15292.5391\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7790.1041 - val_loss: 15292.5352\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7775.9781 - val_loss: 15292.5381\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7955.6938 - val_loss: 15292.5352\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7872.6401 - val_loss: 15292.5352\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7696.7040 - val_loss: 15292.5342\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7353.0467 - val_loss: 15292.5352\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7840.4705 - val_loss: 15292.5332\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7669.0169 - val_loss: 15292.5332\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7341.1761 - val_loss: 15292.5312\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7480.3650 - val_loss: 15292.5312\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.3086 - val_loss: 4.9292\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1278 - val_loss: 4.9044\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1100 - val_loss: 4.8793\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0626 - val_loss: 4.8712\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0508 - val_loss: 4.8573\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0830 - val_loss: 4.8409\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0692 - val_loss: 4.8303\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0466 - val_loss: 4.8221\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0446 - val_loss: 4.8206\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0341 - val_loss: 4.8102\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0331 - val_loss: 4.8081\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0204 - val_loss: 4.8006\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0279 - val_loss: 4.7939\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0283 - val_loss: 4.7913\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0222 - val_loss: 4.7928\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0180 - val_loss: 4.7832\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0232 - val_loss: 4.7837\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9889 - val_loss: 4.7780\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0008 - val_loss: 4.7750\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0041 - val_loss: 4.7789\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9867 - val_loss: 4.7709\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0139 - val_loss: 4.7683\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9898 - val_loss: 4.7665\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0044 - val_loss: 4.7674\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0035 - val_loss: 4.7630\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9857 - val_loss: 4.7621\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0093 - val_loss: 4.7614\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9810 - val_loss: 4.7608\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9676 - val_loss: 4.7581\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9852 - val_loss: 4.7585\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9802 - val_loss: 4.7572\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9881 - val_loss: 4.7624\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9736 - val_loss: 4.7550\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9883 - val_loss: 4.7847\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9903 - val_loss: 4.7528\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9929 - val_loss: 4.7501\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9699 - val_loss: 4.7498\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9807 - val_loss: 4.7508\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0124 - val_loss: 4.7481\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9796 - val_loss: 4.7493\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9717 - val_loss: 4.7617\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9584 - val_loss: 4.7484\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 3.9922 - val_loss: 4.7450\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9887 - val_loss: 4.7490\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9994 - val_loss: 4.7535\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9935 - val_loss: 4.7445\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9898 - val_loss: 4.7425\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9727 - val_loss: 4.7433\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0033 - val_loss: 4.7455\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9913 - val_loss: 4.7424\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7670.7085 - val_loss: 15292.9736\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7665.4125 - val_loss: 15292.9062\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7482.4355 - val_loss: 15292.8516\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7519.0984 - val_loss: 15292.7979\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7449.8955 - val_loss: 15292.7676\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7315.4464 - val_loss: 15292.7500\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7544.0795 - val_loss: 15292.7285\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7486.8776 - val_loss: 15292.7109\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7584.0239 - val_loss: 15292.6855\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7641.4013 - val_loss: 15292.6738\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7568.6043 - val_loss: 15292.6592\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7790.9156 - val_loss: 15292.6426\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7504.4990 - val_loss: 15292.6357\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7347.4354 - val_loss: 15292.6318\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7416.5061 - val_loss: 15292.6152\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7373.3863 - val_loss: 15292.6152\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7073.0606 - val_loss: 15292.6035\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7477.6068 - val_loss: 15292.5967\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7308.3057 - val_loss: 15292.6006\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7366.9925 - val_loss: 15292.5889\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7429.9514 - val_loss: 15292.5811\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7582.5979 - val_loss: 15292.5742\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7691.9513 - val_loss: 15292.5713\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7321.4721 - val_loss: 15292.5664\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7490.6706 - val_loss: 15292.5703\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7459.3273 - val_loss: 15292.5723\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7800.7558 - val_loss: 15292.5664\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7514.2361 - val_loss: 15292.5566\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7649.2220 - val_loss: 15292.5557\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8079.1173 - val_loss: 15292.5498\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7879.6038 - val_loss: 15292.5469\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7758.5388 - val_loss: 15292.5430\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7412.1965 - val_loss: 15292.5400\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8149.1310 - val_loss: 15292.5439\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7957.2962 - val_loss: 15292.5439\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8146.0548 - val_loss: 15292.5479\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7477.9775 - val_loss: 15292.5352\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7442.8747 - val_loss: 15292.5381\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7831.9730 - val_loss: 15292.5371\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7750.7716 - val_loss: 15292.5312\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7573.0967 - val_loss: 15292.5264\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7710.6308 - val_loss: 15292.5273\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7609.8498 - val_loss: 15292.5283\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7437.6549 - val_loss: 15292.5244\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7498.1875 - val_loss: 15292.5303\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7600.1837 - val_loss: 15292.5176\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7875.4789 - val_loss: 15292.5166\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7565.1825 - val_loss: 15292.5166\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7286.1493 - val_loss: 15292.5176\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7734.4518 - val_loss: 15292.5264\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: 30.6135\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 25.5282 - val_loss: 25.3885\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 21.6206 - val_loss: 22.4289\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 18.9898 - val_loss: 20.3923\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 17.5704 - val_loss: 18.8684\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 16.1314 - val_loss: 17.6679\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 15.1895 - val_loss: 16.6937\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 14.3978 - val_loss: 15.8851\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13.7283 - val_loss: 15.2027\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 13.1461 - val_loss: 14.6190\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 28492.3906\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13617.2138 - val_loss: 26011.1270\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 12707.8181 - val_loss: 24579.4043\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 12403.4399 - val_loss: 23587.6562\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11798.8951 - val_loss: 22841.5117\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11314.6574 - val_loss: 22255.3945\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11601.5285 - val_loss: 21780.9883\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11079.7364 - val_loss: 21387.8359\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11247.0731 - val_loss: 21056.1934\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 10740.7602 - val_loss: 20771.4961\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 52.4437 - val_loss: 6.7179\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.8965 - val_loss: 3.1585\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.7197 - val_loss: 2.9504\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0354 - val_loss: 2.9168\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.2189 - val_loss: 3.0009\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.9964 - val_loss: 3.1487\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.6567 - val_loss: 3.1970\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.5967 - val_loss: 4.7421\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.2458 - val_loss: 3.1348\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.3374 - val_loss: 3.0888\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 5.6381 - val_loss: 4.5512\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 5.1537 - val_loss: 3.0945\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.5615 - val_loss: 2.9852\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 5.9815 - val_loss: 2.9560\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.3730 - val_loss: 2.5275\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1816 - val_loss: 2.5422\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1465 - val_loss: 2.4876\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1558 - val_loss: 2.4995\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1399 - val_loss: 2.5062\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1267 - val_loss: 2.5506\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1211 - val_loss: 2.4776\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1175 - val_loss: 2.4992\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1098 - val_loss: 2.4916\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1176 - val_loss: 2.5134\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1177 - val_loss: 2.4808\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1192 - val_loss: 2.4791\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1069 - val_loss: 2.4873\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0995 - val_loss: 2.5055\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1136 - val_loss: 2.5184\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0991 - val_loss: 2.4924\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1013 - val_loss: 2.5006\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0652 - val_loss: 2.4652\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0621 - val_loss: 2.4655\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0566 - val_loss: 2.4589\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0599 - val_loss: 2.4629\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0575 - val_loss: 2.4616\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0584 - val_loss: 2.4632\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0640 - val_loss: 2.4629\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0630 - val_loss: 2.4631\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0652 - val_loss: 2.4620\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0543 - val_loss: 2.4578\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0569 - val_loss: 2.4622\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0588 - val_loss: 2.4607\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0576 - val_loss: 2.4637\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0577 - val_loss: 2.4602\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0595 - val_loss: 2.4602\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2.0581 - val_loss: 2.4639\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0582 - val_loss: 2.4570\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0589 - val_loss: 2.4572\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0600 - val_loss: 2.4627\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 1633.0371 - val_loss: 1833.7012\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 984.7590 - val_loss: 1850.3297\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 909.8960 - val_loss: 1689.7264\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 902.6263 - val_loss: 1846.5494\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 876.5055 - val_loss: 1876.0057\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.3683 - val_loss: 1762.4768\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 834.7009 - val_loss: 2046.6578\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 895.2667 - val_loss: 1665.3225\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 888.5183 - val_loss: 1930.9243\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 925.3862 - val_loss: 1816.3958\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 879.9662 - val_loss: 1998.6890\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 883.7343 - val_loss: 1721.2056\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 867.5276 - val_loss: 1781.3680\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 863.5423 - val_loss: 1726.8679\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 852.0242 - val_loss: 1815.7692\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 918.3834 - val_loss: 1997.3400\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 940.6275 - val_loss: 1835.7699\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.3814 - val_loss: 1786.2725\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.1728 - val_loss: 1848.7430\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 859.7597 - val_loss: 1871.5481\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 835.8238 - val_loss: 1831.2389\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 829.7753 - val_loss: 1816.1398\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 854.7099 - val_loss: 1815.1272\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 833.8657 - val_loss: 1831.2545\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 848.9986 - val_loss: 1821.8838\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 853.6850 - val_loss: 1864.9032\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 896.5700 - val_loss: 1825.7649\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 860.7074 - val_loss: 1841.6177\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 880.8345 - val_loss: 1841.7578\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 894.6365 - val_loss: 1846.2665\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 823.1524 - val_loss: 1842.2067\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.7338 - val_loss: 1842.5985\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 850.4980 - val_loss: 1845.5956\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.0588 - val_loss: 1845.0479\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 871.4073 - val_loss: 1843.1947\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 850.8613 - val_loss: 1844.1074\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 843.5238 - val_loss: 1841.8358\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 829.0529 - val_loss: 1840.9951\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 865.0873 - val_loss: 1841.0502\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 850.8031 - val_loss: 1841.4260\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 882.6051 - val_loss: 1841.5806\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 803.3018 - val_loss: 1841.5985\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 845.8348 - val_loss: 1841.5785\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 879.3117 - val_loss: 1841.7454\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 827.9129 - val_loss: 1841.7399\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 834.3552 - val_loss: 1841.6807\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 939.0489 - val_loss: 1841.9735\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 886.9672 - val_loss: 1842.0646\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 890.6554 - val_loss: 1842.0575\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 856.5727 - val_loss: 1842.0592\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 3.8101 - val_loss: 2.5977\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1585 - val_loss: 2.4889\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0979 - val_loss: 2.5064\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0937 - val_loss: 2.5021\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0742 - val_loss: 2.4885\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0871 - val_loss: 2.4917\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0815 - val_loss: 2.4751\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0850 - val_loss: 2.4793\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0795 - val_loss: 2.4704\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0892 - val_loss: 2.4841\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0821 - val_loss: 2.4724\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0910 - val_loss: 2.4914\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0950 - val_loss: 2.4931\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0936 - val_loss: 2.4850\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0972 - val_loss: 2.4809\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0924 - val_loss: 2.5129\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0890 - val_loss: 2.4844\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0927 - val_loss: 2.4990\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0976 - val_loss: 2.4995\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0717 - val_loss: 2.4807\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0560 - val_loss: 2.4727\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0643 - val_loss: 2.4654\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0616 - val_loss: 2.4621\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0531 - val_loss: 2.4603\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0552 - val_loss: 2.4592\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0581 - val_loss: 2.4646\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0544 - val_loss: 2.4632\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0526 - val_loss: 2.4616\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0561 - val_loss: 2.4631\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0612 - val_loss: 2.4676\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0648 - val_loss: 2.4574\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0602 - val_loss: 2.4673\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0562 - val_loss: 2.4591\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0568 - val_loss: 2.4602\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0588 - val_loss: 2.4696\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0627 - val_loss: 2.4599\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0575 - val_loss: 2.4610\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0563 - val_loss: 2.4666\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0598 - val_loss: 2.4660\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0573 - val_loss: 2.4605\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0576 - val_loss: 2.4686\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0576 - val_loss: 2.4602\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0583 - val_loss: 2.4622\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0589 - val_loss: 2.4616\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0540 - val_loss: 2.4596\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0497 - val_loss: 2.4613\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0615 - val_loss: 2.4616\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0557 - val_loss: 2.4610\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0525 - val_loss: 2.4616\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0569 - val_loss: 2.4605\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 3112.6631 - val_loss: 2413.5100\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 1071.8696 - val_loss: 2199.3867\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 910.8301 - val_loss: 1965.5878\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 875.2678 - val_loss: 1795.0419\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 937.0533 - val_loss: 1937.8900\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 879.2385 - val_loss: 1849.6566\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 893.4522 - val_loss: 1998.1621\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 868.7521 - val_loss: 1863.0125\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 858.7626 - val_loss: 1837.1703\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 849.0900 - val_loss: 1950.0914\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 827.7899 - val_loss: 1818.0930\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 869.0302 - val_loss: 1827.2954\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 891.4290 - val_loss: 1854.6865\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 847.0759 - val_loss: 1703.4829\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 859.1197 - val_loss: 1895.8164\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 875.7402 - val_loss: 1767.1681\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 837.3180 - val_loss: 1891.8954\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 844.2232 - val_loss: 1727.7456\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 900.3021 - val_loss: 1743.5929\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 899.5144 - val_loss: 1968.2402\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 926.9154 - val_loss: 1828.1581\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 858.1950 - val_loss: 1878.1681\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 875.9436 - val_loss: 1854.9142\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 874.4728 - val_loss: 1766.2736\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 846.5036 - val_loss: 1828.4900\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 865.9461 - val_loss: 1841.5034\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 859.5826 - val_loss: 1828.2942\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 826.1048 - val_loss: 1829.7520\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 857.6458 - val_loss: 1840.0209\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 881.3530 - val_loss: 1861.1448\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 826.4771 - val_loss: 1830.8623\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.9788 - val_loss: 1868.8879\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 868.0573 - val_loss: 1845.0229\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 839.1111 - val_loss: 1816.3300\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 840.8299 - val_loss: 1824.0164\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 831.1959 - val_loss: 1828.9492\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 917.1880 - val_loss: 1832.0153\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 844.5893 - val_loss: 1834.4884\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 807.9743 - val_loss: 1835.3582\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 871.6434 - val_loss: 1836.3774\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 846.4833 - val_loss: 1836.2585\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 877.0781 - val_loss: 1838.0293\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 875.9394 - val_loss: 1838.0524\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 876.4098 - val_loss: 1838.0518\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 856.4459 - val_loss: 1838.0616\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.3116 - val_loss: 1838.0802\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 857.0578 - val_loss: 1838.1161\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 899.6725 - val_loss: 1838.1610\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 820.6878 - val_loss: 1838.2778\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 823.4621 - val_loss: 1838.2777\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 3.9714 - val_loss: 4.7006\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9408 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9156 - val_loss: 4.7006\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9227 - val_loss: 4.7008\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9339 - val_loss: 4.7006\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9195 - val_loss: 4.7007\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9547 - val_loss: 4.7016\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9442 - val_loss: 4.7007\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9377 - val_loss: 4.7007\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9198 - val_loss: 4.7007\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9341 - val_loss: 4.7009\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9188 - val_loss: 4.7010\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9136 - val_loss: 4.7009\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9553 - val_loss: 4.7014\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9142 - val_loss: 4.7009\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9306 - val_loss: 4.7011\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9176 - val_loss: 4.7009\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.8875 - val_loss: 4.7008\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9094 - val_loss: 4.7009\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9448 - val_loss: 4.7012\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9340 - val_loss: 4.7012\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9358 - val_loss: 4.7011\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9098 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9316 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9179 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9375 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9167 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9287 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9402 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9132 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9064 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9469 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9251 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9181 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9205 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9435 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9448 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9242 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9230 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9217 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9186 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9289 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9177 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9387 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9036 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9229 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9413 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9279 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9368 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9116 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7573.4711 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7378.2438 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7216.3125 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7478.4773 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7632.4160 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7581.7687 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7432.6900 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7805.7840 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7539.3382 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7571.8357 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7564.1983 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 7580.9276 - val_loss: 15292.4346\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 8121.4412 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7635.1403 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7806.5900 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7539.9005 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7882.0448 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7289.1961 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7299.3821 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7697.1871 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7324.4265 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7512.3928 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7323.5205 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7447.6256 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7910.9728 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7478.8747 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7524.3014 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7669.9620 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7619.9873 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7463.9183 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7738.1185 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7876.4321 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7718.9029 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8138.5922 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7512.6399 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7599.8032 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7390.0133 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7922.5962 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7482.4317 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7711.0468 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7752.8353 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7234.2536 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7737.8585 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7734.8759 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7469.9987 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7505.3519 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7490.9146 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7617.7371 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7582.6744 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7634.6464 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 4.0743 - val_loss: 4.7010\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9180 - val_loss: 4.7008\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9123 - val_loss: 4.7010\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9277 - val_loss: 4.7009\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9334 - val_loss: 4.7013\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9488 - val_loss: 4.7008\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9300 - val_loss: 4.7008\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9143 - val_loss: 4.7007\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9366 - val_loss: 4.7010\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9208 - val_loss: 4.7011\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.8975 - val_loss: 4.7008\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9487 - val_loss: 4.7012\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9423 - val_loss: 4.7011\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9243 - val_loss: 4.7011\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9361 - val_loss: 4.7011\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9211 - val_loss: 4.7010\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9336 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9430 - val_loss: 4.7011\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9552 - val_loss: 4.7011\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9478 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9279 - val_loss: 4.7010\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9253 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9008 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9094 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9195 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9371 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9134 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9371 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9354 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9392 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9523 - val_loss: 4.7010\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9100 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9306 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9462 - val_loss: 4.7010\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9494 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9287 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9266 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9559 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9166 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9406 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9292 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9423 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9197 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9199 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9365 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9321 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9321 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9596 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9120 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9188 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 7712.2775 - val_loss: 15292.4414\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7472.7162 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7528.0588 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7576.6964 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7517.0825 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7470.9341 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7572.7190 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7673.6652 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7355.1211 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7831.4224 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7564.2815 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7362.3737 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7562.5210 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7816.7035 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7766.4132 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7597.9204 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7525.4752 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8028.4214 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7493.9202 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7822.2252 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7370.4510 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7513.9294 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8206.2361 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7515.0049 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7616.6751 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7429.4068 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7695.4846 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7625.1705 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7475.3010 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7295.9186 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7788.5033 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7658.2270 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7593.8499 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7536.1758 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7564.4699 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7501.7406 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7277.6367 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7498.4014 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7243.7658 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7345.5863 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7470.6372 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7731.5476 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7354.4089 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7658.0684 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7634.9532 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7713.3889 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7388.6895 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7946.1316 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7595.4411 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7324.8982 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 25.5336 - val_loss: 4.6947\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.8366 - val_loss: 3.2558\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8.5717 - val_loss: 3.5722\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 6.1596 - val_loss: 4.5784\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.9110 - val_loss: 4.9451\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8.4818 - val_loss: 3.2035\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8.5618 - val_loss: 4.7249\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 14.8480 - val_loss: 6.9678\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 17.2625 - val_loss: 39.9429\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 11.3880 - val_loss: 5.1707\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8.6642 - val_loss: 39.8743\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 8.0707 - val_loss: 5.6394\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.9673 - val_loss: 7.3551\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 6.3621 - val_loss: 4.7396\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 6.5645 - val_loss: 4.6801\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 8.7613 - val_loss: 8.2629\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 5.2535 - val_loss: 3.7311\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.1028 - val_loss: 3.6002\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.1600 - val_loss: 3.5092\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.4578 - val_loss: 3.4900\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.2004 - val_loss: 3.4611\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.9737 - val_loss: 3.4269\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.0201 - val_loss: 3.4082\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.9685 - val_loss: 3.3956\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.0531 - val_loss: 3.3880\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.9537 - val_loss: 3.3816\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.0057 - val_loss: 3.3713\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8343 - val_loss: 3.3755\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.8433 - val_loss: 3.3658\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.8325 - val_loss: 3.3675\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.8353 - val_loss: 3.3672\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.8184 - val_loss: 3.3592\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.8278 - val_loss: 3.3604\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.8235 - val_loss: 3.3577\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.8140 - val_loss: 3.3580\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.8229 - val_loss: 3.3545\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8273 - val_loss: 3.3539\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.8178 - val_loss: 3.3536\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 2.8307 - val_loss: 3.3528\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.8156 - val_loss: 3.3532\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8085 - val_loss: 3.3525\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8314 - val_loss: 3.3536\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8027 - val_loss: 3.3531\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8122 - val_loss: 3.3538\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.8310 - val_loss: 3.3538\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8208 - val_loss: 3.3533\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.8127 - val_loss: 3.3532\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8098 - val_loss: 3.3532\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8180 - val_loss: 3.3531\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.8082 - val_loss: 3.3531\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 1703.8714 - val_loss: 1813.8204\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 934.4581 - val_loss: 1981.3047\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 883.8912 - val_loss: 2064.5815\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 916.7830 - val_loss: 1899.3915\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 858.8311 - val_loss: 1999.5090\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 909.0924 - val_loss: 1865.0979\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 900.8316 - val_loss: 1676.5516\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 868.7359 - val_loss: 1760.9725\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 884.1826 - val_loss: 1942.0912\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 935.1156 - val_loss: 2174.9041\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 911.2128 - val_loss: 1879.7959\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 937.1966 - val_loss: 1870.2256\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 936.6397 - val_loss: 2025.2788\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 969.6294 - val_loss: 1895.7303\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 899.8479 - val_loss: 1810.9401\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 905.0590 - val_loss: 1850.9303\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 901.5600 - val_loss: 1724.2017\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 873.5344 - val_loss: 1819.1454\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 868.2512 - val_loss: 1858.8505\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 862.9869 - val_loss: 1868.5837\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 859.8439 - val_loss: 1830.8009\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 833.4283 - val_loss: 1839.6056\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 855.0342 - val_loss: 1828.8533\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 875.9790 - val_loss: 1860.9587\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 905.8259 - val_loss: 1866.4050\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 841.6019 - val_loss: 1827.0042\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 908.6006 - val_loss: 1875.2383\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 864.9214 - val_loss: 1862.2954\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 864.9474 - val_loss: 1858.4502\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 835.0462 - val_loss: 1848.5898\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 864.8588 - val_loss: 1849.0343\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 842.8644 - val_loss: 1845.0050\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 802.6742 - val_loss: 1842.4014\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 881.5217 - val_loss: 1841.5331\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 876.8452 - val_loss: 1844.1172\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 819.2782 - val_loss: 1841.4296\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 843.4553 - val_loss: 1838.2140\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 861.7388 - val_loss: 1838.9019\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 856.5413 - val_loss: 1839.3124\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 826.4712 - val_loss: 1839.5449\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 807.9536 - val_loss: 1839.7369\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 875.6326 - val_loss: 1839.9669\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 808.7336 - val_loss: 1840.6509\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 845.4027 - val_loss: 1840.7810\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 811.1941 - val_loss: 1840.8279\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 837.0739 - val_loss: 1840.7284\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 856.7641 - val_loss: 1840.6229\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 833.3974 - val_loss: 1840.6204\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 830.7530 - val_loss: 1840.6338\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 833.8930 - val_loss: 1840.6345\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 4.4094 - val_loss: 2.7755\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3207 - val_loss: 2.6401\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.2774 - val_loss: 2.7224\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2554 - val_loss: 2.6217\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2195 - val_loss: 2.6176\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2180 - val_loss: 2.6052\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2417 - val_loss: 2.5995\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: 2.2145 - val_loss: 2.5732\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2375 - val_loss: 2.5682\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2196 - val_loss: 2.6240\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2026 - val_loss: 2.5495\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5292 - val_loss: 2.5550\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2339 - val_loss: 2.5608\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2274 - val_loss: 2.5518\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2054 - val_loss: 2.6725\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1498 - val_loss: 2.5210\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1415 - val_loss: 2.5117\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4597 - val_loss: 2.4842\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2007 - val_loss: 2.5036\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.3660 - val_loss: 2.5130\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5215 - val_loss: 2.4906\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2601 - val_loss: 2.4783\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2676 - val_loss: 2.4700\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3723 - val_loss: 6.9028\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3605 - val_loss: 2.4941\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.4532 - val_loss: 2.4578\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3962 - val_loss: 4.5551\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5537 - val_loss: 2.4917\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3577 - val_loss: 2.7497\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2059 - val_loss: 2.4546\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2941 - val_loss: 2.4930\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8192 - val_loss: 2.4664\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2860 - val_loss: 2.4618\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4430 - val_loss: 2.9366\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.9686 - val_loss: 2.4729\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.6382 - val_loss: 2.4599\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4203 - val_loss: 2.5071\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5469 - val_loss: 2.4755\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.5436 - val_loss: 2.4611\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2112 - val_loss: 2.4757\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0548 - val_loss: 2.4475\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0396 - val_loss: 2.4493\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0512 - val_loss: 2.4496\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0454 - val_loss: 2.4514\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0370 - val_loss: 2.4518\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0416 - val_loss: 2.4506\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0429 - val_loss: 2.4502\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0612 - val_loss: 2.4491\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0484 - val_loss: 2.4508\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.0353 - val_loss: 2.4445\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3217.5273 - val_loss: 2357.7754\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 1093.3406 - val_loss: 2082.0662\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 927.2815 - val_loss: 1917.8495\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 866.6858 - val_loss: 1932.6680\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 926.7517 - val_loss: 1863.7684\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 859.7313 - val_loss: 1815.6195\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 830.6915 - val_loss: 1857.2264\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 837.7015 - val_loss: 1793.8685\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 878.3601 - val_loss: 1931.4907\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 909.7290 - val_loss: 1835.3854\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 886.0409 - val_loss: 1805.8213\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 889.8561 - val_loss: 1816.4854\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 901.1859 - val_loss: 1915.0022\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 808.3806 - val_loss: 1842.9919\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 882.8669 - val_loss: 1780.4736\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 910.2291 - val_loss: 1877.2870\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 874.9055 - val_loss: 1907.5442\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 850.4370 - val_loss: 1796.4608\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 875.7160 - val_loss: 1886.2814\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 817.9521 - val_loss: 1766.5560\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 853.3793 - val_loss: 1859.4608\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 901.3461 - val_loss: 1851.3457\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 911.8200 - val_loss: 1879.8840\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 801.6089 - val_loss: 1774.1494\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 880.0973 - val_loss: 1780.2870\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 889.6995 - val_loss: 1932.1864\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 921.4802 - val_loss: 1867.9918\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 818.9292 - val_loss: 1831.1096\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 849.3995 - val_loss: 1893.5232\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 867.7829 - val_loss: 1846.9755\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 845.6351 - val_loss: 1850.7533\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 892.6855 - val_loss: 1850.6350\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 849.9513 - val_loss: 1850.5833\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 799.4446 - val_loss: 1829.5223\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 824.2267 - val_loss: 1835.8103\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 856.2500 - val_loss: 1854.6450\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 841.7555 - val_loss: 1837.5756\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 825.6267 - val_loss: 1835.4164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 873.5162 - val_loss: 1846.9030\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 826.0665 - val_loss: 1841.0618\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 807.5874 - val_loss: 1841.8187\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 907.3960 - val_loss: 1843.0718\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 856.6579 - val_loss: 1843.3097\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 819.4523 - val_loss: 1843.1178\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 846.8016 - val_loss: 1842.8123\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 825.7411 - val_loss: 1842.7545\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 830.3280 - val_loss: 1841.8700\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 842.3492 - val_loss: 1842.8240\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 845.3695 - val_loss: 1842.6907\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 876.1590 - val_loss: 1843.5907\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.2679 - val_loss: 4.9392\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1705 - val_loss: 4.9172\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1186 - val_loss: 4.8654\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.1203 - val_loss: 4.8588\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0563 - val_loss: 4.8260\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0605 - val_loss: 4.8227\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0479 - val_loss: 4.8118\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0501 - val_loss: 4.8084\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0515 - val_loss: 4.8111\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0363 - val_loss: 4.8070\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0422 - val_loss: 4.8006\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0441 - val_loss: 4.8092\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0592 - val_loss: 4.8031\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0238 - val_loss: 4.8173\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0095 - val_loss: 4.8011\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0324 - val_loss: 4.8013\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0038 - val_loss: 4.7954\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0279 - val_loss: 4.7981\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0168 - val_loss: 4.7977\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0275 - val_loss: 4.7931\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0164 - val_loss: 4.7960\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0204 - val_loss: 4.7925\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0314 - val_loss: 4.7928\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0564 - val_loss: 4.7913\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0087 - val_loss: 4.7950\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0211 - val_loss: 4.7907\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0413 - val_loss: 4.7938\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0251 - val_loss: 4.7921\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0329 - val_loss: 4.7906\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0487 - val_loss: 4.7956\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0575 - val_loss: 4.7895\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0316 - val_loss: 4.7940\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0196 - val_loss: 4.7907\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0005 - val_loss: 4.7892\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0209 - val_loss: 4.7918\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0138 - val_loss: 4.7883\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0183 - val_loss: 4.7910\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9899 - val_loss: 4.7898\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0210 - val_loss: 4.7931\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0311 - val_loss: 4.7918\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0086 - val_loss: 4.7891\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9956 - val_loss: 4.7881\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0312 - val_loss: 4.7912\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9997 - val_loss: 4.7874\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9941 - val_loss: 4.7887\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0184 - val_loss: 4.7896\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0027 - val_loss: 4.7880\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 4.0441 - val_loss: 4.7980\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0315 - val_loss: 4.7855\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0382 - val_loss: 4.7903\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7374.0467 - val_loss: 15292.8076\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7660.9440 - val_loss: 15292.7461\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7577.7586 - val_loss: 15292.7402\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7680.1945 - val_loss: 15292.7100\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7401.1249 - val_loss: 15292.6787\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7686.2797 - val_loss: 15292.6514\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7992.4922 - val_loss: 15292.6465\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7356.1920 - val_loss: 15292.6309\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7470.0530 - val_loss: 15292.6152\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7932.7375 - val_loss: 15292.6172\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7374.5836 - val_loss: 15292.6055\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7490.0731 - val_loss: 15292.5996\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7686.6037 - val_loss: 15292.5957\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7389.4984 - val_loss: 15292.5850\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 7395.3558 - val_loss: 15292.5889\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7989.1089 - val_loss: 15292.5801\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7301.5019 - val_loss: 15292.5781\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7530.8118 - val_loss: 15292.5703\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7577.6832 - val_loss: 15292.5654\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7899.2647 - val_loss: 15292.5645\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8023.9252 - val_loss: 15292.5674\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7683.7858 - val_loss: 15292.5635\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7706.1522 - val_loss: 15292.5625\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7627.7664 - val_loss: 15292.5615\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8157.4662 - val_loss: 15292.5527\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7989.6345 - val_loss: 15292.5518\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7522.9201 - val_loss: 15292.5547\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7687.3021 - val_loss: 15292.5488\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7458.4898 - val_loss: 15292.5469\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7572.2778 - val_loss: 15292.5488\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8029.7650 - val_loss: 15292.5459\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7881.6252 - val_loss: 15292.5469\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7521.9135 - val_loss: 15292.5439\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7589.6450 - val_loss: 15292.5469\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7939.1185 - val_loss: 15292.5410\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7709.0652 - val_loss: 15292.5430\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7351.8379 - val_loss: 15292.5400\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7654.3481 - val_loss: 15292.5430\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7267.6263 - val_loss: 15292.5430\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7181.3216 - val_loss: 15292.5381\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7556.9754 - val_loss: 15292.5391\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7467.2643 - val_loss: 15292.5381\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7408.2711 - val_loss: 15292.5361\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7928.7157 - val_loss: 15292.5381\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7792.0106 - val_loss: 15292.5352\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7983.5878 - val_loss: 15292.5352\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7744.3897 - val_loss: 15292.5352\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7412.9007 - val_loss: 15292.5361\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7421.6144 - val_loss: 15292.5332\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7574.7922 - val_loss: 15292.5312\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.2737 - val_loss: 4.9283\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1192 - val_loss: 4.9000\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1308 - val_loss: 4.8809\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0973 - val_loss: 4.8649\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0599 - val_loss: 4.8667\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0599 - val_loss: 4.8418\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0649 - val_loss: 4.8321\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0599 - val_loss: 4.8225\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0532 - val_loss: 4.8150\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0035 - val_loss: 4.8097\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0488 - val_loss: 4.8042\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0662 - val_loss: 4.8264\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0279 - val_loss: 4.7967\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0198 - val_loss: 4.7901\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0264 - val_loss: 4.7868\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0039 - val_loss: 4.7847\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0290 - val_loss: 4.7809\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0108 - val_loss: 4.7781\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0031 - val_loss: 4.7759\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0200 - val_loss: 4.7790\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9982 - val_loss: 4.7705\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9738 - val_loss: 4.7726\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0079 - val_loss: 4.7779\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9904 - val_loss: 4.7646\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9829 - val_loss: 4.7642\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9917 - val_loss: 4.7878\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0030 - val_loss: 4.7630\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0069 - val_loss: 4.7594\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9766 - val_loss: 4.8429\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0158 - val_loss: 4.7584\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9844 - val_loss: 4.7557\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0080 - val_loss: 4.7591\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9901 - val_loss: 4.7548\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9557 - val_loss: 4.7550\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9811 - val_loss: 4.7533\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9636 - val_loss: 4.7539\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9645 - val_loss: 4.7499\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9951 - val_loss: 4.7510\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9925 - val_loss: 4.7488\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0056 - val_loss: 4.7509\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9849 - val_loss: 4.7466\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9864 - val_loss: 4.7471\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9588 - val_loss: 4.7460\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9881 - val_loss: 4.7459\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9577 - val_loss: 4.7502\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9540 - val_loss: 4.7495\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9650 - val_loss: 4.7473\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9670 - val_loss: 4.7447\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9789 - val_loss: 4.7458\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9737 - val_loss: 4.7434\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7555.3829 - val_loss: 15292.9707\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7529.8430 - val_loss: 15292.9102\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7871.7125 - val_loss: 15292.8711\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7698.5667 - val_loss: 15292.8076\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7968.9985 - val_loss: 15292.7842\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7497.6010 - val_loss: 15292.7432\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7779.8351 - val_loss: 15292.7227\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7302.0344 - val_loss: 15292.7021\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7363.4521 - val_loss: 15292.6826\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7795.6796 - val_loss: 15292.6797\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8089.4818 - val_loss: 15292.6582\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7479.2262 - val_loss: 15292.6572\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7368.3081 - val_loss: 15292.6377\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7405.3797 - val_loss: 15292.6221\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7558.4765 - val_loss: 15292.6211\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7584.5391 - val_loss: 15292.6143\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7882.4326 - val_loss: 15292.6045\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8442.7793 - val_loss: 15292.6074\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7701.3333 - val_loss: 15292.5967\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7520.7714 - val_loss: 15292.5947\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7648.3292 - val_loss: 15292.5781\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7740.4128 - val_loss: 15292.5762\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7800.8034 - val_loss: 15292.5723\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7439.0107 - val_loss: 15292.5713\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7453.5851 - val_loss: 15292.5703\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7576.7989 - val_loss: 15292.5625\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7532.0545 - val_loss: 15292.5586\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7344.5906 - val_loss: 15292.5547\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7867.3299 - val_loss: 15292.5557\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7695.4324 - val_loss: 15292.5547\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7618.6010 - val_loss: 15292.5537\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7771.9694 - val_loss: 15292.5625\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7899.7378 - val_loss: 15292.5459\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7814.3087 - val_loss: 15292.5400\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7818.1669 - val_loss: 15292.5371\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7455.5806 - val_loss: 15292.5400\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7636.9392 - val_loss: 15292.5381\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7794.3318 - val_loss: 15292.5381\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7365.3410 - val_loss: 15292.5352\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7424.2893 - val_loss: 15292.5410\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7799.7960 - val_loss: 15292.5254\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7589.1651 - val_loss: 15292.5312\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7567.7441 - val_loss: 15292.5215\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7915.9761 - val_loss: 15292.5205\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7383.2260 - val_loss: 15292.5244\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7551.8319 - val_loss: 15292.5215\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7371.1785 - val_loss: 15292.5176\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7555.6905 - val_loss: 15292.5176\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7863.1264 - val_loss: 15292.5176\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7314.1507 - val_loss: 15292.5146\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 30.6190\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 25.4460 - val_loss: 25.3868\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 21.6138 - val_loss: 22.4284\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 19.1892 - val_loss: 20.3923\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 17.6521 - val_loss: 18.8690\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 16.3892 - val_loss: 17.6683\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 15.2464 - val_loss: 16.6943\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 14.4446 - val_loss: 15.8852\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 13.6915 - val_loss: 15.2025\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 13.2263 - val_loss: 14.6186\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 19.2139 - val_loss: 6.1991\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.4383 - val_loss: 2.9474\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.6189 - val_loss: 4.3125\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.3786 - val_loss: 2.9100\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.0619 - val_loss: 2.9029\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.7920 - val_loss: 3.1160\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.9136 - val_loss: 11.3905\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.5907 - val_loss: 2.9302\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.4061 - val_loss: 5.7514\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.6411 - val_loss: 10.9363\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7.8929 - val_loss: 14.6732\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.5454 - val_loss: 3.0072\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.9441 - val_loss: 4.0129\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.9579 - val_loss: 3.4200\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.4829 - val_loss: 20.5196\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.4160 - val_loss: 2.6728\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.3088 - val_loss: 2.5649\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1891 - val_loss: 2.5019\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1503 - val_loss: 2.5130\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1476 - val_loss: 2.5116\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1398 - val_loss: 2.5022\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1284 - val_loss: 2.5422\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1305 - val_loss: 2.5102\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1124 - val_loss: 2.5010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1311 - val_loss: 2.5379\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1209 - val_loss: 2.4741\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1038 - val_loss: 2.4953\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2.1112 - val_loss: 2.5036\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1088 - val_loss: 2.4982\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1398 - val_loss: 2.5441\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1166 - val_loss: 2.4962\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1107 - val_loss: 2.5857\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1184 - val_loss: 2.6027\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1046 - val_loss: 2.4793\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1112 - val_loss: 2.5040\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0916 - val_loss: 2.4772\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0664 - val_loss: 2.4647\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0528 - val_loss: 2.4606\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0592 - val_loss: 2.4582\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0586 - val_loss: 2.4623\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0470 - val_loss: 2.4632\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0580 - val_loss: 2.4636\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0530 - val_loss: 2.4663\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0604 - val_loss: 2.4637\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0563 - val_loss: 2.4641\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0582 - val_loss: 2.4591\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0595 - val_loss: 2.4652\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0582 - val_loss: 2.4628\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0529 - val_loss: 2.4610\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0589 - val_loss: 2.4597\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 1604.1429 - val_loss: 1973.3540\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 953.6765 - val_loss: 1823.9121\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 891.1676 - val_loss: 2119.1897\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 889.6407 - val_loss: 1915.5104\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 908.6732 - val_loss: 2311.7158\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 842.0568 - val_loss: 1883.6401\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 870.7419 - val_loss: 1994.0784\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 898.2950 - val_loss: 1825.0878\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 927.2306 - val_loss: 1923.1415\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 911.1188 - val_loss: 2003.7986\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 906.3221 - val_loss: 1810.3406\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 924.9066 - val_loss: 1989.9918\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 796.9384 - val_loss: 1813.6782\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 867.0002 - val_loss: 1697.7638\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 843.1351 - val_loss: 1796.4529\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 859.0140 - val_loss: 1760.2814\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 884.9719 - val_loss: 1943.8319\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.1332 - val_loss: 1834.6245\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 908.6239 - val_loss: 1756.2443\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 837.7091 - val_loss: 1857.5950\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 869.3446 - val_loss: 1920.2469\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 904.1082 - val_loss: 1927.0881\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 893.3822 - val_loss: 1862.7212\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.9241 - val_loss: 1826.6460\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 899.8636 - val_loss: 1855.4012\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 843.9253 - val_loss: 1835.5695\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 863.7402 - val_loss: 1835.6039\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 802.9538 - val_loss: 1828.6130\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 848.0203 - val_loss: 1828.9536\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.1451 - val_loss: 1850.1263\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 868.0373 - val_loss: 1866.4225\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 879.3834 - val_loss: 1836.2612\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 853.8920 - val_loss: 1842.4556\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 863.3526 - val_loss: 1861.8945\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 876.7290 - val_loss: 1851.9476\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 832.4697 - val_loss: 1849.1926\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.0925 - val_loss: 1846.5939\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 877.5470 - val_loss: 1847.1379\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.3479 - val_loss: 1842.2316\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 827.5655 - val_loss: 1842.6659\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 834.1228 - val_loss: 1841.2725\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 827.7895 - val_loss: 1841.9790\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 835.9153 - val_loss: 1841.3484\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 822.9635 - val_loss: 1840.9888\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 916.4489 - val_loss: 1841.3333\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 855.4084 - val_loss: 1841.4354\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 898.3255 - val_loss: 1841.5577\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 845.5057 - val_loss: 1841.5487\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 894.8566 - val_loss: 1841.6069\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 858.5337 - val_loss: 1841.6404\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 26ms/step - loss: 3.9770 - val_loss: 2.5992\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1373 - val_loss: 2.4985\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0948 - val_loss: 2.4830\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0903 - val_loss: 2.4782\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0933 - val_loss: 2.5035\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0893 - val_loss: 2.4933\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0865 - val_loss: 2.4780\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0862 - val_loss: 2.4852\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0888 - val_loss: 2.4776\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0877 - val_loss: 2.4707\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0865 - val_loss: 2.4873\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0939 - val_loss: 2.4746\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0844 - val_loss: 2.4849\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0860 - val_loss: 2.4714\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0889 - val_loss: 2.4722\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0879 - val_loss: 2.4915\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0985 - val_loss: 2.4810\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0996 - val_loss: 2.4909\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0867 - val_loss: 2.5096\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0959 - val_loss: 2.5203\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0667 - val_loss: 2.4648\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0625 - val_loss: 2.4633\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0593 - val_loss: 2.4619\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0570 - val_loss: 2.4631\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0609 - val_loss: 2.4642\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0589 - val_loss: 2.4677\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0616 - val_loss: 2.4623\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0605 - val_loss: 2.4598\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0626 - val_loss: 2.4676\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0581 - val_loss: 2.4668\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0576 - val_loss: 2.4629\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0576 - val_loss: 2.4668\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0553 - val_loss: 2.4608\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0629 - val_loss: 2.4607\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0578 - val_loss: 2.4584\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0609 - val_loss: 2.4599\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0609 - val_loss: 2.4561\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0544 - val_loss: 2.4654\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0564 - val_loss: 2.4688\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0578 - val_loss: 2.4621\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0588 - val_loss: 2.4712\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0581 - val_loss: 2.4664\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0575 - val_loss: 2.4624\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0653 - val_loss: 2.4546\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0567 - val_loss: 2.4645\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0647 - val_loss: 2.4703\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0659 - val_loss: 2.4795\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0548 - val_loss: 2.4678\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0583 - val_loss: 2.4551\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0652 - val_loss: 2.4725\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2586.4247 - val_loss: 2370.9910\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 1036.0376 - val_loss: 1998.0248\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 942.0719 - val_loss: 1918.0096\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 872.4053 - val_loss: 1881.3257\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 880.6240 - val_loss: 1901.1947\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 884.3318 - val_loss: 1901.4437\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 858.0155 - val_loss: 1813.3633\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 851.5373 - val_loss: 1872.4524\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 857.8383 - val_loss: 1927.8903\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 924.3195 - val_loss: 1823.3672\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.9562 - val_loss: 1835.6732\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 900.7410 - val_loss: 1878.1515\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 852.6017 - val_loss: 1829.2513\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 865.4177 - val_loss: 1891.9331\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.8231 - val_loss: 1724.3530\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 893.9325 - val_loss: 1908.1862\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 869.4791 - val_loss: 1920.4556\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 846.9969 - val_loss: 1938.2189\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 907.7333 - val_loss: 1857.9211\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 907.6536 - val_loss: 1830.5906\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.0664 - val_loss: 1833.2716\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 864.5506 - val_loss: 1857.4421\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 852.8446 - val_loss: 1919.4105\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 823.2825 - val_loss: 1847.1635\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 868.4663 - val_loss: 1867.3213\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 841.7219 - val_loss: 1855.8373\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 885.1503 - val_loss: 1838.1864\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 869.6356 - val_loss: 1847.1732\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 903.7815 - val_loss: 1851.6761\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 866.0838 - val_loss: 1851.6755\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 846.6090 - val_loss: 1842.7474\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 859.6629 - val_loss: 1821.8317\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 820.0835 - val_loss: 1844.5328\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 859.9424 - val_loss: 1849.2855\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 800.0081 - val_loss: 1812.4908\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 868.2299 - val_loss: 1820.8832\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 879.7658 - val_loss: 1826.0680\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 864.9779 - val_loss: 1829.9534\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 842.2844 - val_loss: 1833.1458\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 796.1762 - val_loss: 1833.6565\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 862.0408 - val_loss: 1835.3936\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 836.8528 - val_loss: 1836.2926\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 797.1702 - val_loss: 1836.4679\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 890.0545 - val_loss: 1838.2125\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 895.6130 - val_loss: 1838.2671\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.4703 - val_loss: 1838.3422\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 877.7449 - val_loss: 1838.4819\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 880.0701 - val_loss: 1838.4919\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 26ms/step - loss: 874.4374 - val_loss: 1838.6033\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 855.2307 - val_loss: 1838.7119\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 3.9858 - val_loss: 4.7006\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9073 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9401 - val_loss: 4.7007\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9305 - val_loss: 4.7006\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9295 - val_loss: 4.7007\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9396 - val_loss: 4.7008\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9401 - val_loss: 4.7006\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9369 - val_loss: 4.7018\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9334 - val_loss: 4.7019\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9317 - val_loss: 4.7007\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9492 - val_loss: 4.7007\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9130 - val_loss: 4.7010\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9186 - val_loss: 4.7009\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9263 - val_loss: 4.7010\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9483 - val_loss: 4.7011\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9317 - val_loss: 4.7010\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9310 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9157 - val_loss: 4.7009\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9244 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9220 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9238 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9217 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9395 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9452 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9360 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9371 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9167 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9513 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9187 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9345 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9487 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9264 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9297 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9454 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9045 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9268 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9632 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9383 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9283 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9325 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9264 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9357 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9167 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9531 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9243 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9318 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9364 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8978 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9278 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9393 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 7364.0912 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7774.1397 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7627.1365 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7497.0739 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7704.2471 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7678.6189 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7721.7841 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7735.8105 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7277.4883 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7789.0683 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7785.3156 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7649.7382 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7634.0045 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7533.2901 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7817.1819 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7679.4858 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7672.7873 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7858.4540 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7514.8505 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7599.8471 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7622.8067 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: 7816.9382 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7763.9647 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7314.8643 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7580.9963 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7379.0128 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8121.0221 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7682.3035 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7835.4006 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8043.8658 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7469.1942 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7187.4980 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7600.5129 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7680.1027 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7715.9880 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7676.4792 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7875.4851 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7529.3889 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7585.6159 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7606.4344 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7421.6371 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7600.1540 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7558.0269 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7381.3370 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7256.4873 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7727.4213 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8071.0474 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7842.8821 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7320.4346 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7586.3604 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 4.0382 - val_loss: 4.7009\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9408 - val_loss: 4.7016\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9087 - val_loss: 4.7009\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9340 - val_loss: 4.7008\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9494 - val_loss: 4.7010\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9524 - val_loss: 4.7010\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9308 - val_loss: 4.7011\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9447 - val_loss: 4.7008\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9135 - val_loss: 4.7011\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9179 - val_loss: 4.7008\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9235 - val_loss: 4.7008\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9203 - val_loss: 4.7009\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9416 - val_loss: 4.7014\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9202 - val_loss: 4.7009\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9267 - val_loss: 4.7009\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9361 - val_loss: 4.7009\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9177 - val_loss: 4.7009\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9129 - val_loss: 4.7008\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9060 - val_loss: 4.7008\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9464 - val_loss: 4.7009\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9260 - val_loss: 4.7010\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9358 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9557 - val_loss: 4.7011\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9376 - val_loss: 4.7011\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9049 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9266 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9328 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9124 - val_loss: 4.7009\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9539 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9222 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9134 - val_loss: 4.7010\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9181 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9712 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9409 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9462 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9314 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9261 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9486 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9493 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9169 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9089 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.8995 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9189 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9303 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9354 - val_loss: 4.7010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9279 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9172 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9448 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9296 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9158 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7279.2534 - val_loss: 15292.4375\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7482.6808 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7686.0116 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7506.1924 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7699.1974 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7647.7615 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7801.9969 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7606.8351 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7708.6061 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7503.5729 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7600.7118 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7663.2865 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7628.3531 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8050.9653 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7282.6477 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: 7577.3473 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7630.5803 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7648.7926 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7303.5323 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7770.9952 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7709.5196 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7512.3792 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: 8023.0208 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7842.8034 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7898.2555 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: 7738.4629 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7519.9679 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7686.3917 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7420.4387 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8195.9776 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8001.6426 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7824.0444 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7587.5043 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7893.9066 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7600.3025 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7187.9377 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7313.3574 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7121.4833 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7201.5615 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7463.0395 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7654.9258 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7764.0109 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7912.7582 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7593.6448 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7363.6249 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7519.9362 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7889.1558 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7769.7879 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7279.3605 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7865.5123 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 24.5178 - val_loss: 6.2786\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.2845 - val_loss: 3.1817\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 4.0948 - val_loss: 43.6220\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 12.9327 - val_loss: 6.2061\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.6232 - val_loss: 6.7200\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 6.9359 - val_loss: 4.7656\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 9.8644 - val_loss: 4.9899\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 8.3534 - val_loss: 4.9136\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7.2792 - val_loss: 5.4495\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 8.0871 - val_loss: 4.9234\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 8.9175 - val_loss: 5.9598\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7.2332 - val_loss: 5.3146\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0964 - val_loss: 5.3006\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 4.1337 - val_loss: 5.2993\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 4.1211 - val_loss: 5.2826\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0187 - val_loss: 5.2920\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.1130 - val_loss: 5.2925\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.1112 - val_loss: 5.2766\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.1141 - val_loss: 5.2593\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.1151 - val_loss: 5.2528\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0737 - val_loss: 5.2478\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0068 - val_loss: 5.2284\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0424 - val_loss: 5.2235\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0017 - val_loss: 5.2254\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0492 - val_loss: 5.2238\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.0332 - val_loss: 5.2238\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0680 - val_loss: 5.2262\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0644 - val_loss: 5.2248\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 4.0338 - val_loss: 5.2252\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0132 - val_loss: 5.2243\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0514 - val_loss: 5.2256\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.1010 - val_loss: 5.2275\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9958 - val_loss: 5.2265\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9788 - val_loss: 5.2259\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0311 - val_loss: 5.2255\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0461 - val_loss: 5.2254\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0637 - val_loss: 5.2251\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0547 - val_loss: 5.2251\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0591 - val_loss: 5.2251\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0829 - val_loss: 5.2251\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0610 - val_loss: 5.2249\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9893 - val_loss: 5.2248\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0384 - val_loss: 5.2248\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0453 - val_loss: 5.2248\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0491 - val_loss: 5.2248\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0076 - val_loss: 5.2248\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.1114 - val_loss: 5.2248\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0376 - val_loss: 5.2248\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0542 - val_loss: 5.2248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0263 - val_loss: 5.2248\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 1676.7036 - val_loss: 1877.3754\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 948.0398 - val_loss: 1967.1405\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 884.9068 - val_loss: 1834.7550\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 926.5743 - val_loss: 2012.8843\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 906.4199 - val_loss: 1883.5262\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 967.0528 - val_loss: 2107.7175\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 890.9989 - val_loss: 1983.1830\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 918.2095 - val_loss: 2032.6637\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 904.9451 - val_loss: 1866.6378\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 863.8947 - val_loss: 2219.6328\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 877.5765 - val_loss: 1819.2126\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 858.2911 - val_loss: 1896.6033\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 931.5522 - val_loss: 1948.2510\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 895.5237 - val_loss: 2147.2756\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 860.5127 - val_loss: 1766.3140\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 883.4302 - val_loss: 1895.3262\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 782.4290 - val_loss: 2205.6274\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 922.0310 - val_loss: 2106.4546\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 851.4419 - val_loss: 1717.4401\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 914.5235 - val_loss: 1816.7584\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 925.8813 - val_loss: 1725.1401\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 891.8833 - val_loss: 3041.5830\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 921.7479 - val_loss: 2591.2783\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 887.9498 - val_loss: 2094.7473\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 906.0113 - val_loss: 1965.7942\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 855.4618 - val_loss: 1856.0154\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 900.5458 - val_loss: 1817.9540\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 888.2615 - val_loss: 1840.4500\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 898.5349 - val_loss: 2137.3713\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 852.0492 - val_loss: 1813.5354\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 834.6927 - val_loss: 1811.8472\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 874.7879 - val_loss: 1842.3649\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 821.7993 - val_loss: 1819.8896\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 842.9987 - val_loss: 1841.6344\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 858.5564 - val_loss: 1812.8268\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 882.9870 - val_loss: 1840.0737\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 892.0274 - val_loss: 1847.7894\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 851.6008 - val_loss: 1833.6633\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 816.2017 - val_loss: 1853.6498\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 823.9457 - val_loss: 1839.3572\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 875.6443 - val_loss: 1839.1575\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 868.9803 - val_loss: 1836.5334\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 858.0595 - val_loss: 1835.2496\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 827.3871 - val_loss: 1833.7072\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 843.5339 - val_loss: 1833.4819\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 882.7652 - val_loss: 1835.9418\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 835.7753 - val_loss: 1832.2438\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 835.6105 - val_loss: 1831.2496\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 843.5406 - val_loss: 1833.3301\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 858.4592 - val_loss: 1833.1570\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 3.7758 - val_loss: 2.6980\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.2787 - val_loss: 2.6968\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2480 - val_loss: 2.6423\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.2302 - val_loss: 2.6163\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.2254 - val_loss: 2.6054\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2165 - val_loss: 2.6106\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.2269 - val_loss: 2.6243\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2109 - val_loss: 2.6171\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2169 - val_loss: 2.6071\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1977 - val_loss: 2.5934\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2062 - val_loss: 2.5924\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2015 - val_loss: 2.5962\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2110 - val_loss: 2.6228\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2098 - val_loss: 2.6049\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.1970 - val_loss: 2.5897\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2031 - val_loss: 2.6003\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2051 - val_loss: 2.5947\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2224 - val_loss: 2.5774\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2261 - val_loss: 2.5990\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2990 - val_loss: 2.5969\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2155 - val_loss: 2.5787\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3334 - val_loss: 2.5878\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2450 - val_loss: 2.6042\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2079 - val_loss: 2.5634\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3180 - val_loss: 2.6193\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.2870 - val_loss: 2.5531\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2590 - val_loss: 2.5200\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4089 - val_loss: 2.4783\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2516 - val_loss: 2.5028\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.2022 - val_loss: 2.4843\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2122 - val_loss: 2.5637\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4643 - val_loss: 2.4916\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1472 - val_loss: 2.4913\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1568 - val_loss: 2.5660\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4315 - val_loss: 2.4664\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1935 - val_loss: 2.4703\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3427 - val_loss: 2.4836\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4530 - val_loss: 2.5181\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4628 - val_loss: 2.5121\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2825 - val_loss: 2.4798\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2850 - val_loss: 2.5217\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.8007 - val_loss: 2.4804\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3028 - val_loss: 2.5137\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.3436 - val_loss: 2.4594\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8405 - val_loss: 2.4614\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3139 - val_loss: 2.4481\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5368 - val_loss: 2.4724\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.3517 - val_loss: 2.6675\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5955 - val_loss: 2.4798\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1939 - val_loss: 2.4584\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2552.7028 - val_loss: 2341.7383\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 1076.2958 - val_loss: 2007.8400\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 907.6814 - val_loss: 1878.0626\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 946.1855 - val_loss: 1930.8470\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 882.7450 - val_loss: 1816.7052\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 882.8870 - val_loss: 1844.5524\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 787.0341 - val_loss: 1787.1005\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 881.5708 - val_loss: 1818.0614\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 816.0971 - val_loss: 1810.5205\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 858.5331 - val_loss: 1771.6403\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 879.8983 - val_loss: 1798.5389\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 956.8991 - val_loss: 1951.6456\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 946.1190 - val_loss: 1932.1207\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 862.8899 - val_loss: 1900.5393\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 865.5417 - val_loss: 1911.9268\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 809.7986 - val_loss: 1795.3666\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 844.5253 - val_loss: 1738.2341\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 851.9483 - val_loss: 1943.4109\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 839.2096 - val_loss: 1856.8500\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 826.8566 - val_loss: 1924.1870\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 880.1236 - val_loss: 1793.2854\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 896.9706 - val_loss: 1961.3312\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 899.1240 - val_loss: 1806.1350\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 896.5944 - val_loss: 1834.0736\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 834.0185 - val_loss: 1842.8615\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 843.3996 - val_loss: 1816.8059\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 877.8002 - val_loss: 1787.7882\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 892.1437 - val_loss: 1847.4015\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 839.0977 - val_loss: 1844.1211\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 801.8632 - val_loss: 1823.0870\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 842.8451 - val_loss: 1829.7362\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 862.7560 - val_loss: 1834.3190\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 864.4173 - val_loss: 1837.4976\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 881.6330 - val_loss: 1854.5670\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 830.2407 - val_loss: 1844.4011\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 882.9484 - val_loss: 1840.7452\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 813.0732 - val_loss: 1822.9808\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 823.7147 - val_loss: 1828.7385\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 849.7559 - val_loss: 1834.1340\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 869.5204 - val_loss: 1837.1859\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 861.8894 - val_loss: 1839.2550\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 820.9513 - val_loss: 1839.3611\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 868.6237 - val_loss: 1841.8037\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 869.3319 - val_loss: 1842.1713\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 828.3767 - val_loss: 1842.0425\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 898.9669 - val_loss: 1843.2621\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 840.3494 - val_loss: 1843.0988\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 860.7738 - val_loss: 1843.0273\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 22ms/step - loss: 831.4436 - val_loss: 1842.9401\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 844.6988 - val_loss: 1842.8928\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.2219 - val_loss: 4.9396\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1360 - val_loss: 4.9305\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1455 - val_loss: 4.9029\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1136 - val_loss: 4.8555\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0647 - val_loss: 4.8479\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0805 - val_loss: 4.8504\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0350 - val_loss: 4.8354\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0539 - val_loss: 4.8117\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0252 - val_loss: 4.8051\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0221 - val_loss: 4.8019\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0136 - val_loss: 4.8173\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0301 - val_loss: 4.8032\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0338 - val_loss: 4.8012\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0371 - val_loss: 4.7970\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0712 - val_loss: 4.7974\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0388 - val_loss: 4.7964\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0369 - val_loss: 4.7973\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0266 - val_loss: 4.7937\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0257 - val_loss: 4.8214\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0198 - val_loss: 4.7969\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9901 - val_loss: 4.8009\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0105 - val_loss: 4.7928\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0257 - val_loss: 4.7997\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0389 - val_loss: 4.7937\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0382 - val_loss: 4.8043\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0201 - val_loss: 4.7926\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0402 - val_loss: 4.7891\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0102 - val_loss: 4.7921\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0292 - val_loss: 4.7931\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0222 - val_loss: 4.7936\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0165 - val_loss: 4.7887\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0275 - val_loss: 4.7902\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0493 - val_loss: 4.7906\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0081 - val_loss: 4.7896\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0271 - val_loss: 4.7904\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0120 - val_loss: 4.7913\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0363 - val_loss: 4.7876\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0373 - val_loss: 4.7901\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0248 - val_loss: 4.7908\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0291 - val_loss: 4.7928\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0421 - val_loss: 4.7879\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0064 - val_loss: 4.7906\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0278 - val_loss: 4.7906\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0134 - val_loss: 4.7910\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0271 - val_loss: 4.7900\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0318 - val_loss: 4.7865\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0004 - val_loss: 4.7921\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0288 - val_loss: 4.7912\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0091 - val_loss: 4.7863\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0318 - val_loss: 4.7897\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7536.1766 - val_loss: 15292.8115\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7626.1195 - val_loss: 15292.7559\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7606.4847 - val_loss: 15292.7432\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7445.2399 - val_loss: 15292.7275\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7536.6695 - val_loss: 15292.7002\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7685.3208 - val_loss: 15292.6846\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7479.5747 - val_loss: 15292.6582\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7870.2195 - val_loss: 15292.6426\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7951.3232 - val_loss: 15292.6260\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7828.4254 - val_loss: 15292.6221\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7642.5345 - val_loss: 15292.6035\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7525.4677 - val_loss: 15292.6016\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7563.7951 - val_loss: 15292.5996\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7658.6502 - val_loss: 15292.5859\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7492.3944 - val_loss: 15292.5752\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7789.6003 - val_loss: 15292.5742\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7867.7719 - val_loss: 15292.5703\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7743.2446 - val_loss: 15292.5742\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7346.6773 - val_loss: 15292.5654\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7484.9862 - val_loss: 15292.5674\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7436.4177 - val_loss: 15292.5635\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7509.3572 - val_loss: 15292.5957\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8018.0866 - val_loss: 15292.5615\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7489.2532 - val_loss: 15292.5586\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7501.1334 - val_loss: 15292.5547\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7953.1438 - val_loss: 15292.5527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7876.1202 - val_loss: 15292.5566\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7763.9187 - val_loss: 15292.5547\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7713.0854 - val_loss: 15292.5518\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7523.0149 - val_loss: 15292.5488\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7786.0677 - val_loss: 15292.5449\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7432.0675 - val_loss: 15292.5439\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7766.7733 - val_loss: 15292.5488\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7708.0710 - val_loss: 15292.5439\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 8386.4439 - val_loss: 15292.5430\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7243.9051 - val_loss: 15292.5400\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7814.5814 - val_loss: 15292.5400\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7600.5666 - val_loss: 15292.5381\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7537.0086 - val_loss: 15292.5391\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7388.1314 - val_loss: 15292.5400\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7651.1651 - val_loss: 15292.5371\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7389.0262 - val_loss: 15292.5381\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7851.2990 - val_loss: 15292.5381\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7698.9355 - val_loss: 15292.5361\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7680.3314 - val_loss: 15292.5352\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7285.9612 - val_loss: 15292.5352\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7419.3241 - val_loss: 15292.5332\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7922.8105 - val_loss: 15292.5332\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7283.1879 - val_loss: 15292.5332\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7570.7028 - val_loss: 15292.5312\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.3103 - val_loss: 4.9280\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1609 - val_loss: 4.9029\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0963 - val_loss: 4.8860\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.1027 - val_loss: 4.8653\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0871 - val_loss: 4.8567\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0474 - val_loss: 4.8455\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0604 - val_loss: 4.8322\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0342 - val_loss: 4.8300\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0407 - val_loss: 4.8174\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0702 - val_loss: 4.8085\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0385 - val_loss: 4.8030\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0156 - val_loss: 4.8006\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0104 - val_loss: 4.8024\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0400 - val_loss: 4.7914\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0249 - val_loss: 4.8327\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9876 - val_loss: 4.7870\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0087 - val_loss: 4.7810\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0188 - val_loss: 4.7773\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0079 - val_loss: 4.7805\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0211 - val_loss: 4.7756\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9862 - val_loss: 4.7735\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0082 - val_loss: 4.7692\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0083 - val_loss: 4.7667\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0097 - val_loss: 4.7672\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0241 - val_loss: 4.7671\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0035 - val_loss: 4.7645\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9872 - val_loss: 4.7604\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9763 - val_loss: 4.7620\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9829 - val_loss: 4.7575\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9771 - val_loss: 4.7572\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9924 - val_loss: 4.7688\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0141 - val_loss: 4.7552\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9962 - val_loss: 4.7538\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9836 - val_loss: 4.7551\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9959 - val_loss: 4.7524\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9915 - val_loss: 4.7542\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0007 - val_loss: 4.7501\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9669 - val_loss: 4.7513\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9605 - val_loss: 4.7503\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9980 - val_loss: 4.7479\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9869 - val_loss: 4.7713\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9913 - val_loss: 4.7622\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9648 - val_loss: 4.7518\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9973 - val_loss: 4.7476\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9775 - val_loss: 4.7513\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9886 - val_loss: 4.7465\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9660 - val_loss: 4.7454\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9576 - val_loss: 4.7496\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9694 - val_loss: 4.7430\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9648 - val_loss: 4.7428\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 27ms/step - loss: 7696.6290 - val_loss: 15292.9688\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7956.5938 - val_loss: 15292.8916\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7172.8257 - val_loss: 15292.8623\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 7930.6580 - val_loss: 15292.8242\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 7555.7835 - val_loss: 15292.7744\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7707.9744 - val_loss: 15292.7480\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7852.0142 - val_loss: 15292.7227\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7667.0381 - val_loss: 15292.6982\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7372.4608 - val_loss: 15292.6797\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7933.6633 - val_loss: 15292.6660\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7657.6756 - val_loss: 15292.6572\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7660.1782 - val_loss: 15292.6533\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7602.7417 - val_loss: 15292.6377\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7938.4304 - val_loss: 15292.6230\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7736.6098 - val_loss: 15292.6182\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7611.7511 - val_loss: 15292.6055\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7629.7185 - val_loss: 15292.6084\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7685.2071 - val_loss: 15292.5957\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7580.0889 - val_loss: 15292.5938\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7420.8146 - val_loss: 15292.5918\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7818.9735 - val_loss: 15292.5830\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7717.4771 - val_loss: 15292.5801\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7784.4363 - val_loss: 15292.5762\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7418.6287 - val_loss: 15292.5742\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7582.1538 - val_loss: 15292.5674\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7400.6291 - val_loss: 15292.5645\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7435.9763 - val_loss: 15292.5615\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7617.9641 - val_loss: 15292.5566\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 7796.6482 - val_loss: 15292.5518\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7844.1806 - val_loss: 15292.5518\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8059.4058 - val_loss: 15292.5498\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7728.2097 - val_loss: 15292.5449\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7604.5975 - val_loss: 15292.5469\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7742.6567 - val_loss: 15292.5400\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7541.8487 - val_loss: 15292.5430\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7660.1389 - val_loss: 15292.5391\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8087.8840 - val_loss: 15292.5352\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7312.6187 - val_loss: 15292.5430\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7537.2153 - val_loss: 15292.5410\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7595.2778 - val_loss: 15292.5293\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7837.0235 - val_loss: 15292.5371\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7774.9400 - val_loss: 15292.5264\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7764.8277 - val_loss: 15292.5225\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7539.7011 - val_loss: 15292.5205\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7576.5780 - val_loss: 15292.5215\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7665.4559 - val_loss: 15292.5244\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8012.6665 - val_loss: 15292.5156\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7457.4616 - val_loss: 15292.5205\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8055.3265 - val_loss: 15292.5176\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7775.5945 - val_loss: 15292.5166\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: 30.6223\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 25.6801 - val_loss: 25.3904\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 21.4050 - val_loss: 22.4269\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 19.2746 - val_loss: 20.3953\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 17.4753 - val_loss: 18.8692\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 16.2165 - val_loss: 17.6687\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 15.2528 - val_loss: 16.6939\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 14.4281 - val_loss: 15.8848\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 13.7031 - val_loss: 15.2035\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13.1918 - val_loss: 14.6187\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: 28493.7852\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13948.1448 - val_loss: 26011.8887\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13507.9317 - val_loss: 24579.7734\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11892.0893 - val_loss: 23587.6895\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11629.6659 - val_loss: 22842.1523\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11431.6236 - val_loss: 22255.6973\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11461.4196 - val_loss: 21781.1426\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11284.6583 - val_loss: 21386.6191\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 10666.3821 - val_loss: 21056.3223\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11751.4779 - val_loss: 20771.5156\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 18.8264 - val_loss: 32.6515\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8.0799 - val_loss: 2.8568\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.6278 - val_loss: 21.3879\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.8476 - val_loss: 2.9366\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.5442 - val_loss: 32.2482\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.5917 - val_loss: 21.8360\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.9145 - val_loss: 20.8423\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.9245 - val_loss: 3.1798\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 5.2409 - val_loss: 3.1508\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.9311 - val_loss: 8.2629\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7.9798 - val_loss: 3.1457\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.2358 - val_loss: 2.9081\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.3965 - val_loss: 2.5496\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1737 - val_loss: 2.5050\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1569 - val_loss: 2.5034\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1276 - val_loss: 2.5329\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 2.1123 - val_loss: 2.4889\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1111 - val_loss: 2.4985\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1085 - val_loss: 2.5549\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.1162 - val_loss: 2.4813\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1037 - val_loss: 2.4848\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1038 - val_loss: 2.4762\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1069 - val_loss: 2.4717\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1054 - val_loss: 2.6972\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1137 - val_loss: 2.5009\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1071 - val_loss: 2.4934\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1174 - val_loss: 2.5016\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1062 - val_loss: 2.5351\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1055 - val_loss: 2.5298\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1095 - val_loss: 2.4983\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0984 - val_loss: 2.5488\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1058 - val_loss: 2.4974\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0948 - val_loss: 2.4946\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0683 - val_loss: 2.4695\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0568 - val_loss: 2.4687\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0612 - val_loss: 2.4630\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0599 - val_loss: 2.4606\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0575 - val_loss: 2.4608\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0558 - val_loss: 2.4638\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0514 - val_loss: 2.4590\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0611 - val_loss: 2.4586\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0612 - val_loss: 2.4640\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0694 - val_loss: 2.4654\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0584 - val_loss: 2.4628\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0624 - val_loss: 2.4621\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0662 - val_loss: 2.4684\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0593 - val_loss: 2.4611\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0562 - val_loss: 2.4551\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0577 - val_loss: 2.4570\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0574 - val_loss: 2.4618\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 1569.2903 - val_loss: 1938.6837\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 951.0508 - val_loss: 2416.0618\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 860.3885 - val_loss: 1790.0925\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.4483 - val_loss: 1991.0718\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 904.1676 - val_loss: 1780.3795\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 865.0566 - val_loss: 2032.0205\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 858.0424 - val_loss: 1751.4128\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.0783 - val_loss: 2114.8232\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 852.5877 - val_loss: 1720.9917\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 883.2584 - val_loss: 1809.0986\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 841.0628 - val_loss: 2030.2556\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 938.3013 - val_loss: 2016.4852\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 903.9224 - val_loss: 1863.7660\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 876.6747 - val_loss: 1779.6570\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 892.4968 - val_loss: 1777.6176\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 881.2334 - val_loss: 2105.3572\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 927.8661 - val_loss: 1992.9318\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 870.4040 - val_loss: 1915.3021\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 855.4984 - val_loss: 1798.1923\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 909.6082 - val_loss: 1840.0675\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 820.2105 - val_loss: 1820.2373\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 878.9160 - val_loss: 1857.4727\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 805.1533 - val_loss: 1811.0139\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 826.9242 - val_loss: 1850.1755\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 911.8246 - val_loss: 1863.1571\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 855.9411 - val_loss: 1826.1575\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 876.3158 - val_loss: 1851.4434\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 839.6579 - val_loss: 1857.8491\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 861.4778 - val_loss: 1822.0144\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 858.7056 - val_loss: 1832.3225\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 845.7279 - val_loss: 1835.6494\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 842.8954 - val_loss: 1838.0999\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 859.5863 - val_loss: 1836.9730\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 862.1664 - val_loss: 1839.3855\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 854.8944 - val_loss: 1840.4515\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 907.1925 - val_loss: 1841.4449\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 856.0371 - val_loss: 1841.0325\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 826.1748 - val_loss: 1842.0106\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 881.6806 - val_loss: 1841.5719\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 871.1864 - val_loss: 1841.5875\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 808.8652 - val_loss: 1841.6483\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 894.4629 - val_loss: 1841.8287\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 823.0956 - val_loss: 1841.8562\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 881.1683 - val_loss: 1841.8866\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 908.8397 - val_loss: 1842.0690\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.9849 - val_loss: 1842.0355\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 833.0476 - val_loss: 1842.1305\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 866.6083 - val_loss: 1842.0662\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.3547 - val_loss: 1842.0009\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 853.2983 - val_loss: 1842.0044\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 4.0173 - val_loss: 2.7365\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.2340 - val_loss: 2.5485\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1488 - val_loss: 2.4969\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1077 - val_loss: 2.5000\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1039 - val_loss: 2.4995\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0989 - val_loss: 2.4961\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1005 - val_loss: 2.4953\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0972 - val_loss: 2.4930\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0960 - val_loss: 2.5390\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0972 - val_loss: 2.4864\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0851 - val_loss: 2.4902\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0938 - val_loss: 2.5022\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0965 - val_loss: 2.4710\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0939 - val_loss: 2.4909\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0998 - val_loss: 2.4910\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0961 - val_loss: 2.4795\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0888 - val_loss: 2.5017\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.1035 - val_loss: 2.4910\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0937 - val_loss: 2.4835\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0900 - val_loss: 2.5058\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1082 - val_loss: 2.5148\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0917 - val_loss: 2.4830\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1162 - val_loss: 2.5297\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0884 - val_loss: 2.4656\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0664 - val_loss: 2.4699\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0610 - val_loss: 2.4592\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0551 - val_loss: 2.4615\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0647 - val_loss: 2.4634\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0565 - val_loss: 2.4674\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0630 - val_loss: 2.4564\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0661 - val_loss: 2.4691\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0660 - val_loss: 2.4659\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0554 - val_loss: 2.4683\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0590 - val_loss: 2.4566\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0550 - val_loss: 2.4682\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0590 - val_loss: 2.4592\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0579 - val_loss: 2.4655\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0606 - val_loss: 2.4636\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0621 - val_loss: 2.4728\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0619 - val_loss: 2.4645\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0562 - val_loss: 2.4622\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0633 - val_loss: 2.4634\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0572 - val_loss: 2.4610\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0648 - val_loss: 2.4618\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0588 - val_loss: 2.4625\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0550 - val_loss: 2.4610\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0545 - val_loss: 2.4591\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0636 - val_loss: 2.4613\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0653 - val_loss: 2.4611\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0624 - val_loss: 2.4601\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 2600.7867 - val_loss: 2470.3765\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 1064.9802 - val_loss: 2040.3898\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 928.8019 - val_loss: 1956.1704\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 927.3364 - val_loss: 2000.4806\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 826.5420 - val_loss: 1852.7826\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 882.1001 - val_loss: 1856.4581\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 845.5925 - val_loss: 1873.8436\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 909.7563 - val_loss: 1885.3141\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 820.4052 - val_loss: 1762.2609\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 878.4065 - val_loss: 1878.0299\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 819.3919 - val_loss: 1719.7288\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 853.0786 - val_loss: 1796.2767\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 879.4473 - val_loss: 2042.1427\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 864.6646 - val_loss: 1948.7322\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 867.0711 - val_loss: 1877.0975\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 903.8757 - val_loss: 1847.0721\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 883.8496 - val_loss: 1792.6853\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 859.5466 - val_loss: 1755.1622\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 935.4541 - val_loss: 1779.9934\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 838.0512 - val_loss: 1862.2212\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 915.3137 - val_loss: 1850.1947\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 840.6290 - val_loss: 1817.9395\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.9897 - val_loss: 1835.7682\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 852.1238 - val_loss: 1852.0790\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 904.4045 - val_loss: 1862.2418\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 835.2104 - val_loss: 1851.8326\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 873.4072 - val_loss: 1837.5919\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 839.8320 - val_loss: 1829.3450\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 810.5569 - val_loss: 1831.0530\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 919.1490 - val_loss: 1847.1772\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 833.7393 - val_loss: 1840.8470\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 843.8667 - val_loss: 1840.7446\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 834.2061 - val_loss: 1840.7953\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 896.8585 - val_loss: 1840.7346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 892.7621 - val_loss: 1841.7455\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 844.7067 - val_loss: 1840.7440\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 863.6392 - val_loss: 1840.6981\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 860.1954 - val_loss: 1841.1980\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.8234 - val_loss: 1841.9506\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 908.7420 - val_loss: 1843.1056\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 852.0796 - val_loss: 1842.0996\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 812.3835 - val_loss: 1842.0314\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 829.6999 - val_loss: 1841.9655\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 870.6450 - val_loss: 1841.9845\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 836.5331 - val_loss: 1841.9465\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 910.5763 - val_loss: 1841.9888\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.1745 - val_loss: 1841.9435\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 825.8990 - val_loss: 1841.8757\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 832.3997 - val_loss: 1842.1035\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 922.8112 - val_loss: 1842.0690\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 3.9649 - val_loss: 4.7006\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9275 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9240 - val_loss: 4.7007\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9449 - val_loss: 4.7011\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9161 - val_loss: 4.7010\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9459 - val_loss: 4.7009\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9093 - val_loss: 4.7012\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9419 - val_loss: 4.7008\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9265 - val_loss: 4.7013\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9252 - val_loss: 4.7007\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9239 - val_loss: 4.7022\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9277 - val_loss: 4.7010\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9373 - val_loss: 4.7015\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9037 - val_loss: 4.7011\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9183 - val_loss: 4.7010\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9313 - val_loss: 4.7019\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9666 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9450 - val_loss: 4.7011\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9195 - val_loss: 4.7009\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9334 - val_loss: 4.7009\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9118 - val_loss: 4.7013\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9554 - val_loss: 4.7011\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9614 - val_loss: 4.7011\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9265 - val_loss: 4.7011\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9085 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9127 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9266 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9276 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9480 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9215 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9429 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9282 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9563 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9203 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9403 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9262 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9417 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9401 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9237 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9131 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9177 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9199 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9164 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9159 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9304 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9319 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9375 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9195 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9473 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9276 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 7774.0486 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7897.0387 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7866.8469 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7612.7050 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8042.5975 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7594.1160 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7582.4711 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7883.6216 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7582.6689 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7814.8703 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7491.6078 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7457.1142 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7489.0998 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 8044.6479 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7703.5704 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7266.2480 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7657.0451 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7783.8748 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7676.1976 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7617.1358 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7744.2156 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7435.3922 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7598.4751 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7713.0627 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7436.4396 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7108.2186 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7523.3183 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7412.2749 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7544.7679 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7611.2754 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7853.5315 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 7542.0575 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7878.5302 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7625.8855 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7558.3199 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7982.4060 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7290.1453 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7446.1848 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7516.8052 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8215.5354 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7749.5909 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7340.9133 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7979.0474 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7518.3628 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7741.0724 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7519.9278 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7656.0409 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: 7604.6936 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7558.2317 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7223.1893 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 4.0332 - val_loss: 4.7013\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9231 - val_loss: 4.7009\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9303 - val_loss: 4.7008\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9362 - val_loss: 4.7009\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9442 - val_loss: 4.7008\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9439 - val_loss: 4.7010\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9284 - val_loss: 4.7008\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9303 - val_loss: 4.7010\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9440 - val_loss: 4.7010\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9062 - val_loss: 4.7008\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9478 - val_loss: 4.7008\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9354 - val_loss: 4.7008\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9412 - val_loss: 4.7020\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9028 - val_loss: 4.7012\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9429 - val_loss: 4.7010\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9297 - val_loss: 4.7011\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9191 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9323 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9337 - val_loss: 4.7011\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9248 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9519 - val_loss: 4.7011\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9381 - val_loss: 4.7011\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9001 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9346 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9057 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9472 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9161 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9178 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9201 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9220 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9273 - val_loss: 4.7010\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9100 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9275 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9092 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9439 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9372 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9163 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9368 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9380 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9258 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9507 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9663 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9321 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9363 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9092 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9249 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9299 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9545 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9431 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9553 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 7759.1178 - val_loss: 15292.4414\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7543.4244 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7928.4766 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7707.8868 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7499.9781 - val_loss: 15292.4346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8015.1642 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7832.7982 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7397.5004 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7347.8076 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7847.0505 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7442.3616 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7801.7644 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7494.9876 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7734.8002 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7685.9648 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7326.8445 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7766.2186 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7762.4860 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7602.6412 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7467.1686 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7449.3213 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7673.1967 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7463.0668 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8136.3740 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7585.5264 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7732.6510 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7723.7165 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7496.6355 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7831.7725 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7755.3331 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7691.0038 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7534.7853 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7657.5204 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7646.1755 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7599.8956 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7722.8622 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7635.2759 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7830.0702 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7669.9452 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7439.9974 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7899.4992 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7716.2320 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7634.5275 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7526.5837 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7718.9570 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7624.0132 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7476.4163 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7881.9556 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7888.1078 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7098.5956 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 24ms/step - loss: 54.9060 - val_loss: 11.5509\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7.4598 - val_loss: 4.6153\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.5048 - val_loss: 15.6438\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 15.0017 - val_loss: 6.3040\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7.9039 - val_loss: 5.7656\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 12.0991 - val_loss: 6.5185\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.7296 - val_loss: 34.7287\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 11.2230 - val_loss: 4.8786\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7.6861 - val_loss: 4.8726\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 11.0937 - val_loss: 6.0412\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8.4592 - val_loss: 5.2626\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.8304 - val_loss: 7.9293\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6915 - val_loss: 4.1315\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7101 - val_loss: 4.0808\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.6120 - val_loss: 3.9445\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6801 - val_loss: 3.8873\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.3609 - val_loss: 3.8256\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.4124 - val_loss: 3.7791\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.4985 - val_loss: 18.1597\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.3669 - val_loss: 3.6568\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.3437 - val_loss: 3.6676\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.1464 - val_loss: 3.6678\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.2511 - val_loss: 3.5802\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6844 - val_loss: 3.5809\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.1338 - val_loss: 3.5344\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.3378 - val_loss: 3.5407\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.0169 - val_loss: 3.5446\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.9732 - val_loss: 3.5241\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.1106 - val_loss: 3.5184\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.0776 - val_loss: 3.5132\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.0318 - val_loss: 3.4897\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.0651 - val_loss: 3.5082\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.0401 - val_loss: 3.4617\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.0929 - val_loss: 3.4795\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.9529 - val_loss: 3.4636\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.9319 - val_loss: 3.4656\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.9495 - val_loss: 3.4672\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.0232 - val_loss: 3.4677\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.0940 - val_loss: 3.4168\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.0946 - val_loss: 3.4235\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.0059 - val_loss: 3.4179\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.1013 - val_loss: 3.3943\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8958 - val_loss: 3.3956\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.9358 - val_loss: 3.3767\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.9094 - val_loss: 4.2127\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.9336 - val_loss: 3.3641\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.2106 - val_loss: 3.3672\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.0397 - val_loss: 3.3664\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.1394 - val_loss: 3.3675\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.9819 - val_loss: 3.5485\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 1909.7520 - val_loss: 1961.9534\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 962.7024 - val_loss: 1932.3054\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 920.0239 - val_loss: 1773.3223\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 908.0609 - val_loss: 1803.3843\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 912.8713 - val_loss: 1885.9545\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 920.4846 - val_loss: 1943.1154\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 863.7076 - val_loss: 1737.5410\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 917.3313 - val_loss: 1742.3776\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 878.1358 - val_loss: 1785.4232\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 906.3337 - val_loss: 2012.1703\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 884.6248 - val_loss: 1940.5070\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 932.6509 - val_loss: 1804.5005\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 906.6586 - val_loss: 2138.7014\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 895.1652 - val_loss: 1853.1824\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 927.4129 - val_loss: 1923.3116\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 864.1882 - val_loss: 1771.8627\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 927.0624 - val_loss: 1795.3496\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 899.3975 - val_loss: 1858.5653\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 846.0178 - val_loss: 1820.8221\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 847.1117 - val_loss: 1813.1377\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 852.9946 - val_loss: 1861.4200\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 850.6659 - val_loss: 1839.1759\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 860.3593 - val_loss: 1870.2386\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 828.4737 - val_loss: 1837.2976\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 852.5243 - val_loss: 1823.3772\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 843.8689 - val_loss: 1840.1919\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 855.9625 - val_loss: 1824.4642\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 855.1588 - val_loss: 1828.4274\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 819.2990 - val_loss: 1830.8097\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 864.1483 - val_loss: 1830.8552\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 832.3318 - val_loss: 1831.7113\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 833.6878 - val_loss: 1834.0146\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 843.5747 - val_loss: 1836.0938\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 877.2472 - val_loss: 1837.5859\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 857.0529 - val_loss: 1834.3701\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 861.5485 - val_loss: 1834.6755\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 853.1620 - val_loss: 1835.1908\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 802.8775 - val_loss: 1835.4395\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 865.4318 - val_loss: 1835.4254\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 823.8445 - val_loss: 1835.5187\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 855.3862 - val_loss: 1835.6500\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 854.0507 - val_loss: 1835.6680\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 877.8833 - val_loss: 1835.8649\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 862.3968 - val_loss: 1835.8616\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 899.4310 - val_loss: 1835.8918\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 828.1710 - val_loss: 1836.1273\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 847.9121 - val_loss: 1836.2374\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 854.9236 - val_loss: 1836.2347\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 847.6263 - val_loss: 1836.2311\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 861.4138 - val_loss: 1836.2388\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 4.2000 - val_loss: 2.8084\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3504 - val_loss: 2.6710\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2361 - val_loss: 2.5982\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2178 - val_loss: 2.5997\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2040 - val_loss: 2.6120\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1421 - val_loss: 2.5488\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1371 - val_loss: 2.5448\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1374 - val_loss: 2.5646\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1578 - val_loss: 2.5329\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1766 - val_loss: 2.5347\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1431 - val_loss: 2.5315\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1491 - val_loss: 2.5425\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1564 - val_loss: 2.5664\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.1318 - val_loss: 2.5509\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1191 - val_loss: 2.5083\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1174 - val_loss: 2.5264\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1144 - val_loss: 2.4991\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1669 - val_loss: 3.5514\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1439 - val_loss: 2.5153\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1540 - val_loss: 2.5135\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1391 - val_loss: 2.5063\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1851 - val_loss: 2.5185\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1374 - val_loss: 2.5173\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1388 - val_loss: 2.5257\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2699 - val_loss: 2.5216\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1944 - val_loss: 2.5216\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2179 - val_loss: 2.5300\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0903 - val_loss: 2.5126\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0867 - val_loss: 2.5020\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0934 - val_loss: 2.5036\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0820 - val_loss: 2.4982\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0937 - val_loss: 2.5074\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0838 - val_loss: 2.4998\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0896 - val_loss: 2.5027\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0847 - val_loss: 2.4980\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0841 - val_loss: 2.5044\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0774 - val_loss: 2.4950\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0850 - val_loss: 2.4990\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0826 - val_loss: 2.4920\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0787 - val_loss: 2.4993\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0795 - val_loss: 2.4997\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0820 - val_loss: 2.5047\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0790 - val_loss: 2.4970\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0828 - val_loss: 2.4929\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0740 - val_loss: 2.5007\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0800 - val_loss: 2.4945\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0778 - val_loss: 2.5063\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0775 - val_loss: 2.4970\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0831 - val_loss: 2.5081\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0655 - val_loss: 2.4928\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2786.6053 - val_loss: 2503.1687\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 1031.4417 - val_loss: 2027.5081\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 918.5773 - val_loss: 1905.8203\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 925.0117 - val_loss: 1943.7245\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 872.6275 - val_loss: 1942.9113\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 908.8435 - val_loss: 1971.6658\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 822.3048 - val_loss: 1780.3257\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 872.6965 - val_loss: 1960.0789\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 906.9744 - val_loss: 1765.3075\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 937.0459 - val_loss: 1966.9805\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 896.0065 - val_loss: 1890.2881\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 791.1788 - val_loss: 1741.3114\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 863.2296 - val_loss: 1875.3068\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 849.2306 - val_loss: 1812.4105\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 865.8201 - val_loss: 1775.8286\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 840.4250 - val_loss: 1779.9749\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 839.2176 - val_loss: 1806.4205\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 907.6692 - val_loss: 1915.5223\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 814.6400 - val_loss: 1810.1475\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 905.8948 - val_loss: 1757.0262\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 873.7201 - val_loss: 1813.2769\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 889.3167 - val_loss: 1825.1877\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 843.2997 - val_loss: 1831.9407\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 842.6112 - val_loss: 1850.9792\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 849.0608 - val_loss: 1864.3390\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 835.8792 - val_loss: 1825.6160\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 853.6839 - val_loss: 1821.9945\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 843.8222 - val_loss: 1839.5568\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 883.5438 - val_loss: 1852.2098\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 771.2307 - val_loss: 1816.5880\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 884.9364 - val_loss: 1850.0686\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 814.1821 - val_loss: 1817.0486\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 844.3936 - val_loss: 1826.9421\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 844.1324 - val_loss: 1832.9615\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 845.1681 - val_loss: 1835.7864\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 849.3296 - val_loss: 1837.7108\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 842.7416 - val_loss: 1837.9907\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 886.1764 - val_loss: 1839.4235\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 868.6822 - val_loss: 1840.6942\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 802.9877 - val_loss: 1840.0424\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 821.7121 - val_loss: 1840.0311\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 898.0962 - val_loss: 1843.1494\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 852.9672 - val_loss: 1843.0913\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 871.2563 - val_loss: 1843.1467\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 847.3181 - val_loss: 1843.0723\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 864.9248 - val_loss: 1842.9084\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 844.3228 - val_loss: 1842.7758\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 835.8972 - val_loss: 1842.9626\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 827.2792 - val_loss: 1842.9353\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 841.4374 - val_loss: 1842.9292\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 4.2504 - val_loss: 4.9454\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1635 - val_loss: 4.9226\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1310 - val_loss: 4.8654\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0987 - val_loss: 4.8570\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0892 - val_loss: 4.8446\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0710 - val_loss: 4.8204\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0506 - val_loss: 4.8188\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0685 - val_loss: 4.8170\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0429 - val_loss: 4.8151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0411 - val_loss: 4.8050\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0355 - val_loss: 4.8043\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0311 - val_loss: 4.8000\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0526 - val_loss: 4.7978\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0317 - val_loss: 4.7983\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0558 - val_loss: 4.7986\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0285 - val_loss: 4.7955\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0255 - val_loss: 4.7949\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0124 - val_loss: 4.7976\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0567 - val_loss: 4.7954\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0081 - val_loss: 4.7914\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0271 - val_loss: 4.8092\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0089 - val_loss: 4.7964\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0296 - val_loss: 4.7931\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9876 - val_loss: 4.8004\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0187 - val_loss: 4.7925\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0306 - val_loss: 4.7936\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0365 - val_loss: 4.8080\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0190 - val_loss: 4.7932\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0283 - val_loss: 4.7914\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0336 - val_loss: 4.7905\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0200 - val_loss: 4.7916\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9990 - val_loss: 4.7914\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0364 - val_loss: 4.8014\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0255 - val_loss: 4.7945\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0491 - val_loss: 4.7913\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0101 - val_loss: 4.7920\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0263 - val_loss: 4.7905\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0267 - val_loss: 4.7898\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0231 - val_loss: 4.7914\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0266 - val_loss: 4.7903\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9834 - val_loss: 4.7891\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0231 - val_loss: 4.7966\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0342 - val_loss: 4.7875\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9914 - val_loss: 4.7934\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0143 - val_loss: 4.7877\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9967 - val_loss: 4.7881\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0196 - val_loss: 4.7894\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0061 - val_loss: 4.7897\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0139 - val_loss: 4.7897\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0397 - val_loss: 4.7903\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 7431.5842 - val_loss: 15292.8174\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7923.4642 - val_loss: 15292.7402\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7580.1755 - val_loss: 15292.7314\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7920.7502 - val_loss: 15292.7383\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7712.2966 - val_loss: 15292.7041\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7465.7732 - val_loss: 15292.6865\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7363.5651 - val_loss: 15292.6582\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7803.3443 - val_loss: 15292.6406\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7382.1200 - val_loss: 15292.6182\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7975.4397 - val_loss: 15292.6143\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7365.6668 - val_loss: 15292.6025\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7533.5548 - val_loss: 15292.5967\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7629.9558 - val_loss: 15292.5908\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7211.7125 - val_loss: 15292.5928\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7462.2553 - val_loss: 15292.5859\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7711.6694 - val_loss: 15292.5811\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7429.6587 - val_loss: 15292.5781\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7580.1670 - val_loss: 15292.5771\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8004.7075 - val_loss: 15292.5850\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7729.9744 - val_loss: 15292.5820\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7734.0351 - val_loss: 15292.5674\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8180.7904 - val_loss: 15292.5615\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7589.7419 - val_loss: 15292.5615\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7447.1982 - val_loss: 15292.5576\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7830.5892 - val_loss: 15292.5566\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8015.7929 - val_loss: 15292.5566\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7630.8106 - val_loss: 15292.5566\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7591.3984 - val_loss: 15292.5479\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7621.0596 - val_loss: 15292.5615\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7858.1654 - val_loss: 15292.5498\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7677.1723 - val_loss: 15292.5439\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7826.2213 - val_loss: 15292.5488\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7818.8831 - val_loss: 15292.5400\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8054.5496 - val_loss: 15292.5439\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7904.0138 - val_loss: 15292.5430\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7389.1150 - val_loss: 15292.5410\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 7628.2350 - val_loss: 15292.5391\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7761.3113 - val_loss: 15292.5391\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7538.1798 - val_loss: 15292.5381\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7556.7147 - val_loss: 15292.5361\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7668.0273 - val_loss: 15292.5381\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7271.3202 - val_loss: 15292.5410\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7680.9686 - val_loss: 15292.5352\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7776.2609 - val_loss: 15292.5361\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7216.7853 - val_loss: 15292.5332\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7521.6199 - val_loss: 15292.5332\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7426.0050 - val_loss: 15292.5332\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7521.1924 - val_loss: 15292.5352\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7594.3407 - val_loss: 15292.5312\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8051.3427 - val_loss: 15292.5312\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.2756 - val_loss: 4.9259\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1223 - val_loss: 4.9042\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1017 - val_loss: 4.8849\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0907 - val_loss: 4.8635\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0598 - val_loss: 4.8516\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0848 - val_loss: 4.8444\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0490 - val_loss: 4.8308\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0262 - val_loss: 4.8286\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0298 - val_loss: 4.8149\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0481 - val_loss: 4.8086\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0457 - val_loss: 4.8037\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0216 - val_loss: 4.8000\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0495 - val_loss: 4.7945\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0242 - val_loss: 4.7907\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0340 - val_loss: 4.7872\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9877 - val_loss: 4.7837\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0173 - val_loss: 4.7816\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9892 - val_loss: 4.7862\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9840 - val_loss: 4.7800\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0003 - val_loss: 4.7732\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0002 - val_loss: 4.7703\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0078 - val_loss: 4.7758\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9990 - val_loss: 4.7678\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0102 - val_loss: 4.7708\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9776 - val_loss: 4.7635\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9856 - val_loss: 4.7674\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9828 - val_loss: 4.7633\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9810 - val_loss: 4.7594\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9929 - val_loss: 4.7571\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9674 - val_loss: 4.7660\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9907 - val_loss: 4.7566\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9817 - val_loss: 4.7709\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9826 - val_loss: 4.7766\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9898 - val_loss: 4.7536\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9838 - val_loss: 4.7596\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9882 - val_loss: 4.7509\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9868 - val_loss: 4.7559\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9589 - val_loss: 4.7501\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9770 - val_loss: 4.7515\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9704 - val_loss: 4.7673\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9651 - val_loss: 4.7631\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9667 - val_loss: 4.7631\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9377 - val_loss: 4.7494\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9690 - val_loss: 4.7448\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9664 - val_loss: 4.7561\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9655 - val_loss: 4.7513\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9774 - val_loss: 4.7438\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9561 - val_loss: 4.7430\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9511 - val_loss: 4.7424\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9869 - val_loss: 4.7451\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7574.5530 - val_loss: 15292.9824\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7660.2494 - val_loss: 15292.9082\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7660.6484 - val_loss: 15292.8438\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7826.9976 - val_loss: 15292.8203\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7681.5486 - val_loss: 15292.7705\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7823.9845 - val_loss: 15292.7471\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7873.0048 - val_loss: 15292.7207\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7643.1191 - val_loss: 15292.7061\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7845.8844 - val_loss: 15292.6855\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8041.6529 - val_loss: 15292.6719\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7575.5577 - val_loss: 15292.6582\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7788.9306 - val_loss: 15292.6475\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7715.7022 - val_loss: 15292.6377\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 7539.9350 - val_loss: 15292.6533\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7518.7772 - val_loss: 15292.6201\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8227.7935 - val_loss: 15292.6152\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7671.4198 - val_loss: 15292.6074\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7469.9542 - val_loss: 15292.6055\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7616.8826 - val_loss: 15292.5996\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7528.3622 - val_loss: 15292.5967\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7600.2132 - val_loss: 15292.5869\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7440.9024 - val_loss: 15292.5732\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7888.3940 - val_loss: 15292.5811\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7472.0174 - val_loss: 15292.5664\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 8048.7638 - val_loss: 15292.5664\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7340.8418 - val_loss: 15292.5635\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7585.0759 - val_loss: 15292.5586\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7819.4381 - val_loss: 15292.5547\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7154.6627 - val_loss: 15292.5566\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7663.6408 - val_loss: 15292.5498\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7777.6948 - val_loss: 15292.5635\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7795.0233 - val_loss: 15292.5488\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7968.9438 - val_loss: 15292.5430\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7575.1131 - val_loss: 15292.5410\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7906.8163 - val_loss: 15292.5381\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7539.7488 - val_loss: 15292.5381\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7781.3274 - val_loss: 15292.5352\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7863.0765 - val_loss: 15292.5361\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7675.5438 - val_loss: 15292.5352\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7656.3086 - val_loss: 15292.5283\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7507.9968 - val_loss: 15292.5361\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7655.7872 - val_loss: 15292.5293\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7652.5344 - val_loss: 15292.5264\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7497.5454 - val_loss: 15292.5205\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7580.6904 - val_loss: 15292.5244\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7838.2801 - val_loss: 15292.5215\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7524.1515 - val_loss: 15292.5264\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7839.9423 - val_loss: 15292.5264\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7136.8939 - val_loss: 15292.5166\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7370.8832 - val_loss: 15292.5156\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: 30.6172\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 25.7046 - val_loss: 25.3889\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 21.5572 - val_loss: 22.4282\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 19.1923 - val_loss: 20.3958\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 17.4803 - val_loss: 18.8690\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 16.2814 - val_loss: 17.6701\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 15.2186 - val_loss: 16.6944\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 14.4317 - val_loss: 15.8851\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13.6084 - val_loss: 15.2029\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13.1224 - val_loss: 14.6186\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 28494.1602\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 14511.8450 - val_loss: 26012.0566\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 13368.9421 - val_loss: 24580.1289\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 12327.0389 - val_loss: 23587.2852\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 12002.6886 - val_loss: 22842.9492\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 26ms/step - loss: 11958.0062 - val_loss: 22256.2109\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11047.2827 - val_loss: 21782.2207\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 10789.3800 - val_loss: 21388.1309\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11168.2602 - val_loss: 21056.7559\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 10242.7971 - val_loss: 20772.0684\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 17.1949 - val_loss: 3.0478\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1980 - val_loss: 2.8480\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7.1844 - val_loss: 3.2078\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.0099 - val_loss: 3.7515\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.3618 - val_loss: 3.2425\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.6017 - val_loss: 2.8317\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7.6678 - val_loss: 28.9245\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 6.7265 - val_loss: 2.9913\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.2675 - val_loss: 3.2463\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 5.3620 - val_loss: 3.0033\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.8121 - val_loss: 2.9931\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 5.6013 - val_loss: 3.3304\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.3239 - val_loss: 2.9591\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.9645 - val_loss: 4.8044\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.3695 - val_loss: 2.7934\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.7416 - val_loss: 2.8718\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 5.7350 - val_loss: 2.9314\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 5.4554 - val_loss: 17.8206\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.3920 - val_loss: 12.6107\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.7492 - val_loss: 3.3874\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.0542 - val_loss: 2.9770\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.8588 - val_loss: 2.9597\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.4377 - val_loss: 3.3448\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.0829 - val_loss: 2.8426\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.4850 - val_loss: 24.9685\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.2649 - val_loss: 2.5124\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1614 - val_loss: 2.5283\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1449 - val_loss: 2.5012\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1237 - val_loss: 2.4853\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1190 - val_loss: 2.5650\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1528 - val_loss: 2.5143\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1130 - val_loss: 2.5108\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1249 - val_loss: 2.5927\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1089 - val_loss: 2.4979\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1120 - val_loss: 2.6157\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0998 - val_loss: 2.4892\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1120 - val_loss: 2.4823\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1036 - val_loss: 2.5661\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1067 - val_loss: 2.5093\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0967 - val_loss: 2.4883\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1102 - val_loss: 2.4928\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1083 - val_loss: 2.5043\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1101 - val_loss: 2.4970\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1020 - val_loss: 2.4748\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1018 - val_loss: 2.4860\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0976 - val_loss: 2.5436\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1098 - val_loss: 2.4789\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1018 - val_loss: 2.4781\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1224 - val_loss: 2.4902\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0936 - val_loss: 2.4905\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 1487.6779 - val_loss: 1817.1707\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 980.1471 - val_loss: 1913.7628\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 936.1224 - val_loss: 1878.4104\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 919.1349 - val_loss: 1920.7875\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 917.0740 - val_loss: 1735.0687\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 878.9708 - val_loss: 1854.1509\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 884.2725 - val_loss: 2310.5337\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 872.0123 - val_loss: 1739.8575\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 938.5132 - val_loss: 1938.1019\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 934.6895 - val_loss: 1963.4243\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 909.5758 - val_loss: 1724.5490\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 923.8071 - val_loss: 1897.3369\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 863.2095 - val_loss: 1856.5105\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 848.5998 - val_loss: 1754.5859\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 872.9502 - val_loss: 2036.5537\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 884.3746 - val_loss: 1826.6074\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 897.7501 - val_loss: 1835.4803\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 876.2454 - val_loss: 1774.6934\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 890.5606 - val_loss: 1747.0872\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 892.7791 - val_loss: 2001.1119\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 883.3989 - val_loss: 1803.7155\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 26ms/step - loss: 853.2106 - val_loss: 1853.3202\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 824.4482 - val_loss: 1825.9285\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 850.7678 - val_loss: 1824.4354\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 825.5653 - val_loss: 1834.4729\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 800.9127 - val_loss: 1810.2959\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 875.5792 - val_loss: 1835.4395\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 871.4763 - val_loss: 1859.1731\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 871.7027 - val_loss: 1841.7887\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 930.0286 - val_loss: 1857.9493\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 818.2645 - val_loss: 1821.0228\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 887.9756 - val_loss: 1831.7574\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 851.1386 - val_loss: 1835.7948\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 795.2471 - val_loss: 1834.8158\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 855.4702 - val_loss: 1836.4875\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 883.4356 - val_loss: 1838.6344\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 824.5408 - val_loss: 1838.4518\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 918.8631 - val_loss: 1842.4399\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 863.7159 - val_loss: 1840.6206\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 856.4883 - val_loss: 1842.3041\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 889.5853 - val_loss: 1841.8069\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 814.8587 - val_loss: 1841.7893\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 844.1768 - val_loss: 1841.6481\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 877.9634 - val_loss: 1841.6333\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 824.5440 - val_loss: 1841.5250\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 899.8312 - val_loss: 1841.5394\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 869.1237 - val_loss: 1841.5098\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 871.6072 - val_loss: 1841.5494\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 865.8548 - val_loss: 1841.6173\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 858.4067 - val_loss: 1841.4965\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 3.9135 - val_loss: 2.5784\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1446 - val_loss: 2.4956\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1074 - val_loss: 2.4908\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1098 - val_loss: 2.4981\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1057 - val_loss: 2.5133\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1043 - val_loss: 2.4924\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.1075 - val_loss: 2.5407\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1067 - val_loss: 2.4934\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1037 - val_loss: 2.4789\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1005 - val_loss: 2.5199\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1010 - val_loss: 2.5339\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1041 - val_loss: 2.4949\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1001 - val_loss: 2.5043\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0953 - val_loss: 2.4970\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0992 - val_loss: 2.4999\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1022 - val_loss: 2.4974\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1112 - val_loss: 2.5023\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1021 - val_loss: 2.4869\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1008 - val_loss: 2.4970\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0880 - val_loss: 2.4744\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0777 - val_loss: 2.4749\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0705 - val_loss: 2.4778\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0678 - val_loss: 2.4737\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0711 - val_loss: 2.4736\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0741 - val_loss: 2.4722\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0760 - val_loss: 2.4730\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0693 - val_loss: 2.4721\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0697 - val_loss: 2.4797\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0735 - val_loss: 2.4744\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0744 - val_loss: 2.4759\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0657 - val_loss: 2.4770\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0710 - val_loss: 2.4812\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0715 - val_loss: 2.4743\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0751 - val_loss: 2.4721\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0764 - val_loss: 2.4716\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0791 - val_loss: 2.4885\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0696 - val_loss: 2.4778\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0704 - val_loss: 2.4746\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0705 - val_loss: 2.4710\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0681 - val_loss: 2.4685\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0727 - val_loss: 2.4687\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0738 - val_loss: 2.4893\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0750 - val_loss: 2.4840\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0653 - val_loss: 2.4761\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0710 - val_loss: 2.4761\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0729 - val_loss: 2.4778\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0702 - val_loss: 2.4753\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0706 - val_loss: 2.4850\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0768 - val_loss: 2.4680\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0790 - val_loss: 2.4866\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 3073.0575 - val_loss: 2325.8628\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 1117.8267 - val_loss: 2100.0388\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 880.0752 - val_loss: 2046.3217\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 884.5444 - val_loss: 1939.1119\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 893.2630 - val_loss: 2045.2094\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.7220 - val_loss: 2035.3553\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 917.5910 - val_loss: 1857.1741\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 888.6134 - val_loss: 1952.6338\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 843.8901 - val_loss: 1797.2574\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 897.3349 - val_loss: 1892.1586\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 929.0615 - val_loss: 2078.1755\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 824.3325 - val_loss: 1877.5333\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 896.4337 - val_loss: 1836.4419\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.8820 - val_loss: 1820.5441\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 921.3052 - val_loss: 1824.7590\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 874.6864 - val_loss: 1808.6157\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 885.5077 - val_loss: 1801.8635\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 814.3866 - val_loss: 1827.0135\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 857.5645 - val_loss: 1703.1670\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 865.1831 - val_loss: 1739.1636\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 918.1861 - val_loss: 1820.0016\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 835.6031 - val_loss: 1842.1926\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 823.4131 - val_loss: 1858.6399\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 828.0794 - val_loss: 1900.2686\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 862.7884 - val_loss: 1881.0948\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.5181 - val_loss: 1818.0165\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 859.3933 - val_loss: 1845.0576\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 901.4734 - val_loss: 1925.0720\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 903.9573 - val_loss: 1923.2441\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 886.6616 - val_loss: 1850.9794\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 898.1556 - val_loss: 1862.7474\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 906.7237 - val_loss: 1858.3396\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 838.2881 - val_loss: 1836.8375\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 851.5576 - val_loss: 1842.0962\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.1684 - val_loss: 1832.7874\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 883.0505 - val_loss: 1845.5939\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 820.9043 - val_loss: 1833.9192\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 839.4163 - val_loss: 1848.0968\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 901.8325 - val_loss: 1859.7377\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 795.4320 - val_loss: 1854.9539\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 844.9453 - val_loss: 1851.0074\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 836.2321 - val_loss: 1849.6849\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 841.0843 - val_loss: 1847.6207\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 822.2019 - val_loss: 1844.6235\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 863.0531 - val_loss: 1845.5129\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 838.5706 - val_loss: 1843.8695\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 835.6699 - val_loss: 1842.9709\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 822.0154 - val_loss: 1842.8340\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 877.8653 - val_loss: 1843.2825\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 830.3328 - val_loss: 1843.1698\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9915 - val_loss: 4.7006\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9176 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9307 - val_loss: 4.7006\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9104 - val_loss: 4.7010\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9102 - val_loss: 4.7007\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9215 - val_loss: 4.7007\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9432 - val_loss: 4.7010\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9464 - val_loss: 4.7007\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9261 - val_loss: 4.7010\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9064 - val_loss: 4.7007\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9481 - val_loss: 4.7007\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9451 - val_loss: 4.7011\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9318 - val_loss: 4.7011\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9444 - val_loss: 4.7010\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9381 - val_loss: 4.7009\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9244 - val_loss: 4.7010\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8944 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9090 - val_loss: 4.7008\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9267 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9090 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9141 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9159 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9495 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9031 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9469 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9019 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9286 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9150 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9159 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9237 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9550 - val_loss: 4.7011\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9062 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9369 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9611 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9305 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9373 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9252 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9359 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9145 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9429 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9230 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9291 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9349 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9015 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9151 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.8972 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9479 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9399 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9178 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9421 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 21ms/step - loss: 7657.9677 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7552.5673 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7586.0361 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7390.5354 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7424.9077 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7530.9555 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7417.0593 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7514.1063 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7510.0681 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7823.1664 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7467.0237 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7762.1615 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7638.6150 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7513.0320 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7382.4192 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7773.8185 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7462.3563 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7922.7794 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7463.8348 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7950.4885 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7628.5201 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7620.2305 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7765.4124 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8143.2070 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7793.9305 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7340.3947 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7675.4074 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8027.2338 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7579.2607 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7739.2560 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7603.4993 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7853.8637 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7652.9773 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7743.3368 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7329.5663 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7311.5620 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7377.5434 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7908.7499 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7531.2517 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7772.1859 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7627.7042 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7844.1717 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7833.2995 - val_loss: 15292.4346\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 7397.7711 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7200.0353 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7310.7032 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7599.8464 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7982.0627 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7697.0590 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7738.3094 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 4.0577 - val_loss: 4.7009\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9235 - val_loss: 4.7010\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9159 - val_loss: 4.7010\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9352 - val_loss: 4.7010\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9180 - val_loss: 4.7011\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9594 - val_loss: 4.7013\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9320 - val_loss: 4.7011\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9104 - val_loss: 4.7015\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9241 - val_loss: 4.7010\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9093 - val_loss: 4.7009\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9569 - val_loss: 4.7011\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9372 - val_loss: 4.7011\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9194 - val_loss: 4.7010\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9407 - val_loss: 4.7010\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9267 - val_loss: 4.7010\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9054 - val_loss: 4.7010\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9500 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9356 - val_loss: 4.7011\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9393 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9168 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9529 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9359 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9128 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9243 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9148 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9333 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9231 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8936 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9421 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9559 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9105 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9254 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9192 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9353 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9477 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9431 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9553 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9637 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9299 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9423 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9301 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9329 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9151 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9143 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9319 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9653 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9372 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9017 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9152 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9330 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7586.4080 - val_loss: 15292.4385\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7224.8451 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7393.4629 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7822.5304 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7714.7160 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 7661.0100 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7796.9626 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7651.5140 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7414.1924 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7432.5701 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7663.0245 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7947.6122 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7311.7942 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7293.5351 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7773.4993 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7328.1037 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7853.4707 - val_loss: 15292.4346\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: 7430.9305 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7811.5623 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7684.2638 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7547.2665 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7426.4301 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7600.9994 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7441.2922 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7321.9294 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7526.2612 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7577.4156 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7473.0662 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7289.3128 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7742.7240 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7583.5558 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7506.9729 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7695.3005 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7757.7861 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7673.2299 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7465.5837 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 7471.4456 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7569.5534 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7527.1524 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7669.8687 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7386.3362 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7642.4684 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7784.0510 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7614.3908 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7426.8249 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7276.4494 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7684.9117 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7396.1225 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7629.1719 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7203.3941 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 22.1847 - val_loss: 3.0514\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 6.5967 - val_loss: 3.5466\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 6.4709 - val_loss: 4.6734\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.5542 - val_loss: 16.3124\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.8196 - val_loss: 3.2091\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 11.6996 - val_loss: 8.3813\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7.9894 - val_loss: 37.6863\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 6.4058 - val_loss: 6.2773\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 6.8080 - val_loss: 4.6920\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7.5328 - val_loss: 4.8310\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 6.0999 - val_loss: 38.5142\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 9.1232 - val_loss: 3.7988\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.0738 - val_loss: 3.4781\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.0505 - val_loss: 3.4142\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8966 - val_loss: 3.3058\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.9449 - val_loss: 3.2861\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.5010 - val_loss: 3.2371\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.9075 - val_loss: 3.2100\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.7800 - val_loss: 3.1791\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8230 - val_loss: 3.1994\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.7693 - val_loss: 3.1454\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.6443 - val_loss: 3.1468\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6336 - val_loss: 3.1444\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.6340 - val_loss: 3.1184\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.6145 - val_loss: 3.1182\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.6792 - val_loss: 3.1146\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.6059 - val_loss: 3.1130\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.6318 - val_loss: 3.1098\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6156 - val_loss: 3.1097\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.6460 - val_loss: 3.1090\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.6088 - val_loss: 3.0952\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6009 - val_loss: 3.0931\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5945 - val_loss: 3.0927\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6062 - val_loss: 3.0936\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5928 - val_loss: 3.0924\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6066 - val_loss: 3.0929\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.5991 - val_loss: 3.0937\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5946 - val_loss: 3.0932\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6028 - val_loss: 3.0940\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.5998 - val_loss: 3.0933\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5973 - val_loss: 3.0937\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6036 - val_loss: 3.0936\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6094 - val_loss: 3.0936\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5887 - val_loss: 3.0936\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5988 - val_loss: 3.0936\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5858 - val_loss: 3.0936\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.6005 - val_loss: 3.0936\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6046 - val_loss: 3.0936\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5924 - val_loss: 3.0935\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.6004 - val_loss: 3.0936\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 1760.1700 - val_loss: 2487.0371\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 920.0179 - val_loss: 1925.2126\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 881.0094 - val_loss: 1728.6426\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 907.0519 - val_loss: 1837.1097\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 891.5160 - val_loss: 1755.1827\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 871.9677 - val_loss: 1817.6486\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 872.2994 - val_loss: 1938.3167\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 886.6551 - val_loss: 1844.6898\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 920.5624 - val_loss: 1964.6111\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 917.2016 - val_loss: 1836.1451\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 840.0006 - val_loss: 1832.4989\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 888.5614 - val_loss: 1875.7043\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 899.8921 - val_loss: 1803.1420\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 846.3251 - val_loss: 1819.7711\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 858.3726 - val_loss: 1856.1027\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 906.3839 - val_loss: 1835.5476\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 919.8329 - val_loss: 1866.0304\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 840.2838 - val_loss: 1845.8688\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 937.4013 - val_loss: 1853.4938\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 907.5506 - val_loss: 1864.6261\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 863.9135 - val_loss: 1843.1506\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 918.9776 - val_loss: 1863.5939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 864.6887 - val_loss: 1845.5325\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 825.9990 - val_loss: 1843.0876\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 809.6574 - val_loss: 1843.0547\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 854.6171 - val_loss: 1843.0951\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 830.5063 - val_loss: 1841.4836\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 838.3197 - val_loss: 1839.3861\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 841.5635 - val_loss: 1837.5176\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 855.9733 - val_loss: 1838.6404\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 876.1647 - val_loss: 1841.0320\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 817.9815 - val_loss: 1838.6078\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 846.1537 - val_loss: 1839.0219\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 802.2972 - val_loss: 1839.2018\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 835.6296 - val_loss: 1839.2600\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 852.2114 - val_loss: 1839.5106\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 849.9316 - val_loss: 1839.4757\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 788.9663 - val_loss: 1839.4486\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 888.3270 - val_loss: 1839.4935\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 815.3429 - val_loss: 1839.6115\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 800.0197 - val_loss: 1839.7697\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 875.3515 - val_loss: 1839.8925\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 863.5586 - val_loss: 1839.8551\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 906.7268 - val_loss: 1839.8771\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 862.4777 - val_loss: 1839.8937\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 858.5094 - val_loss: 1839.8774\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 829.2621 - val_loss: 1839.8787\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 843.7279 - val_loss: 1839.8754\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 917.8390 - val_loss: 1839.8851\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 903.5282 - val_loss: 1839.8925\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 3.8438 - val_loss: 2.6320\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1987 - val_loss: 2.5295\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1368 - val_loss: 2.5647\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1301 - val_loss: 2.5057\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1274 - val_loss: 2.5525\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1256 - val_loss: 2.5178\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1188 - val_loss: 2.5152\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1208 - val_loss: 2.4995\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1166 - val_loss: 2.5103\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.1205 - val_loss: 2.5325\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1250 - val_loss: 2.4874\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0895 - val_loss: 2.5010\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0821 - val_loss: 2.4781\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0813 - val_loss: 2.4782\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0755 - val_loss: 2.4720\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0723 - val_loss: 2.4820\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0689 - val_loss: 2.4563\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0829 - val_loss: 2.4499\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0775 - val_loss: 2.4780\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1330 - val_loss: 2.5340\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0791 - val_loss: 2.5198\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1063 - val_loss: 2.4574\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0821 - val_loss: 2.4499\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0899 - val_loss: 2.4557\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0716 - val_loss: 2.4698\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1464 - val_loss: 5.6503\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.2116 - val_loss: 2.5126\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2262 - val_loss: 2.4546\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0445 - val_loss: 2.4450\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0300 - val_loss: 2.4443\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0353 - val_loss: 2.4416\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.0390 - val_loss: 2.4492\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0361 - val_loss: 2.4403\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0332 - val_loss: 2.4458\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0325 - val_loss: 2.4428\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0309 - val_loss: 2.4425\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0322 - val_loss: 2.4445\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0294 - val_loss: 2.4434\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0394 - val_loss: 2.4481\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0293 - val_loss: 2.4456\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0327 - val_loss: 2.4380\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.0190 - val_loss: 2.4432\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0294 - val_loss: 2.4405\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0334 - val_loss: 2.4416\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0298 - val_loss: 2.4409\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0292 - val_loss: 2.4437\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0322 - val_loss: 2.4384\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0250 - val_loss: 2.4445\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0277 - val_loss: 2.4367\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0310 - val_loss: 2.4411\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3196.9002 - val_loss: 2394.2336\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 1033.6429 - val_loss: 2040.5774\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 909.9643 - val_loss: 2075.6731\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 879.6316 - val_loss: 1875.8120\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 908.3491 - val_loss: 2004.6084\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 927.4809 - val_loss: 1888.7507\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 903.5637 - val_loss: 1863.4148\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 912.7538 - val_loss: 1837.4689\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 860.8646 - val_loss: 1841.7560\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 902.2289 - val_loss: 1894.4961\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 863.5541 - val_loss: 1875.5277\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 875.0982 - val_loss: 1931.0791\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 903.7080 - val_loss: 1838.5554\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 898.5588 - val_loss: 1820.2539\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 865.3058 - val_loss: 1798.1769\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 868.3702 - val_loss: 1926.8389\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 864.5194 - val_loss: 1862.1160\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 831.8138 - val_loss: 1754.8931\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 888.0199 - val_loss: 1798.4281\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 892.0473 - val_loss: 1798.2397\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 861.6421 - val_loss: 1825.2802\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 883.9269 - val_loss: 1810.4929\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 880.6641 - val_loss: 1861.2845\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 805.0170 - val_loss: 1729.8812\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 880.5881 - val_loss: 1726.5150\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 854.3589 - val_loss: 1843.6970\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 896.5309 - val_loss: 1935.9849\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 840.1792 - val_loss: 1841.9034\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 904.6795 - val_loss: 1877.3967\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 847.8529 - val_loss: 1834.5049\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 826.0675 - val_loss: 1827.7861\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 839.3752 - val_loss: 1976.6155\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 857.7832 - val_loss: 1873.5054\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 855.6103 - val_loss: 1793.4376\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 859.1316 - val_loss: 1903.1082\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 848.0748 - val_loss: 1847.6956\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 857.0768 - val_loss: 1846.3724\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 820.8639 - val_loss: 1833.6866\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 882.6108 - val_loss: 1846.8823\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 816.4534 - val_loss: 1824.0001\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 832.6815 - val_loss: 1824.9514\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 816.1265 - val_loss: 1822.5304\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 841.0311 - val_loss: 1850.3132\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 853.2594 - val_loss: 1847.1127\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 852.1346 - val_loss: 1847.1390\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 827.6714 - val_loss: 1847.5878\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 826.7921 - val_loss: 1846.4200\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 853.6423 - val_loss: 1844.9279\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 898.0474 - val_loss: 1845.9572\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 837.6555 - val_loss: 1844.3977\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.2445 - val_loss: 4.9503\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1603 - val_loss: 4.9105\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.1270 - val_loss: 4.8758\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0891 - val_loss: 4.8417\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0859 - val_loss: 4.8296\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0744 - val_loss: 4.8201\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0279 - val_loss: 4.8146\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0524 - val_loss: 4.8056\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0500 - val_loss: 4.8058\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0270 - val_loss: 4.8032\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0558 - val_loss: 4.8001\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0384 - val_loss: 4.8054\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0396 - val_loss: 4.8004\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0283 - val_loss: 4.7967\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0544 - val_loss: 4.8001\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0319 - val_loss: 4.7967\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0279 - val_loss: 4.7945\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0133 - val_loss: 4.7981\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0269 - val_loss: 4.7972\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0322 - val_loss: 4.7944\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0123 - val_loss: 4.7936\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0185 - val_loss: 4.7919\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0335 - val_loss: 4.7944\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0278 - val_loss: 4.7943\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0367 - val_loss: 4.8034\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0024 - val_loss: 4.7933\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0163 - val_loss: 4.7983\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0279 - val_loss: 4.8074\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0249 - val_loss: 4.8033\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0353 - val_loss: 4.7930\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0147 - val_loss: 4.7918\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9995 - val_loss: 4.7941\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9282 - val_loss: 4.7183\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9612 - val_loss: 4.7171\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9643 - val_loss: 4.7168\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9340 - val_loss: 4.7167\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9419 - val_loss: 4.7167\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9401 - val_loss: 4.7166\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9407 - val_loss: 4.7164\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9600 - val_loss: 4.7164\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9495 - val_loss: 4.7164\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9365 - val_loss: 4.7164\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9525 - val_loss: 4.7171\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9347 - val_loss: 4.7169\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9444 - val_loss: 4.7164\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9313 - val_loss: 4.7166\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9395 - val_loss: 4.7165\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9634 - val_loss: 4.7165\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9628 - val_loss: 4.7166\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9485 - val_loss: 4.7092\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7627.3821 - val_loss: 15292.8896\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7550.0297 - val_loss: 15292.7480\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7802.1040 - val_loss: 15292.7422\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7676.1479 - val_loss: 15292.7051\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7581.9766 - val_loss: 15292.6855\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7919.8240 - val_loss: 15292.6631\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7538.5198 - val_loss: 15292.6455\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7397.6693 - val_loss: 15292.6240\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7410.1434 - val_loss: 15292.6221\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7547.0657 - val_loss: 15292.6211\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7674.5824 - val_loss: 15292.6152\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7529.1697 - val_loss: 15292.5996\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7062.2321 - val_loss: 15292.5918\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7686.5602 - val_loss: 15292.5850\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7991.6779 - val_loss: 15292.5801\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7674.7743 - val_loss: 15292.5781\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7595.5868 - val_loss: 15292.5703\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7492.3923 - val_loss: 15292.5703\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7808.9009 - val_loss: 15292.5654\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7327.5391 - val_loss: 15292.5664\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7659.1546 - val_loss: 15292.5625\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7923.2070 - val_loss: 15292.5576\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7784.2475 - val_loss: 15292.5566\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7106.1465 - val_loss: 15292.5615\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7562.4669 - val_loss: 15292.5547\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7641.5926 - val_loss: 15292.5527\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7623.1351 - val_loss: 15292.5488\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7441.3842 - val_loss: 15292.5527\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7904.0201 - val_loss: 15292.5449\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7744.0377 - val_loss: 15292.5459\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7362.7674 - val_loss: 15292.5469\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7402.9846 - val_loss: 15292.5430\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7390.3220 - val_loss: 15292.5410\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7736.4382 - val_loss: 15292.5430\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7555.6929 - val_loss: 15292.5430\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7575.8627 - val_loss: 15292.5488\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7769.9370 - val_loss: 15292.5410\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7672.9544 - val_loss: 15292.5391\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7404.9042 - val_loss: 15292.5381\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7651.4159 - val_loss: 15292.5361\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7716.3167 - val_loss: 15292.5371\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7338.2456 - val_loss: 15292.5352\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7667.8208 - val_loss: 15292.5352\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7910.3284 - val_loss: 15292.5352\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7564.2981 - val_loss: 15292.5381\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7694.9810 - val_loss: 15292.5332\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7763.9816 - val_loss: 15292.5352\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 7385.9831 - val_loss: 15292.5352\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7711.9566 - val_loss: 15292.5332\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7768.4595 - val_loss: 15292.5312\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 24ms/step - loss: 4.2785 - val_loss: 4.9327\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.1362 - val_loss: 4.9016\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1181 - val_loss: 4.8812\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0979 - val_loss: 4.8637\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1016 - val_loss: 4.8497\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0753 - val_loss: 4.8400\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0388 - val_loss: 4.8308\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0520 - val_loss: 4.8208\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0327 - val_loss: 4.8160\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0072 - val_loss: 4.8095\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0341 - val_loss: 4.8057\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0202 - val_loss: 4.7981\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0317 - val_loss: 4.7948\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0568 - val_loss: 4.7935\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0117 - val_loss: 4.7866\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0239 - val_loss: 4.7836\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0084 - val_loss: 4.7795\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0036 - val_loss: 4.7841\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0034 - val_loss: 4.7774\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0161 - val_loss: 4.7733\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9992 - val_loss: 4.7716\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9890 - val_loss: 4.7701\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9892 - val_loss: 4.7686\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9893 - val_loss: 4.7646\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0123 - val_loss: 4.7660\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9664 - val_loss: 4.7676\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9672 - val_loss: 4.7638\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9770 - val_loss: 4.7638\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9755 - val_loss: 4.7583\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9665 - val_loss: 4.7568\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9916 - val_loss: 4.7571\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9702 - val_loss: 4.7560\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9940 - val_loss: 4.7535\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0015 - val_loss: 4.7529\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9741 - val_loss: 4.7518\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9790 - val_loss: 4.7548\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9362 - val_loss: 4.7499\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9886 - val_loss: 4.7552\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9516 - val_loss: 4.7544\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9624 - val_loss: 4.7506\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9857 - val_loss: 4.7493\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9951 - val_loss: 4.7525\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9956 - val_loss: 4.7486\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9782 - val_loss: 4.7487\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9587 - val_loss: 4.7456\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9714 - val_loss: 4.7575\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9883 - val_loss: 4.7469\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9629 - val_loss: 4.7530\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9496 - val_loss: 4.7553\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9506 - val_loss: 4.7565\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 7650.7771 - val_loss: 15292.9619\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7656.5201 - val_loss: 15292.9072\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7366.7751 - val_loss: 15292.8428\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7413.9350 - val_loss: 15292.8213\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7900.4671 - val_loss: 15292.7803\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7509.6680 - val_loss: 15292.7480\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7544.3080 - val_loss: 15292.7217\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7571.7422 - val_loss: 15292.7021\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7744.7306 - val_loss: 15292.6914\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7567.1597 - val_loss: 15292.6826\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7413.2687 - val_loss: 15292.6602\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7576.2852 - val_loss: 15292.6465\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7376.6144 - val_loss: 15292.6406\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7625.2959 - val_loss: 15292.6475\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7541.3530 - val_loss: 15292.6182\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7191.7870 - val_loss: 15292.6113\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7439.2125 - val_loss: 15292.6104\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7651.0448 - val_loss: 15292.5996\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7466.7581 - val_loss: 15292.5957\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7490.0698 - val_loss: 15292.5889\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7240.5833 - val_loss: 15292.5996\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7843.0327 - val_loss: 15292.5840\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7286.2052 - val_loss: 15292.5732\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7684.7164 - val_loss: 15292.5674\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7991.6908 - val_loss: 15292.5723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7799.9794 - val_loss: 15292.5703\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7577.1882 - val_loss: 15292.5586\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7824.7481 - val_loss: 15292.5537\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7707.3402 - val_loss: 15292.5566\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7518.3516 - val_loss: 15292.5479\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7413.5918 - val_loss: 15292.5469\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7844.4094 - val_loss: 15292.5469\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7543.6286 - val_loss: 15292.5449\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7952.4281 - val_loss: 15292.5439\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7624.8392 - val_loss: 15292.5479\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7580.6540 - val_loss: 15292.5352\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7745.9478 - val_loss: 15292.5381\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7561.4220 - val_loss: 15292.5303\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7619.3040 - val_loss: 15292.5352\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 8003.1544 - val_loss: 15292.5332\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7693.6075 - val_loss: 15292.5283\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7818.5979 - val_loss: 15292.5361\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7816.2272 - val_loss: 15292.5283\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7906.9617 - val_loss: 15292.5293\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7551.2978 - val_loss: 15292.5195\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7277.1484 - val_loss: 15292.5215\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7413.6188 - val_loss: 15292.5195\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7483.2673 - val_loss: 15292.5166\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7538.2330 - val_loss: 15292.5156\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7654.3798 - val_loss: 15292.5156\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 30.6107\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 25.5654 - val_loss: 25.3898\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 21.7364 - val_loss: 22.4314\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 19.2290 - val_loss: 20.3970\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 17.4471 - val_loss: 18.8706\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 16.1549 - val_loss: 17.6707\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 15.3392 - val_loss: 16.6958\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 14.4313 - val_loss: 15.8864\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 13.6807 - val_loss: 15.2033\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 13.1966 - val_loss: 14.6195\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 28499.1406\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 14878.3992 - val_loss: 26014.4766\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 12851.9730 - val_loss: 24581.4707\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 12902.4879 - val_loss: 23588.0234\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 12087.2731 - val_loss: 22842.4414\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11758.8638 - val_loss: 22256.0605\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11194.7403 - val_loss: 21780.5820\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11029.3685 - val_loss: 21387.4844\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 10458.2198 - val_loss: 21056.1621\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 10580.3006 - val_loss: 20772.5254\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 22.3017 - val_loss: 3.0901\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.9245 - val_loss: 3.0359\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.0658 - val_loss: 3.7766\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 5.3202 - val_loss: 3.1091\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.7820 - val_loss: 4.9354\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.1840 - val_loss: 2.9469\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.3673 - val_loss: 3.0247\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 5.9577 - val_loss: 3.1998\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.1466 - val_loss: 4.2749\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.4356 - val_loss: 2.8257\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.1408 - val_loss: 3.4708\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 7.0443 - val_loss: 3.4565\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.1293 - val_loss: 3.4029\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 6.7405 - val_loss: 3.2919\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 5.5784 - val_loss: 3.1119\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.5506 - val_loss: 28.7555\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.2301 - val_loss: 2.9988\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 6.9722 - val_loss: 2.9544\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.6098 - val_loss: 9.9983\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.1273 - val_loss: 3.6420\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.4609 - val_loss: 2.5134\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2.1775 - val_loss: 2.5051\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.2176 - val_loss: 2.5166\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.2393 - val_loss: 2.5031\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1536 - val_loss: 2.4874\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1642 - val_loss: 2.5999\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1525 - val_loss: 2.5311\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1192 - val_loss: 2.5026\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1394 - val_loss: 2.4973\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1329 - val_loss: 2.5401\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1307 - val_loss: 2.5024\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1424 - val_loss: 2.5127\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1147 - val_loss: 2.4771\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1153 - val_loss: 2.5479\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1100 - val_loss: 2.5054\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1197 - val_loss: 2.4951\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1069 - val_loss: 2.5087\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1144 - val_loss: 2.5155\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1110 - val_loss: 2.5016\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1014 - val_loss: 2.5091\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1082 - val_loss: 2.4764\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1042 - val_loss: 2.4896\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1125 - val_loss: 2.4709\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1217 - val_loss: 2.4942\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1049 - val_loss: 2.4911\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1261 - val_loss: 2.5125\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0971 - val_loss: 2.5135\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0946 - val_loss: 2.5076\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1032 - val_loss: 2.5257\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1209 - val_loss: 2.4719\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 1865.8958 - val_loss: 2113.7400\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 953.4867 - val_loss: 1833.7296\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 912.7799 - val_loss: 1985.2250\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 890.4013 - val_loss: 2131.0007\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 897.1693 - val_loss: 1787.7173\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.2007 - val_loss: 1928.7874\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 936.3547 - val_loss: 1926.1381\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 868.2619 - val_loss: 1843.5529\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 861.7465 - val_loss: 2071.8162\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 857.8606 - val_loss: 1821.0696\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 856.1119 - val_loss: 1896.4552\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 877.2514 - val_loss: 1812.1804\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 885.7034 - val_loss: 1909.1123\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 866.6529 - val_loss: 1849.6965\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 916.4836 - val_loss: 1871.3621\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 888.9012 - val_loss: 1867.5542\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 860.4775 - val_loss: 1835.4995\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 849.5371 - val_loss: 1856.8462\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 847.0608 - val_loss: 1834.6035\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 901.8062 - val_loss: 1839.2534\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 879.3743 - val_loss: 1825.1448\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 864.4047 - val_loss: 1849.4518\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 846.0255 - val_loss: 1852.3063\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 902.2720 - val_loss: 1853.4026\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 909.2608 - val_loss: 1841.2784\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 854.3313 - val_loss: 1840.4017\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 894.9201 - val_loss: 1840.8755\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 843.8615 - val_loss: 1838.5051\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.7368 - val_loss: 1838.5316\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 861.9461 - val_loss: 1836.9058\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 841.8331 - val_loss: 1834.9052\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 841.8009 - val_loss: 1834.7493\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 861.5628 - val_loss: 1835.5865\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 26ms/step - loss: 816.7634 - val_loss: 1836.7285\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 896.7716 - val_loss: 1840.6329\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 869.7013 - val_loss: 1840.3773\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 879.5281 - val_loss: 1840.1384\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 868.1160 - val_loss: 1840.0061\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 824.5324 - val_loss: 1839.7974\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 852.0344 - val_loss: 1839.7281\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 867.2866 - val_loss: 1839.5668\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 842.7341 - val_loss: 1839.4066\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 806.9987 - val_loss: 1839.2930\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 823.1486 - val_loss: 1839.0717\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 847.9618 - val_loss: 1838.8782\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 868.6408 - val_loss: 1838.8990\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 817.5693 - val_loss: 1838.9060\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 822.6313 - val_loss: 1838.8881\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 788.9987 - val_loss: 1838.8893\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 894.5415 - val_loss: 1838.8871\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 3.8619 - val_loss: 2.5438\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1428 - val_loss: 2.4849\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1171 - val_loss: 2.4831\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1033 - val_loss: 2.4790\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0922 - val_loss: 2.4785\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2.0982 - val_loss: 2.4854\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0954 - val_loss: 2.5380\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0901 - val_loss: 2.4683\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1054 - val_loss: 2.4824\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0835 - val_loss: 2.4829\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0929 - val_loss: 2.4796\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0817 - val_loss: 2.4869\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0917 - val_loss: 2.4697\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0938 - val_loss: 2.4682\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0910 - val_loss: 2.4759\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0983 - val_loss: 2.5392\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0952 - val_loss: 2.4894\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0902 - val_loss: 2.4784\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0566 - val_loss: 2.4640\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0574 - val_loss: 2.4578\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0572 - val_loss: 2.4702\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0585 - val_loss: 2.4594\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0631 - val_loss: 2.4614\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0511 - val_loss: 2.4618\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0620 - val_loss: 2.4623\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0549 - val_loss: 2.4587\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0628 - val_loss: 2.4626\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0526 - val_loss: 2.4586\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0554 - val_loss: 2.4581\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0643 - val_loss: 2.4593\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0555 - val_loss: 2.4598\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0515 - val_loss: 2.4591\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0549 - val_loss: 2.4598\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0591 - val_loss: 2.4602\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0550 - val_loss: 2.4595\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0540 - val_loss: 2.4593\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0601 - val_loss: 2.4592\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0557 - val_loss: 2.4591\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0586 - val_loss: 2.4590\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0564 - val_loss: 2.4599\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0481 - val_loss: 2.4596\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0501 - val_loss: 2.4594\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0603 - val_loss: 2.4593\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0540 - val_loss: 2.4593\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0600 - val_loss: 2.4592\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0578 - val_loss: 2.4592\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0668 - val_loss: 2.4593\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0574 - val_loss: 2.4593\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0573 - val_loss: 2.4593\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0570 - val_loss: 2.4593\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 3152.5643 - val_loss: 2503.9639\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 1000.0055 - val_loss: 2023.4125\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 950.5149 - val_loss: 1958.9629\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 897.6811 - val_loss: 1873.0653\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 900.5712 - val_loss: 1909.6960\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 840.7248 - val_loss: 1897.3500\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 877.1077 - val_loss: 1892.0101\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 890.2794 - val_loss: 1926.0438\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 828.6189 - val_loss: 1770.9730\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 911.9051 - val_loss: 1779.7356\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 846.4546 - val_loss: 1838.9990\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 814.1530 - val_loss: 1759.7183\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 861.9126 - val_loss: 1709.9784\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 899.5808 - val_loss: 1819.7765\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 882.2015 - val_loss: 1847.4539\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 872.9584 - val_loss: 1790.1149\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 873.1293 - val_loss: 1803.2957\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 861.9170 - val_loss: 1880.0929\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 826.5806 - val_loss: 1846.2230\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 891.1074 - val_loss: 1864.5020\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 816.5154 - val_loss: 1808.5292\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 846.5715 - val_loss: 1826.2063\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 836.5748 - val_loss: 1757.7345\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 892.1091 - val_loss: 1835.3361\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 920.2994 - val_loss: 1861.7349\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 896.9705 - val_loss: 1864.1288\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 857.3650 - val_loss: 1829.0858\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 843.0505 - val_loss: 1841.4476\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 864.3216 - val_loss: 1855.3021\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 834.4590 - val_loss: 1831.2845\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 845.2100 - val_loss: 1837.1075\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 849.8374 - val_loss: 1836.3656\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 864.5797 - val_loss: 1843.9915\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 857.2203 - val_loss: 1842.3589\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 844.3987 - val_loss: 1842.0219\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 846.9299 - val_loss: 1841.6586\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 914.2038 - val_loss: 1842.2753\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 868.1072 - val_loss: 1841.7162\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 846.4395 - val_loss: 1840.5687\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 868.0461 - val_loss: 1841.2511\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 898.5517 - val_loss: 1841.9938\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 851.4295 - val_loss: 1841.6057\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 824.6238 - val_loss: 1841.3668\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 879.0993 - val_loss: 1841.2738\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 846.0469 - val_loss: 1841.4050\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 859.5330 - val_loss: 1841.4021\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 883.8527 - val_loss: 1841.3795\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 858.1964 - val_loss: 1841.3878\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 922.9970 - val_loss: 1841.3759\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 889.0238 - val_loss: 1841.3606\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9703 - val_loss: 4.7006\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9456 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9377 - val_loss: 4.7006\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9314 - val_loss: 4.7006\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9150 - val_loss: 4.7007\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9380 - val_loss: 4.7006\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9298 - val_loss: 4.7007\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9185 - val_loss: 4.7007\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9218 - val_loss: 4.7007\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9374 - val_loss: 4.7007\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9318 - val_loss: 4.7007\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9175 - val_loss: 4.7009\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9019 - val_loss: 4.7010\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9160 - val_loss: 4.7009\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9425 - val_loss: 4.7009\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9362 - val_loss: 4.7008\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9176 - val_loss: 4.7008\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9080 - val_loss: 4.7009\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9010 - val_loss: 4.7008\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9516 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9621 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9450 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9219 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9387 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9437 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9141 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9315 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9224 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9437 - val_loss: 4.7010\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9301 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9599 - val_loss: 4.7011\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9219 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9288 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9007 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9402 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9415 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9228 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9241 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9539 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9333 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9487 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9028 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9216 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9112 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9252 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9434 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9367 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9359 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9028 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9271 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7393.9533 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7609.3404 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8014.7497 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7500.0010 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7471.4624 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7355.2523 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7622.5582 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7975.7830 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7453.3414 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7516.8677 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7165.3410 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7554.4957 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7761.3465 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7541.0399 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7900.0052 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7792.2262 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7787.3900 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7623.5646 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7775.3607 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7608.3356 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7717.7623 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7578.5737 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7462.2865 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7401.9400 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7723.8130 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7525.4202 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7913.7089 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7825.3022 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7832.5808 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7612.5582 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7659.1760 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7580.9629 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7730.5752 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7710.1632 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7476.9492 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7281.3685 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7345.2150 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7822.4811 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7185.0315 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7923.2063 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7823.8414 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7193.7409 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7533.1946 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7765.2562 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7883.1706 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7549.8499 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8075.1354 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7777.0112 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7388.9852 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7993.2585 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 22ms/step - loss: 4.0635 - val_loss: 4.7012\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9458 - val_loss: 4.7008\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9448 - val_loss: 4.7009\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9374 - val_loss: 4.7011\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9455 - val_loss: 4.7010\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9438 - val_loss: 4.7014\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9052 - val_loss: 4.7008\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9449 - val_loss: 4.7012\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9270 - val_loss: 4.7017\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9511 - val_loss: 4.7009\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9308 - val_loss: 4.7008\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9242 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8956 - val_loss: 4.7010\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9232 - val_loss: 4.7010\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9449 - val_loss: 4.7011\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9257 - val_loss: 4.7011\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9253 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9422 - val_loss: 4.7011\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9246 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9445 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9265 - val_loss: 4.7010\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9365 - val_loss: 4.7011\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9141 - val_loss: 4.7011\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9030 - val_loss: 4.7011\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9155 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9568 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9036 - val_loss: 4.7011\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9098 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9359 - val_loss: 4.7011\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9122 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9178 - val_loss: 4.7010\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9285 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9160 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9170 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9157 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9367 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9176 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9325 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9530 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9732 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9149 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9123 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9271 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9529 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9147 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9014 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8880 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9207 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9480 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9302 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7540.6340 - val_loss: 15292.4473\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7988.4843 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7430.7366 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7878.0629 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7863.2325 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7868.8653 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7316.6669 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7497.8340 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 8046.9819 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7372.7600 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7808.9406 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7350.2543 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7443.3941 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7951.3670 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 8036.2454 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7481.7020 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7896.0557 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7244.8395 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7198.3293 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7595.9480 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7989.1256 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7606.9848 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7914.1441 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7772.2294 - val_loss: 15292.4346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7392.1785 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7420.7854 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7402.0758 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7829.5373 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7640.6991 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7266.0182 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7516.7141 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7711.1729 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7673.8163 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7283.6774 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7485.9770 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7801.0272 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7757.0073 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7691.1978 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7578.2503 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7595.1190 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7316.6381 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7668.4035 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7563.2089 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7950.9140 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7372.7265 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7727.7246 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7429.6083 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7792.5838 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7522.8351 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7593.8435 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 20.7995 - val_loss: 4.4640\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.5594 - val_loss: 15.7578\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 13.8759 - val_loss: 56.2255\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 21.4741 - val_loss: 24.0813\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 21.0151 - val_loss: 40.0385\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 22.3321 - val_loss: 10.7245\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 22.0454 - val_loss: 42.0575\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 22.4726 - val_loss: 30.5858\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 22.4864 - val_loss: 34.6853\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 21.2280 - val_loss: 11.4640\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 21.5359 - val_loss: 20.9166\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8.9327 - val_loss: 5.5996\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.8666 - val_loss: 5.6568\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.6891 - val_loss: 8.5066\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.8258 - val_loss: 5.3492\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.3631 - val_loss: 5.0304\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.2169 - val_loss: 5.0042\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.4274 - val_loss: 5.7078\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.3651 - val_loss: 4.8486\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.2707 - val_loss: 5.0813\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.1628 - val_loss: 4.6988\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7200 - val_loss: 4.7416\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7019 - val_loss: 4.6747\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.6835 - val_loss: 4.7168\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.6716 - val_loss: 4.6894\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6704 - val_loss: 4.6775\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6948 - val_loss: 4.7095\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.6798 - val_loss: 4.6786\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.6887 - val_loss: 4.6424\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.6613 - val_loss: 4.6717\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.6529 - val_loss: 4.6456\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6759 - val_loss: 4.6556\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6450 - val_loss: 4.6609\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6518 - val_loss: 4.6627\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.6437 - val_loss: 4.6615\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6226 - val_loss: 4.6612\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6980 - val_loss: 4.6648\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.6216 - val_loss: 4.6629\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6759 - val_loss: 4.6656\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6367 - val_loss: 4.6659\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6504 - val_loss: 4.6676\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6693 - val_loss: 4.6674\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.6570 - val_loss: 4.6673\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.6005 - val_loss: 4.6671\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.6877 - val_loss: 4.6672\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6231 - val_loss: 4.6672\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6360 - val_loss: 4.6672\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6763 - val_loss: 4.6674\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6612 - val_loss: 4.6674\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.6853 - val_loss: 4.6674\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 1897.9352 - val_loss: 1896.0559\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 938.2941 - val_loss: 1946.5685\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 881.7854 - val_loss: 1741.0918\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 879.9544 - val_loss: 1646.3230\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 887.8681 - val_loss: 1814.3613\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 900.1780 - val_loss: 1848.9131\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 902.7229 - val_loss: 1983.0475\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 904.9174 - val_loss: 1833.8472\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 919.2121 - val_loss: 1914.7113\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 926.9499 - val_loss: 1915.1185\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 920.1488 - val_loss: 1862.0223\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 868.5385 - val_loss: 1991.2505\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 850.3015 - val_loss: 1915.9902\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 882.7352 - val_loss: 2316.2869\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 961.0102 - val_loss: 1935.2389\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 897.8133 - val_loss: 1870.2838\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 887.3986 - val_loss: 1854.9805\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 852.3560 - val_loss: 1859.0997\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 932.5522 - val_loss: 1868.3203\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 824.5270 - val_loss: 1807.6145\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 842.2477 - val_loss: 1800.7427\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 885.7099 - val_loss: 1886.4155\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 885.8482 - val_loss: 1868.8958\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 876.7503 - val_loss: 1842.2823\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 824.3177 - val_loss: 1843.6777\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 843.7278 - val_loss: 1844.0073\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 813.2588 - val_loss: 1844.1102\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 885.5823 - val_loss: 1847.3331\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 869.2622 - val_loss: 1845.1652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 799.3156 - val_loss: 1841.2804\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 857.0976 - val_loss: 1845.1919\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 849.1774 - val_loss: 1844.9382\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 812.9354 - val_loss: 1841.2062\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 857.6821 - val_loss: 1842.3403\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 840.6522 - val_loss: 1842.1328\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 851.7257 - val_loss: 1841.9094\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 824.1012 - val_loss: 1841.7006\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 817.0801 - val_loss: 1841.5702\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 906.1407 - val_loss: 1841.7264\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 872.7643 - val_loss: 1841.5504\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 816.5216 - val_loss: 1841.4651\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 822.5243 - val_loss: 1841.3534\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 880.3861 - val_loss: 1841.8079\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 865.8661 - val_loss: 1842.1006\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 875.3507 - val_loss: 1842.0942\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 850.5130 - val_loss: 1842.0842\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 856.3292 - val_loss: 1842.0814\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 884.2217 - val_loss: 1842.0856\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 909.9972 - val_loss: 1842.0778\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 848.6156 - val_loss: 1842.0719\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 21ms/step - loss: 4.2111 - val_loss: 2.6734\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2577 - val_loss: 2.6763\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2346 - val_loss: 2.6586\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.2244 - val_loss: 2.6540\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2184 - val_loss: 2.6156\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2211 - val_loss: 2.5566\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2052 - val_loss: 2.5378\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1603 - val_loss: 2.5406\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1482 - val_loss: 2.5461\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1444 - val_loss: 2.5426\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1289 - val_loss: 2.5133\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1311 - val_loss: 2.5240\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.1333 - val_loss: 2.6589\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1384 - val_loss: 2.5121\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1189 - val_loss: 2.5008\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1061 - val_loss: 2.5063\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1258 - val_loss: 2.5884\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1306 - val_loss: 2.4963\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1145 - val_loss: 2.5895\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1243 - val_loss: 2.5217\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1737 - val_loss: 2.5280\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1581 - val_loss: 2.5178\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2004 - val_loss: 2.5213\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1395 - val_loss: 2.5319\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2061 - val_loss: 2.4969\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1376 - val_loss: 2.4964\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1235 - val_loss: 2.5212\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1664 - val_loss: 2.5026\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0745 - val_loss: 2.4901\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0846 - val_loss: 2.4915\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0804 - val_loss: 2.4916\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0779 - val_loss: 2.4924\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0763 - val_loss: 2.4933\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0755 - val_loss: 2.4961\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0716 - val_loss: 2.4935\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0773 - val_loss: 2.4909\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0769 - val_loss: 2.4953\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0727 - val_loss: 2.4926\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0783 - val_loss: 2.4943\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0798 - val_loss: 2.4921\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0761 - val_loss: 2.4935\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0705 - val_loss: 2.4916\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0689 - val_loss: 2.4945\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0758 - val_loss: 2.4924\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0668 - val_loss: 2.4920\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0662 - val_loss: 2.4924\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0689 - val_loss: 2.4906\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0697 - val_loss: 2.4914\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0670 - val_loss: 2.4922\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0622 - val_loss: 2.4922\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2947.7338 - val_loss: 2290.7734\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 1007.6469 - val_loss: 2122.2654\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 953.1200 - val_loss: 1970.2738\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 22ms/step - loss: 909.7432 - val_loss: 1903.5005\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 888.8900 - val_loss: 1819.0363\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 888.1172 - val_loss: 1806.2953\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 871.2547 - val_loss: 1846.0850\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 887.5911 - val_loss: 1789.7539\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 896.2300 - val_loss: 1846.1375\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 890.1363 - val_loss: 1867.7222\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 861.0625 - val_loss: 2032.3153\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 849.9722 - val_loss: 1782.5918\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 840.7013 - val_loss: 1894.1136\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 912.4452 - val_loss: 1899.4291\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 897.8351 - val_loss: 1919.7548\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 855.1760 - val_loss: 1887.6091\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 830.0642 - val_loss: 1924.0800\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 858.8498 - val_loss: 1845.0845\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 836.4367 - val_loss: 1824.7490\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 825.8812 - val_loss: 1797.8591\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 857.2516 - val_loss: 1853.6268\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 817.8252 - val_loss: 1824.4351\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 856.3923 - val_loss: 1849.5951\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 822.1170 - val_loss: 1833.9968\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 832.4046 - val_loss: 1852.3220\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 839.6883 - val_loss: 1843.3296\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 862.1256 - val_loss: 1845.0808\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 834.4726 - val_loss: 1826.1720\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 832.8914 - val_loss: 1833.3944\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 866.0245 - val_loss: 1857.6390\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 845.3700 - val_loss: 1833.5188\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 892.6388 - val_loss: 1847.3356\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 822.6512 - val_loss: 1846.1512\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 901.4742 - val_loss: 1846.8363\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 883.7492 - val_loss: 1845.8962\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 864.5563 - val_loss: 1844.5066\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 870.9581 - val_loss: 1844.0669\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 848.9600 - val_loss: 1843.9153\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 847.3157 - val_loss: 1842.7677\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 847.4754 - val_loss: 1842.3406\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 856.0868 - val_loss: 1843.2062\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 878.2693 - val_loss: 1844.0258\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 905.3542 - val_loss: 1844.1224\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 885.5434 - val_loss: 1844.0717\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 793.1926 - val_loss: 1844.0836\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 863.3128 - val_loss: 1843.9106\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 895.1832 - val_loss: 1843.7832\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 854.5377 - val_loss: 1843.6594\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 836.8318 - val_loss: 1843.5645\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 805.8346 - val_loss: 1843.5082\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 4.2221 - val_loss: 4.9540\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1862 - val_loss: 4.9138\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1433 - val_loss: 4.8699\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1242 - val_loss: 4.8884\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0755 - val_loss: 4.8315\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0677 - val_loss: 4.8386\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0768 - val_loss: 4.8144\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0406 - val_loss: 4.8236\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0599 - val_loss: 4.8082\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0660 - val_loss: 4.8055\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0446 - val_loss: 4.8013\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0470 - val_loss: 4.8014\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0357 - val_loss: 4.7986\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0059 - val_loss: 4.7995\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0270 - val_loss: 4.8042\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0629 - val_loss: 4.7941\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0109 - val_loss: 4.7944\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0302 - val_loss: 4.8173\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0038 - val_loss: 4.7991\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0451 - val_loss: 4.7936\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0405 - val_loss: 4.7925\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0555 - val_loss: 4.7925\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0271 - val_loss: 4.7961\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0300 - val_loss: 4.7935\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0026 - val_loss: 4.7939\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0214 - val_loss: 4.7926\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0225 - val_loss: 4.7910\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0011 - val_loss: 4.7884\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0034 - val_loss: 4.8006\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0412 - val_loss: 4.7955\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0361 - val_loss: 4.7903\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0172 - val_loss: 4.8027\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0340 - val_loss: 4.7955\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0236 - val_loss: 4.7943\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0363 - val_loss: 4.7905\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9983 - val_loss: 4.7944\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0150 - val_loss: 4.7922\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0361 - val_loss: 4.7896\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9763 - val_loss: 4.7163\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9401 - val_loss: 4.7158\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9309 - val_loss: 4.7154\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9559 - val_loss: 4.7160\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9285 - val_loss: 4.7156\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9732 - val_loss: 4.7157\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9641 - val_loss: 4.7160\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9234 - val_loss: 4.7156\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9496 - val_loss: 4.7161\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9580 - val_loss: 4.7164\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9531 - val_loss: 4.7163\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9408 - val_loss: 4.7157\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7530.1792 - val_loss: 15292.7969\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7541.3155 - val_loss: 15292.7686\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7636.2809 - val_loss: 15292.7324\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7601.1652 - val_loss: 15292.7275\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7322.4873 - val_loss: 15292.6914\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8189.5609 - val_loss: 15292.6758\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7443.9471 - val_loss: 15292.6494\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7487.1292 - val_loss: 15292.6426\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7944.5437 - val_loss: 15292.6289\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7595.5089 - val_loss: 15292.6182\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7877.0730 - val_loss: 15292.6152\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7352.2962 - val_loss: 15292.6074\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7516.5288 - val_loss: 15292.5947\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7761.1451 - val_loss: 15292.5938\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7539.4851 - val_loss: 15292.5869\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7531.6113 - val_loss: 15292.5840\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7704.3407 - val_loss: 15292.5811\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7678.4388 - val_loss: 15292.5742\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7532.3789 - val_loss: 15292.5781\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7570.1587 - val_loss: 15292.5654\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7845.8046 - val_loss: 15292.5625\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7641.6741 - val_loss: 15292.5615\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7669.4221 - val_loss: 15292.5615\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7402.0791 - val_loss: 15292.5566\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7676.3039 - val_loss: 15292.5586\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7940.3496 - val_loss: 15292.5576\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7667.6362 - val_loss: 15292.5566\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7569.0896 - val_loss: 15292.5488\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7603.4613 - val_loss: 15292.5547\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7326.0544 - val_loss: 15292.5479\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7762.6918 - val_loss: 15292.5498\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7771.1448 - val_loss: 15292.5488\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7631.2553 - val_loss: 15292.5488\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7465.8656 - val_loss: 15292.5498\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7683.9336 - val_loss: 15292.5400\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7483.4027 - val_loss: 15292.5430\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7875.8391 - val_loss: 15292.5400\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7545.7851 - val_loss: 15292.5400\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8030.1024 - val_loss: 15292.5400\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7165.7581 - val_loss: 15292.5381\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7674.2630 - val_loss: 15292.5410\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7749.9113 - val_loss: 15292.5381\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7695.5176 - val_loss: 15292.5410\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7479.1269 - val_loss: 15292.5361\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7313.0034 - val_loss: 15292.5352\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8030.9034 - val_loss: 15292.5352\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8212.4186 - val_loss: 15292.5332\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7873.3535 - val_loss: 15292.5332\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7828.3272 - val_loss: 15292.5312\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7601.6139 - val_loss: 15292.5332\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.3073 - val_loss: 4.9281\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1331 - val_loss: 4.9072\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0916 - val_loss: 4.8797\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1204 - val_loss: 4.8644\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0920 - val_loss: 4.8497\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0615 - val_loss: 4.8421\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0781 - val_loss: 4.8308\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0421 - val_loss: 4.8238\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0293 - val_loss: 4.8184\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0260 - val_loss: 4.8134\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0232 - val_loss: 4.8034\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0344 - val_loss: 4.8027\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0354 - val_loss: 4.7987\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0351 - val_loss: 4.7909\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 4.0182 - val_loss: 4.7917\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9825 - val_loss: 4.7888\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0136 - val_loss: 4.7844\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0168 - val_loss: 4.7877\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0265 - val_loss: 4.7800\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0040 - val_loss: 4.7747\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9922 - val_loss: 4.7730\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0003 - val_loss: 4.7715\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0013 - val_loss: 4.7671\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9795 - val_loss: 4.7706\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9977 - val_loss: 4.7831\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9896 - val_loss: 4.7848\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9933 - val_loss: 4.7635\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9842 - val_loss: 4.7589\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0112 - val_loss: 4.7588\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9964 - val_loss: 4.7620\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9792 - val_loss: 4.7594\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9950 - val_loss: 4.7553\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9780 - val_loss: 4.7561\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9622 - val_loss: 4.7526\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0019 - val_loss: 4.7679\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9807 - val_loss: 4.7522\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9752 - val_loss: 4.7519\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9751 - val_loss: 4.7513\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9885 - val_loss: 4.7844\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9779 - val_loss: 4.7595\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0036 - val_loss: 4.7496\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9710 - val_loss: 4.7496\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9573 - val_loss: 4.7494\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9656 - val_loss: 4.7460\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9572 - val_loss: 4.7443\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 3.9516 - val_loss: 4.7469\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9982 - val_loss: 4.7436\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9896 - val_loss: 4.7440\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9614 - val_loss: 4.7427\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9768 - val_loss: 4.7464\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 8223.3270 - val_loss: 15292.9717\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7726.8064 - val_loss: 15292.8965\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7834.4626 - val_loss: 15292.8467\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7880.4616 - val_loss: 15292.8203\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7655.8432 - val_loss: 15292.7725\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7751.1848 - val_loss: 15292.7441\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7954.5670 - val_loss: 15292.7207\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7745.5548 - val_loss: 15292.7227\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7767.3909 - val_loss: 15292.6816\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7665.2214 - val_loss: 15292.6719\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7700.1756 - val_loss: 15292.6572\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7450.8710 - val_loss: 15292.6465\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7719.5034 - val_loss: 15292.6318\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7260.1682 - val_loss: 15292.6230\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7487.9273 - val_loss: 15292.6211\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7643.7227 - val_loss: 15292.6113\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7691.1049 - val_loss: 15292.6074\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7304.7811 - val_loss: 15292.5986\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7700.3756 - val_loss: 15292.5908\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7398.6888 - val_loss: 15292.5918\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7637.9052 - val_loss: 15292.5820\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7484.4213 - val_loss: 15292.5811\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7502.8699 - val_loss: 15292.5742\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7554.4206 - val_loss: 15292.5674\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7490.3230 - val_loss: 15292.5703\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7795.9248 - val_loss: 15292.5645\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7403.7563 - val_loss: 15292.5615\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7757.3000 - val_loss: 15292.5547\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7605.9615 - val_loss: 15292.5576\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8045.2960 - val_loss: 15292.5654\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7606.4979 - val_loss: 15292.5479\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 7385.9847 - val_loss: 15292.5479\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7804.1593 - val_loss: 15292.5566\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7558.5858 - val_loss: 15292.5488\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7499.7298 - val_loss: 15292.5400\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7844.1801 - val_loss: 15292.5469\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7462.6520 - val_loss: 15292.5352\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7699.3280 - val_loss: 15292.5332\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7896.4379 - val_loss: 15292.5312\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7543.0172 - val_loss: 15292.5332\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7420.9832 - val_loss: 15292.5303\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7675.3869 - val_loss: 15292.5264\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7553.0074 - val_loss: 15292.5244\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7551.4381 - val_loss: 15292.5215\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7309.3409 - val_loss: 15292.5293\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7509.5277 - val_loss: 15292.5195\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7420.4232 - val_loss: 15292.5195\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7785.8358 - val_loss: 15292.5205\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7678.8038 - val_loss: 15292.5176\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7481.9584 - val_loss: 15292.5195\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 30.6151\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 25.3211 - val_loss: 25.3854\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 21.5832 - val_loss: 22.4271\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 19.3405 - val_loss: 20.3937\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 17.5927 - val_loss: 18.8700\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 16.3216 - val_loss: 17.6693\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 15.3237 - val_loss: 16.6945\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 14.3867 - val_loss: 15.8861\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13.7059 - val_loss: 15.2035\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 13.0840 - val_loss: 14.6187\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 28488.4824\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 13991.0575 - val_loss: 26009.2637\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13162.9395 - val_loss: 24578.1816\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 12213.2413 - val_loss: 23586.8066\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 12226.8540 - val_loss: 22841.4746\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11815.8373 - val_loss: 22255.7754\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11046.9167 - val_loss: 21780.9238\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 10951.4109 - val_loss: 21387.5977\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11505.8299 - val_loss: 21055.2832\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 10637.6885 - val_loss: 20771.1406\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 24.2268 - val_loss: 4.3995\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.5171 - val_loss: 3.9448\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.7943 - val_loss: 4.3113\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 5.8292 - val_loss: 3.0327\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 5.5530 - val_loss: 3.0533\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.2857 - val_loss: 2.9300\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.3845 - val_loss: 4.6370\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.9117 - val_loss: 3.1795\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.3101 - val_loss: 7.1383\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.7393 - val_loss: 12.1033\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 5.3133 - val_loss: 3.1217\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.3995 - val_loss: 3.2567\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.2701 - val_loss: 2.8311\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.3900 - val_loss: 3.0330\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 6.5314 - val_loss: 3.9801\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.0499 - val_loss: 3.5080\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.9624 - val_loss: 2.9753\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.4720 - val_loss: 3.2176\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 5.0222 - val_loss: 2.9588\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.5054 - val_loss: 2.9513\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 5.0737 - val_loss: 2.8625\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.7300 - val_loss: 25.2348\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.3891 - val_loss: 3.1975\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.4348 - val_loss: 2.5312\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.2234 - val_loss: 2.5025\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1952 - val_loss: 2.5591\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1552 - val_loss: 2.5157\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1539 - val_loss: 2.4947\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1892 - val_loss: 2.4991\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1615 - val_loss: 2.5360\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.1589 - val_loss: 2.4935\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1348 - val_loss: 2.5174\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1484 - val_loss: 2.5654\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1489 - val_loss: 2.4940\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1346 - val_loss: 2.5063\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1278 - val_loss: 2.5088\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1229 - val_loss: 2.6570\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1109 - val_loss: 2.5129\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1256 - val_loss: 2.4948\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1342 - val_loss: 2.5017\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1150 - val_loss: 2.4946\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0839 - val_loss: 2.4654\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0595 - val_loss: 2.4709\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0605 - val_loss: 2.4646\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0663 - val_loss: 2.4695\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0612 - val_loss: 2.4608\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0595 - val_loss: 2.4618\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0567 - val_loss: 2.4631\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0612 - val_loss: 2.4710\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0598 - val_loss: 2.4652\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 1702.4388 - val_loss: 2201.6235\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 965.5943 - val_loss: 2050.0396\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 914.7898 - val_loss: 2268.1189\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 898.6177 - val_loss: 1948.8058\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 894.3750 - val_loss: 1806.8871\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 880.4332 - val_loss: 1738.9618\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 927.9959 - val_loss: 2302.3989\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 897.9572 - val_loss: 1729.6053\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 934.3521 - val_loss: 1785.0841\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 948.5608 - val_loss: 1898.4430\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 859.0416 - val_loss: 1712.9351\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 890.3022 - val_loss: 2051.7883\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 879.7612 - val_loss: 1731.4198\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 894.5012 - val_loss: 1840.5504\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 857.9946 - val_loss: 1866.0289\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 875.2409 - val_loss: 1824.7311\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 889.6319 - val_loss: 1722.8110\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 850.8454 - val_loss: 1777.2402\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 909.3410 - val_loss: 2027.7971\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 825.8990 - val_loss: 1858.1724\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 949.4208 - val_loss: 2035.6686\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 847.8429 - val_loss: 1832.0807\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 833.7549 - val_loss: 1856.6469\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 844.8993 - val_loss: 1839.3846\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 866.1791 - val_loss: 1829.2775\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 901.9890 - val_loss: 1863.6609\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 870.0665 - val_loss: 1818.8209\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 874.9195 - val_loss: 1851.0374\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 879.6193 - val_loss: 1820.8263\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 872.4092 - val_loss: 1830.7434\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 825.4251 - val_loss: 1829.1562\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 840.1334 - val_loss: 1832.8572\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 836.7941 - val_loss: 1836.3958\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 850.1839 - val_loss: 1837.4730\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 851.0131 - val_loss: 1839.1616\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 861.3711 - val_loss: 1838.7257\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 924.3823 - val_loss: 1840.8231\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 854.6357 - val_loss: 1838.5906\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 846.9028 - val_loss: 1838.2606\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 834.4705 - val_loss: 1837.5137\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 844.9779 - val_loss: 1837.1981\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 825.7370 - val_loss: 1837.5748\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 837.3768 - val_loss: 1837.6278\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 849.4194 - val_loss: 1837.8231\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 850.1918 - val_loss: 1837.7638\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.7577 - val_loss: 1837.8600\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 878.5701 - val_loss: 1837.9414\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 827.2309 - val_loss: 1837.9386\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 838.5756 - val_loss: 1837.8557\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 848.3791 - val_loss: 1837.8466\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 3.9041 - val_loss: 2.5622\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1506 - val_loss: 2.5017\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1005 - val_loss: 2.6130\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0985 - val_loss: 2.4803\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0903 - val_loss: 2.4969\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0803 - val_loss: 2.4792\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0909 - val_loss: 2.4846\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0751 - val_loss: 2.4881\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0847 - val_loss: 2.4799\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0889 - val_loss: 2.4951\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0878 - val_loss: 2.5286\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0891 - val_loss: 2.4786\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0847 - val_loss: 2.6447\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0937 - val_loss: 2.4832\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0825 - val_loss: 2.4886\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0907 - val_loss: 2.4683\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0851 - val_loss: 2.5100\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0891 - val_loss: 2.5888\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0944 - val_loss: 2.4845\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0935 - val_loss: 2.4837\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0967 - val_loss: 2.4954\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0942 - val_loss: 2.4894\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1010 - val_loss: 2.5077\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0868 - val_loss: 2.4831\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1045 - val_loss: 2.4860\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0996 - val_loss: 2.4810\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0592 - val_loss: 2.4601\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0609 - val_loss: 2.4594\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0592 - val_loss: 2.4688\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0616 - val_loss: 2.4627\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0586 - val_loss: 2.4694\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0587 - val_loss: 2.4669\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0645 - val_loss: 2.4545\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0540 - val_loss: 2.4616\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0655 - val_loss: 2.4631\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0561 - val_loss: 2.4646\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0577 - val_loss: 2.4619\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0547 - val_loss: 2.4669\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0576 - val_loss: 2.4653\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2.0493 - val_loss: 2.4657\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0617 - val_loss: 2.4660\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0555 - val_loss: 2.4616\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0567 - val_loss: 2.4610\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0624 - val_loss: 2.4622\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0617 - val_loss: 2.4613\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0594 - val_loss: 2.4613\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0531 - val_loss: 2.4624\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0633 - val_loss: 2.4616\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0617 - val_loss: 2.4639\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0566 - val_loss: 2.4619\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 2935.1826 - val_loss: 2530.4360\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 1074.2831 - val_loss: 2166.0151\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 945.4107 - val_loss: 1959.5784\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 903.4198 - val_loss: 1910.4532\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 880.3408 - val_loss: 2135.9641\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 872.0944 - val_loss: 1801.6689\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 864.5357 - val_loss: 1964.8937\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 853.4235 - val_loss: 1793.4777\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 877.1269 - val_loss: 1890.3958\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 872.9242 - val_loss: 2041.6931\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 878.5947 - val_loss: 1856.1306\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.9690 - val_loss: 1967.8427\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 844.0244 - val_loss: 1891.3840\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 872.6764 - val_loss: 1923.7571\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 869.1521 - val_loss: 1852.7880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 884.7422 - val_loss: 1905.5704\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 821.1200 - val_loss: 1793.8601\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.0999 - val_loss: 1860.5808\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 852.7506 - val_loss: 1856.9744\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 837.1519 - val_loss: 1838.9926\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 871.4116 - val_loss: 1849.5516\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 852.5438 - val_loss: 1832.5248\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 843.0857 - val_loss: 1852.4501\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 828.7849 - val_loss: 1829.2345\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 842.7951 - val_loss: 1840.7590\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 900.5364 - val_loss: 1837.3326\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 870.7302 - val_loss: 1841.9193\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 834.4521 - val_loss: 1828.3345\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 844.9003 - val_loss: 1833.8818\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 851.6720 - val_loss: 1837.3912\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 842.1137 - val_loss: 1839.1025\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 874.7286 - val_loss: 1841.1647\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 861.5314 - val_loss: 1841.3138\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 835.8035 - val_loss: 1842.4469\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 851.5161 - val_loss: 1842.4816\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 857.1756 - val_loss: 1841.0391\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 881.8327 - val_loss: 1841.4388\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 839.4017 - val_loss: 1840.8052\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 850.5259 - val_loss: 1840.8545\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 838.0125 - val_loss: 1840.8842\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 853.3714 - val_loss: 1840.9563\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 903.3541 - val_loss: 1841.0402\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 852.0736 - val_loss: 1841.0953\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 893.6140 - val_loss: 1841.1455\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 873.1385 - val_loss: 1841.1089\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 897.7075 - val_loss: 1841.0221\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 851.1501 - val_loss: 1841.0543\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 853.9875 - val_loss: 1841.0609\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 866.5905 - val_loss: 1841.0597\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 824.9990 - val_loss: 1841.0709\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 3.9817 - val_loss: 4.7006\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9188 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9137 - val_loss: 4.7006\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9163 - val_loss: 4.7006\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9286 - val_loss: 4.7008\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9275 - val_loss: 4.7007\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9236 - val_loss: 4.7006\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9454 - val_loss: 4.7007\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9288 - val_loss: 4.7009\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9378 - val_loss: 4.7010\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9289 - val_loss: 4.7009\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9081 - val_loss: 4.7008\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9070 - val_loss: 4.7009\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9315 - val_loss: 4.7010\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9336 - val_loss: 4.7009\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9156 - val_loss: 4.7009\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9037 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9490 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9207 - val_loss: 4.7011\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9149 - val_loss: 4.7011\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9306 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9305 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8963 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9229 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9178 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9244 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9222 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9428 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9479 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9297 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9358 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9142 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9342 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9331 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9396 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9306 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9254 - val_loss: 4.7010\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9465 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9178 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9520 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9318 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9200 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9028 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9269 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9324 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9263 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9441 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9208 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9328 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9628 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 23ms/step - loss: 7651.7949 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7615.7084 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7317.9643 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7527.4944 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7777.6668 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7357.0148 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7418.3528 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7652.0055 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7764.1991 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7573.5545 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7637.9477 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7338.3401 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7562.8480 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7848.3364 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7779.6543 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7682.7692 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8046.3936 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7878.4773 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7629.0264 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7622.5638 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7915.9535 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7365.1919 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7784.1002 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7294.0173 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7410.1875 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7492.2210 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7449.6606 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7776.4435 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7837.8576 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7460.1656 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7885.5476 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7597.9483 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7321.2612 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7491.8111 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7302.7067 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7719.1662 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7455.7091 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7360.9679 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7689.4490 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7543.9423 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7190.9133 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7493.8815 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7617.6028 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7508.0790 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7713.7092 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7614.4012 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7634.1439 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7263.8763 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7666.2059 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7496.7905 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 22ms/step - loss: 4.0560 - val_loss: 4.7008\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9151 - val_loss: 4.7009\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9320 - val_loss: 4.7009\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9209 - val_loss: 4.7009\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9236 - val_loss: 4.7010\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9343 - val_loss: 4.7007\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9181 - val_loss: 4.7009\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9238 - val_loss: 4.7007\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.8909 - val_loss: 4.7008\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9317 - val_loss: 4.7008\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9231 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9401 - val_loss: 4.7011\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9202 - val_loss: 4.7011\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9396 - val_loss: 4.7010\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9443 - val_loss: 4.7010\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9508 - val_loss: 4.7010\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9380 - val_loss: 4.7011\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9165 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9551 - val_loss: 4.7011\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9509 - val_loss: 4.7012\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9422 - val_loss: 4.7011\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9329 - val_loss: 4.7011\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9264 - val_loss: 4.7011\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9280 - val_loss: 4.7011\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9420 - val_loss: 4.7011\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9368 - val_loss: 4.7011\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9409 - val_loss: 4.7011\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9450 - val_loss: 4.7011\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9344 - val_loss: 4.7011\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8997 - val_loss: 4.7011\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9158 - val_loss: 4.7011\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9214 - val_loss: 4.7011\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9323 - val_loss: 4.7011\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9405 - val_loss: 4.7011\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9332 - val_loss: 4.7011\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9089 - val_loss: 4.7011\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9108 - val_loss: 4.7011\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9494 - val_loss: 4.7011\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9501 - val_loss: 4.7011\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9142 - val_loss: 4.7011\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9203 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9235 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9182 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9127 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9308 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9199 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.8998 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9270 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9310 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9253 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 7751.5822 - val_loss: 15292.4434\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7622.1949 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7140.0998 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 8018.5740 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7906.2259 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7864.9180 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7266.5899 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7583.1394 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7575.0841 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7978.2731 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7866.8131 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7742.0039 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8061.7812 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7718.0020 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7684.9217 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7490.1363 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7912.8155 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7771.9183 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7352.6970 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8190.0178 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7872.4789 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7884.1120 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7544.4526 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7606.4334 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7612.6476 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7804.7416 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7833.7198 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7571.3331 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7470.9762 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7494.1291 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7570.5409 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7709.6864 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 8159.8395 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8178.0803 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7424.1920 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7794.8983 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: 7822.4126 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7638.3833 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7811.7114 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7125.5740 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7653.6632 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8095.3221 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7891.7411 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7545.6343 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7562.0469 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7801.9859 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7861.3077 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7593.1192 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7361.7998 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7702.4302 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 21.9890 - val_loss: 3.1527\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9687 - val_loss: 10.5372\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8.4445 - val_loss: 22.2781\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 17.6780 - val_loss: 18.2969\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 22.5916 - val_loss: 11.6208\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 19.0882 - val_loss: 42.3745\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 18.9661 - val_loss: 7.3538\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 19.1503 - val_loss: 48.0722\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 20.2548 - val_loss: 9.9385\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 22.1236 - val_loss: 40.4072\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 22.1215 - val_loss: 8.0118\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.3892 - val_loss: 5.1608\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.4370 - val_loss: 8.0728\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.4831 - val_loss: 5.7179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.3089 - val_loss: 5.7506\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 4.0886 - val_loss: 5.7049\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 4.1858 - val_loss: 4.8282\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.2056 - val_loss: 12.9950\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9317 - val_loss: 6.2670\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.2109 - val_loss: 5.6437\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.2459 - val_loss: 7.4011\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.8844 - val_loss: 5.2926\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7944 - val_loss: 5.0122\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.7437 - val_loss: 5.0621\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.7778 - val_loss: 4.7963\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.7369 - val_loss: 4.7542\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7145 - val_loss: 4.8371\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.7589 - val_loss: 4.8150\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.7593 - val_loss: 4.7764\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.7472 - val_loss: 4.7541\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7462 - val_loss: 4.9129\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7602 - val_loss: 4.7802\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.7263 - val_loss: 4.7647\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.6930 - val_loss: 4.7556\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.8171 - val_loss: 4.7631\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7134 - val_loss: 4.7530\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7850 - val_loss: 4.7653\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6883 - val_loss: 4.7565\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.7325 - val_loss: 4.7544\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7572 - val_loss: 4.7587\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7660 - val_loss: 4.7591\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.7319 - val_loss: 4.7592\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7479 - val_loss: 4.7594\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7641 - val_loss: 4.7597\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.7201 - val_loss: 4.7599\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.7266 - val_loss: 4.7598\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7725 - val_loss: 4.7602\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.7105 - val_loss: 4.7603\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.7760 - val_loss: 4.7603\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7714 - val_loss: 4.7605\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2138.3432 - val_loss: 2001.2024\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 961.3037 - val_loss: 2194.1628\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 889.1618 - val_loss: 1731.0038\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 888.0020 - val_loss: 1994.9841\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 913.3643 - val_loss: 1726.8553\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 898.8928 - val_loss: 2015.2100\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 928.0248 - val_loss: 1741.4847\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 840.3298 - val_loss: 1764.1587\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 870.1325 - val_loss: 1806.4153\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 876.1132 - val_loss: 1836.5883\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 865.7847 - val_loss: 1806.9515\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 915.8240 - val_loss: 2099.4165\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 943.4844 - val_loss: 1991.2548\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 918.5803 - val_loss: 2123.0125\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 895.9299 - val_loss: 1884.2102\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 848.3698 - val_loss: 1825.1732\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 825.6838 - val_loss: 1810.7335\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 869.2652 - val_loss: 1833.8899\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 839.5820 - val_loss: 1848.6262\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 838.7652 - val_loss: 1832.7770\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 852.6110 - val_loss: 1850.0171\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 873.4772 - val_loss: 1813.9796\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 852.6187 - val_loss: 1814.7507\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 824.5384 - val_loss: 1815.1995\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 849.2836 - val_loss: 1829.1809\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 829.1585 - val_loss: 1834.7405\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 831.2801 - val_loss: 1835.9934\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 843.7494 - val_loss: 1837.6243\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 863.9979 - val_loss: 1838.7203\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 844.6114 - val_loss: 1834.7909\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 843.1319 - val_loss: 1836.3615\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 842.6505 - val_loss: 1837.0992\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 835.9823 - val_loss: 1838.9814\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 887.9601 - val_loss: 1838.3801\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 858.3512 - val_loss: 1838.1516\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 914.6776 - val_loss: 1839.0104\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 862.2519 - val_loss: 1838.6891\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 853.8061 - val_loss: 1838.5620\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 881.6132 - val_loss: 1838.4425\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 793.0981 - val_loss: 1838.3337\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 846.2518 - val_loss: 1838.4897\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 817.8865 - val_loss: 1838.4944\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 873.5531 - val_loss: 1838.5989\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 833.8961 - val_loss: 1838.7584\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 869.7709 - val_loss: 1838.5475\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 837.8326 - val_loss: 1838.5205\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 863.7690 - val_loss: 1838.5083\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 843.8330 - val_loss: 1838.4934\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 827.4606 - val_loss: 1838.5417\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 817.7685 - val_loss: 1838.5660\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 4.1171 - val_loss: 2.6092\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.2054 - val_loss: 2.5794\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2042 - val_loss: 2.5791\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1651 - val_loss: 2.5637\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1630 - val_loss: 2.5867\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1674 - val_loss: 2.6115\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1682 - val_loss: 2.5623\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1637 - val_loss: 2.5929\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1602 - val_loss: 2.5656\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1683 - val_loss: 2.5549\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1600 - val_loss: 2.5536\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1592 - val_loss: 2.5588\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1438 - val_loss: 2.5699\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1642 - val_loss: 2.5473\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1647 - val_loss: 2.5531\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1706 - val_loss: 2.5623\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.1736 - val_loss: 2.5664\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1651 - val_loss: 2.5913\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.1578 - val_loss: 2.5789\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.1522 - val_loss: 2.5453\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1536 - val_loss: 2.5528\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1729 - val_loss: 2.5537\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1566 - val_loss: 2.5406\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1618 - val_loss: 2.5761\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2246 - val_loss: 2.5552\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1730 - val_loss: 2.5474\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1625 - val_loss: 2.5543\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1755 - val_loss: 2.5810\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1991 - val_loss: 2.5408\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1700 - val_loss: 2.7534\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1705 - val_loss: 2.5611\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2036 - val_loss: 2.5722\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2073 - val_loss: 2.5956\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1308 - val_loss: 2.5381\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1258 - val_loss: 2.5400\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1224 - val_loss: 2.5386\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1166 - val_loss: 2.5425\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1213 - val_loss: 2.5369\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1238 - val_loss: 2.5495\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1095 - val_loss: 2.5380\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1159 - val_loss: 2.5319\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1127 - val_loss: 2.5350\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1164 - val_loss: 2.5412\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1179 - val_loss: 2.5359\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1143 - val_loss: 2.5360\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1129 - val_loss: 2.5304\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1166 - val_loss: 2.5375\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1090 - val_loss: 2.5268\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1088 - val_loss: 2.5355\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1088 - val_loss: 2.5388\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2798.8132 - val_loss: 2305.0557\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 1067.6477 - val_loss: 2051.2483\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 968.5049 - val_loss: 2061.5466\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 922.1309 - val_loss: 2031.2756\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 890.5668 - val_loss: 1855.3899\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 865.7852 - val_loss: 1920.3036\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 880.6889 - val_loss: 1876.0878\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 865.3101 - val_loss: 1813.1434\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 883.2727 - val_loss: 1923.9984\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 840.6888 - val_loss: 1823.8700\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 920.7410 - val_loss: 1876.4370\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 822.5713 - val_loss: 1906.4077\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 849.6351 - val_loss: 1806.2472\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 871.4358 - val_loss: 1863.3700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 818.6147 - val_loss: 1751.9594\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 858.6669 - val_loss: 1986.1924\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 873.5573 - val_loss: 1867.7140\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 856.9552 - val_loss: 1829.6018\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 869.4523 - val_loss: 1865.0651\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 843.8010 - val_loss: 1755.5635\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 840.9022 - val_loss: 1803.5054\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 876.2342 - val_loss: 1892.5886\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 877.9739 - val_loss: 1930.3782\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 875.2568 - val_loss: 1987.4965\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 857.2139 - val_loss: 1854.8322\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 867.1968 - val_loss: 1846.2804\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 892.3709 - val_loss: 1860.0267\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 793.9912 - val_loss: 1836.9683\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 878.7521 - val_loss: 1860.3053\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 810.3900 - val_loss: 1815.2568\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 853.5169 - val_loss: 1849.1130\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 878.1162 - val_loss: 1838.5773\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 839.2006 - val_loss: 1847.0947\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 844.6137 - val_loss: 1847.9220\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 912.8849 - val_loss: 1856.6368\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 816.8699 - val_loss: 1852.5059\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 878.5590 - val_loss: 1850.7406\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 878.3954 - val_loss: 1848.8795\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 847.5721 - val_loss: 1847.5806\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 867.4350 - val_loss: 1847.3300\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 863.5922 - val_loss: 1847.6215\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 839.2081 - val_loss: 1846.0757\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 855.8843 - val_loss: 1845.3671\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 915.1837 - val_loss: 1846.7604\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 866.5593 - val_loss: 1845.3922\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 840.1964 - val_loss: 1845.3596\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 880.1122 - val_loss: 1845.4093\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 849.8607 - val_loss: 1845.5153\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 877.0887 - val_loss: 1845.5776\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 849.6776 - val_loss: 1845.6465\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.2435 - val_loss: 4.9381\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1577 - val_loss: 4.9099\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1254 - val_loss: 4.8719\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0878 - val_loss: 4.8732\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0521 - val_loss: 4.8396\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0791 - val_loss: 4.8201\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0472 - val_loss: 4.8126\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0563 - val_loss: 4.8121\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0528 - val_loss: 4.8128\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0287 - val_loss: 4.8015\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0436 - val_loss: 4.8027\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0207 - val_loss: 4.8008\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0430 - val_loss: 4.8023\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0475 - val_loss: 4.7960\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0263 - val_loss: 4.7982\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0195 - val_loss: 4.7977\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0114 - val_loss: 4.8053\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0142 - val_loss: 4.8004\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0283 - val_loss: 4.7955\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0352 - val_loss: 4.7964\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0360 - val_loss: 4.8011\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0155 - val_loss: 4.7941\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0300 - val_loss: 4.7937\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0160 - val_loss: 4.7896\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0154 - val_loss: 4.7959\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0390 - val_loss: 4.7925\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0139 - val_loss: 4.7900\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0310 - val_loss: 4.7933\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0183 - val_loss: 4.7923\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0472 - val_loss: 4.8133\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0159 - val_loss: 4.7943\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0125 - val_loss: 4.7899\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9916 - val_loss: 4.7889\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0396 - val_loss: 4.7905\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0305 - val_loss: 4.7898\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9983 - val_loss: 4.7963\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0077 - val_loss: 4.7881\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9980 - val_loss: 4.7919\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0559 - val_loss: 4.7921\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0259 - val_loss: 4.7898\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0049 - val_loss: 4.7917\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0305 - val_loss: 4.7957\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0229 - val_loss: 4.7905\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0351 - val_loss: 4.7921\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0048 - val_loss: 4.7908\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0211 - val_loss: 4.7900\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0325 - val_loss: 4.7960\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9662 - val_loss: 4.7161\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9186 - val_loss: 4.7146\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9273 - val_loss: 4.7147\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7749.0594 - val_loss: 15292.7930\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7465.5024 - val_loss: 15292.7461\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7312.3654 - val_loss: 15292.7354\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8137.8254 - val_loss: 15292.7334\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7168.1580 - val_loss: 15292.7227\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7360.8721 - val_loss: 15292.6904\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7339.5521 - val_loss: 15292.6494\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7174.2574 - val_loss: 15292.6377\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7840.5338 - val_loss: 15292.6270\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7812.0707 - val_loss: 15292.6309\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7520.2614 - val_loss: 15292.6182\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7413.2643 - val_loss: 15292.6035\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7699.8726 - val_loss: 15292.5908\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7903.6944 - val_loss: 15292.5898\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7840.2417 - val_loss: 15292.5869\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7651.7123 - val_loss: 15292.5889\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7335.4680 - val_loss: 15292.5898\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7722.3584 - val_loss: 15292.5830\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7830.3917 - val_loss: 15292.5781\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7619.3812 - val_loss: 15292.5713\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7627.7551 - val_loss: 15292.5654\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7777.9902 - val_loss: 15292.5703\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7583.2723 - val_loss: 15292.5635\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7579.6841 - val_loss: 15292.5596\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7459.6254 - val_loss: 15292.5752\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7579.5309 - val_loss: 15292.5566\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7919.6511 - val_loss: 15292.5527\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7833.8833 - val_loss: 15292.5537\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7822.6728 - val_loss: 15292.5527\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7607.8448 - val_loss: 15292.5518\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7506.5175 - val_loss: 15292.5488\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7635.1468 - val_loss: 15292.5488\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7391.5271 - val_loss: 15292.5547\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7386.3462 - val_loss: 15292.5488\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7596.4365 - val_loss: 15292.5410\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7686.3614 - val_loss: 15292.5439\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7661.3389 - val_loss: 15292.5430\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7920.3258 - val_loss: 15292.5430\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7733.9319 - val_loss: 15292.5430\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7881.0740 - val_loss: 15292.5381\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7673.4372 - val_loss: 15292.5381\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7714.5543 - val_loss: 15292.5361\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7454.1161 - val_loss: 15292.5352\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7671.0425 - val_loss: 15292.5371\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7481.7098 - val_loss: 15292.5371\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7527.3145 - val_loss: 15292.5332\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7806.2993 - val_loss: 15292.5391\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7614.1655 - val_loss: 15292.5332\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7757.2916 - val_loss: 15292.5332\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7469.8050 - val_loss: 15292.5361\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.2952 - val_loss: 4.9265\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1373 - val_loss: 4.9005\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.1058 - val_loss: 4.8798\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1004 - val_loss: 4.8648\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0920 - val_loss: 4.8641\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0727 - val_loss: 4.8402\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0555 - val_loss: 4.8486\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0587 - val_loss: 4.8234\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0663 - val_loss: 4.8175\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0332 - val_loss: 4.8085\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0332 - val_loss: 4.8124\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0326 - val_loss: 4.7995\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0237 - val_loss: 4.8016\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0014 - val_loss: 4.7977\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0425 - val_loss: 4.7886\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0197 - val_loss: 4.7841\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0180 - val_loss: 4.7852\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9940 - val_loss: 4.7781\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0024 - val_loss: 4.7760\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9863 - val_loss: 4.7742\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0115 - val_loss: 4.7715\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0270 - val_loss: 4.7715\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0044 - val_loss: 4.8021\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0164 - val_loss: 4.7650\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9936 - val_loss: 4.7665\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9955 - val_loss: 4.7632\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9698 - val_loss: 4.7611\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0014 - val_loss: 4.7625\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9992 - val_loss: 4.7576\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0073 - val_loss: 4.7758\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0013 - val_loss: 4.7660\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9755 - val_loss: 4.7544\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9898 - val_loss: 4.7573\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9868 - val_loss: 4.7716\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9951 - val_loss: 4.7553\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9773 - val_loss: 4.7545\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9702 - val_loss: 4.7510\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9895 - val_loss: 4.7540\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9808 - val_loss: 4.7599\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9412 - val_loss: 4.7485\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9610 - val_loss: 4.7483\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9517 - val_loss: 4.7464\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9799 - val_loss: 4.7531\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9870 - val_loss: 4.7513\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9710 - val_loss: 4.7498\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9707 - val_loss: 4.7466\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9397 - val_loss: 4.7897\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9568 - val_loss: 4.7491\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0051 - val_loss: 4.7586\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9633 - val_loss: 4.7418\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 8100.1941 - val_loss: 15292.9727\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8197.4832 - val_loss: 15292.9043\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7535.5760 - val_loss: 15292.8486\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7653.8861 - val_loss: 15292.8115\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7663.0188 - val_loss: 15292.7695\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7384.4126 - val_loss: 15292.7432\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7710.2330 - val_loss: 15292.7393\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7198.0323 - val_loss: 15292.7031\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8027.4012 - val_loss: 15292.6934\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7578.3138 - val_loss: 15292.6758\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7717.4379 - val_loss: 15292.6562\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7522.6673 - val_loss: 15292.6553\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7846.2982 - val_loss: 15292.6367\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7463.2150 - val_loss: 15292.6270\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7519.9253 - val_loss: 15292.6172\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7617.9241 - val_loss: 15292.6240\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7414.1595 - val_loss: 15292.6016\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7603.8861 - val_loss: 15292.5967\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7554.7326 - val_loss: 15292.6055\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7773.4607 - val_loss: 15292.5908\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7514.7579 - val_loss: 15292.5820\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7454.7880 - val_loss: 15292.5830\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8261.7535 - val_loss: 15292.5752\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7903.5759 - val_loss: 15292.5723\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8078.1890 - val_loss: 15292.5723\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7434.2318 - val_loss: 15292.5645\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7313.1246 - val_loss: 15292.5664\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7688.8594 - val_loss: 15292.5527\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7825.3204 - val_loss: 15292.5547\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7551.1996 - val_loss: 15292.5479\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7568.9838 - val_loss: 15292.5459\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7543.8634 - val_loss: 15292.5439\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7250.4422 - val_loss: 15292.5391\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7958.7886 - val_loss: 15292.5391\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7877.7530 - val_loss: 15292.5410\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7322.7567 - val_loss: 15292.5371\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7455.0668 - val_loss: 15292.5391\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7419.3851 - val_loss: 15292.5449\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8098.3903 - val_loss: 15292.5332\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7872.9765 - val_loss: 15292.5410\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7858.1725 - val_loss: 15292.5273\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 7336.6937 - val_loss: 15292.5244\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7486.8179 - val_loss: 15292.5273\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7332.3291 - val_loss: 15292.5312\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7378.7610 - val_loss: 15292.5215\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7629.5430 - val_loss: 15292.5264\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8008.4099 - val_loss: 15292.5156\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7561.6339 - val_loss: 15292.5176\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7473.2633 - val_loss: 15292.5254\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7712.1830 - val_loss: 15292.5166\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 28493.8926\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 14275.7126 - val_loss: 26011.8809\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 12561.5680 - val_loss: 24579.8438\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 12283.0190 - val_loss: 23586.9688\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11689.3988 - val_loss: 22841.9199\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 11673.5620 - val_loss: 22256.6973\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 10661.5487 - val_loss: 21781.6777\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 10703.1451 - val_loss: 21388.3105\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 10692.4599 - val_loss: 21055.9688\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 10667.6936 - val_loss: 20771.2148\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 20.3801 - val_loss: 3.0188\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.7659 - val_loss: 2.9321\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.6584 - val_loss: 18.4631\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.0388 - val_loss: 2.9561\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.5801 - val_loss: 2.8504\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 5.2765 - val_loss: 3.3420\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9699 - val_loss: 5.0206\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.5026 - val_loss: 3.0162\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.3098 - val_loss: 3.7970\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.3861 - val_loss: 3.3306\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.1445 - val_loss: 20.7134\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.1452 - val_loss: 2.9505\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.9004 - val_loss: 7.1258\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.9237 - val_loss: 2.9037\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.3730 - val_loss: 3.9444\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.6648 - val_loss: 2.5684\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1880 - val_loss: 2.5318\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1601 - val_loss: 2.5021\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1406 - val_loss: 2.9758\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1556 - val_loss: 2.5203\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1279 - val_loss: 2.4999\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1117 - val_loss: 2.4928\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1121 - val_loss: 2.5066\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1058 - val_loss: 2.4951\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1172 - val_loss: 2.7087\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1212 - val_loss: 2.4784\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1081 - val_loss: 2.4922\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1060 - val_loss: 2.5155\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1082 - val_loss: 2.5799\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1172 - val_loss: 2.4698\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1186 - val_loss: 2.5133\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1096 - val_loss: 2.8026\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1246 - val_loss: 2.5059\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1036 - val_loss: 2.4900\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1099 - val_loss: 2.4868\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1063 - val_loss: 2.4940\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1166 - val_loss: 2.5515\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1070 - val_loss: 2.4971\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1163 - val_loss: 2.5048\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1096 - val_loss: 2.5206\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0732 - val_loss: 2.4655\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0657 - val_loss: 2.4699\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0592 - val_loss: 2.4650\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0599 - val_loss: 2.4669\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0589 - val_loss: 2.4621\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0694 - val_loss: 2.4661\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0543 - val_loss: 2.4573\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0572 - val_loss: 2.4622\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0577 - val_loss: 2.4627\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0545 - val_loss: 2.4713\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 1753.7661 - val_loss: 1878.6750\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 974.3937 - val_loss: 2074.3188\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 919.4362 - val_loss: 1987.2161\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 943.0923 - val_loss: 2201.2810\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 953.6744 - val_loss: 1907.2283\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 921.2067 - val_loss: 1971.8619\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 853.1575 - val_loss: 1778.8969\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 880.1858 - val_loss: 1879.6888\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 944.7217 - val_loss: 2122.6353\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 956.0336 - val_loss: 1701.9286\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 900.6584 - val_loss: 1889.9037\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 870.6140 - val_loss: 1830.9790\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 871.9062 - val_loss: 1765.1178\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 916.0939 - val_loss: 1928.6655\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 851.7381 - val_loss: 1896.9781\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 864.1105 - val_loss: 1897.0956\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 882.4999 - val_loss: 1774.3007\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.5153 - val_loss: 1923.0253\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 948.5665 - val_loss: 2030.6855\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 929.0822 - val_loss: 1923.3724\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 930.4912 - val_loss: 1863.0465\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 919.9440 - val_loss: 1844.1959\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 846.6571 - val_loss: 1828.8401\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 841.0923 - val_loss: 1849.1162\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 827.5985 - val_loss: 1811.4410\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 841.8669 - val_loss: 1818.6595\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 884.0187 - val_loss: 1833.3585\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 882.7659 - val_loss: 1822.7268\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 882.2912 - val_loss: 1832.7738\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 828.0531 - val_loss: 1799.6543\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 858.5734 - val_loss: 1817.7577\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 878.6522 - val_loss: 1823.8014\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 822.7465 - val_loss: 1826.1323\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 869.3639 - val_loss: 1829.2461\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 870.7515 - val_loss: 1827.6077\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 883.8646 - val_loss: 1828.9839\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 861.2833 - val_loss: 1829.5031\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 844.9341 - val_loss: 1830.6957\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 874.6168 - val_loss: 1831.9004\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 888.3550 - val_loss: 1832.1160\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 853.5335 - val_loss: 1832.3499\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 874.1918 - val_loss: 1832.5728\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 832.0089 - val_loss: 1832.7754\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 893.8316 - val_loss: 1832.9305\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 865.3650 - val_loss: 1833.1049\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 830.2095 - val_loss: 1833.0962\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 872.1015 - val_loss: 1833.0966\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 907.0322 - val_loss: 1833.2532\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 840.1165 - val_loss: 1833.1667\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 897.5946 - val_loss: 1833.2581\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0252 - val_loss: 2.7880\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1564 - val_loss: 2.5205\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1027 - val_loss: 2.4830\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0964 - val_loss: 2.4808\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0981 - val_loss: 2.4961\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0966 - val_loss: 2.4948\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1081 - val_loss: 2.4955\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0984 - val_loss: 2.4831\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0926 - val_loss: 2.4978\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0995 - val_loss: 2.4968\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1025 - val_loss: 2.5293\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0914 - val_loss: 2.5114\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1122 - val_loss: 2.4883\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1051 - val_loss: 2.4866\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0809 - val_loss: 2.4835\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0814 - val_loss: 2.4850\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0788 - val_loss: 2.4843\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0672 - val_loss: 2.4767\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0744 - val_loss: 2.4786\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0736 - val_loss: 2.4783\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0764 - val_loss: 2.4905\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0780 - val_loss: 2.4833\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0747 - val_loss: 2.4765\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0769 - val_loss: 2.4811\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0688 - val_loss: 2.4752\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0666 - val_loss: 2.4739\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0745 - val_loss: 2.4746\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0681 - val_loss: 2.4717\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0751 - val_loss: 2.4874\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0744 - val_loss: 2.4692\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0657 - val_loss: 2.4806\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0738 - val_loss: 2.4760\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0750 - val_loss: 2.4783\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0715 - val_loss: 2.4677\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0729 - val_loss: 2.4761\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0697 - val_loss: 2.4661\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0678 - val_loss: 2.4699\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0727 - val_loss: 2.4730\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0689 - val_loss: 2.4710\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0699 - val_loss: 2.4712\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0681 - val_loss: 2.4713\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0687 - val_loss: 2.4758\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0693 - val_loss: 2.4710\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0706 - val_loss: 2.4741\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0694 - val_loss: 2.4745\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0692 - val_loss: 2.4630\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2.0722 - val_loss: 2.4652\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0732 - val_loss: 2.4750\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0653 - val_loss: 2.4691\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0681 - val_loss: 2.4655\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2737.9668 - val_loss: 2402.1855\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 951.1616 - val_loss: 2088.2795\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 904.7950 - val_loss: 1897.8278\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 906.2708 - val_loss: 2178.4961\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 831.9526 - val_loss: 1838.1517\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 875.6509 - val_loss: 1992.2670\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 913.3880 - val_loss: 1966.4272\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 893.4997 - val_loss: 1788.4399\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 875.2166 - val_loss: 1827.2731\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 858.9557 - val_loss: 1791.0349\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 847.6389 - val_loss: 1781.8208\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 880.0738 - val_loss: 1896.2472\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 869.5568 - val_loss: 2146.4126\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 814.1787 - val_loss: 1824.5114\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 926.8873 - val_loss: 1842.8623\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 887.4808 - val_loss: 1859.7953\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 877.4128 - val_loss: 1815.9393\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 859.7093 - val_loss: 1783.3936\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 847.0991 - val_loss: 1938.4200\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 859.2545 - val_loss: 1904.7362\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 835.7985 - val_loss: 1781.0684\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 888.3124 - val_loss: 1836.9735\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.8752 - val_loss: 1841.6838\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 889.3819 - val_loss: 1827.1829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 858.8014 - val_loss: 1798.3586\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 874.4023 - val_loss: 1698.0912\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.1814 - val_loss: 1839.7393\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 852.7688 - val_loss: 1856.2761\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.1767 - val_loss: 1889.8341\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 863.3006 - val_loss: 1841.0566\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 800.5480 - val_loss: 1866.3389\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 838.0110 - val_loss: 1729.9700\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 886.0915 - val_loss: 1767.8679\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 889.0329 - val_loss: 1784.4768\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 836.3547 - val_loss: 1828.2953\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.5618 - val_loss: 1782.2108\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 878.2901 - val_loss: 1854.6981\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.7648 - val_loss: 1861.6538\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 858.6174 - val_loss: 1852.8383\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 878.6905 - val_loss: 1845.0148\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.3875 - val_loss: 1851.2137\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 835.5107 - val_loss: 1841.3052\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 850.0902 - val_loss: 1843.6129\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 899.1786 - val_loss: 1837.5867\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 862.5987 - val_loss: 1828.1949\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 907.9533 - val_loss: 1848.3546\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 839.1317 - val_loss: 1845.8842\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 823.0574 - val_loss: 1845.2014\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 855.5878 - val_loss: 1844.6237\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 840.6057 - val_loss: 1845.1912\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9619 - val_loss: 4.7006\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9386 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9377 - val_loss: 4.7008\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9093 - val_loss: 4.7006\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9510 - val_loss: 4.7006\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9143 - val_loss: 4.7011\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9536 - val_loss: 4.7007\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8904 - val_loss: 4.7007\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9323 - val_loss: 4.7012\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9476 - val_loss: 4.7009\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9314 - val_loss: 4.7007\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9207 - val_loss: 4.7009\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9208 - val_loss: 4.7009\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9289 - val_loss: 4.7009\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9283 - val_loss: 4.7012\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9243 - val_loss: 4.7011\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9295 - val_loss: 4.7009\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9306 - val_loss: 4.7011\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9379 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9010 - val_loss: 4.7009\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9350 - val_loss: 4.7008\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9200 - val_loss: 4.7009\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9521 - val_loss: 4.7009\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9526 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9265 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9192 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9359 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9257 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9487 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9398 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9315 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9079 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9412 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9359 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9113 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9251 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9300 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9072 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9447 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9355 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9420 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9693 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9301 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9425 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9403 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9198 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9629 - val_loss: 4.7010\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9190 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9202 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9330 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7459.4538 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7593.1877 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7959.8073 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7605.2030 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7701.2139 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7709.9360 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7921.2262 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7735.2054 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7697.1360 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7579.5319 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8106.4706 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7367.8823 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7647.1460 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7346.0655 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7894.3179 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7765.8169 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7659.5672 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7666.2592 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7435.5925 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7952.8698 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7505.6480 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7774.4196 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7517.6198 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7877.5914 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7298.6632 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7785.7133 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7586.9380 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7294.0570 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7322.3998 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7728.8597 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7617.6733 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7165.3903 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7440.6574 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8083.1122 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7467.2128 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7619.5431 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7873.4245 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7599.9756 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7714.8792 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7859.4757 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7597.4535 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7229.7359 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7784.5909 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7915.2065 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7524.8653 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7779.6691 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7383.2724 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7473.8016 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7522.3328 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7549.4141 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 4.0301 - val_loss: 4.7007\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9239 - val_loss: 4.7008\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9416 - val_loss: 4.7008\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9124 - val_loss: 4.7011\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9241 - val_loss: 4.7010\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9265 - val_loss: 4.7010\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9264 - val_loss: 4.7014\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9120 - val_loss: 4.7011\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9394 - val_loss: 4.7007\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9157 - val_loss: 4.7015\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9329 - val_loss: 4.7013\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9164 - val_loss: 4.7010\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9315 - val_loss: 4.7010\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9348 - val_loss: 4.7010\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9032 - val_loss: 4.7010\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9490 - val_loss: 4.7010\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9582 - val_loss: 4.7011\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9312 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9197 - val_loss: 4.7010\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9355 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9384 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9188 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9218 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9128 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9133 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9300 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9027 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9023 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9522 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9050 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9339 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9393 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9353 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9356 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9534 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9312 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9655 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9287 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9200 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9448 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9145 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9580 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9407 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9575 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9293 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9209 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9187 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9256 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9244 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9404 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7851.3041 - val_loss: 15292.4404\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7914.1438 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7881.6576 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7420.3627 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7766.9926 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7341.0496 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7506.5422 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7615.8929 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7680.0724 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7772.5917 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7933.6013 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7399.8018 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7835.4342 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7432.9807 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7593.2004 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7554.2363 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7573.6726 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7244.3591 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7770.0200 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7745.4618 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7768.8827 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7386.3018 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7663.8206 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7362.1775 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7668.8680 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7577.8245 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7521.2657 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7590.0783 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7387.9502 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7786.4986 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7438.6034 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7793.2355 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7633.7713 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7481.7364 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7889.9125 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7536.1663 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7583.7506 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7798.8891 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7542.9632 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7657.9203 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7726.7504 - val_loss: 15292.4346\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 7515.0893 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8018.8499 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7634.3436 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7950.4075 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7683.7893 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8167.1667 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7361.8637 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7605.0295 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7969.4787 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 42.8750 - val_loss: 7.5147\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7.4709 - val_loss: 49.9783\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 6.7091 - val_loss: 3.1430\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 10.7590 - val_loss: 38.8822\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 6.3252 - val_loss: 4.5744\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.6542 - val_loss: 4.6463\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0879 - val_loss: 4.4932\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.0342 - val_loss: 4.6180\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.2437 - val_loss: 4.7600\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.3288 - val_loss: 5.0230\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7.2795 - val_loss: 4.7376\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 14.2148 - val_loss: 52.0089\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 16.5077 - val_loss: 30.3844\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 8.9882 - val_loss: 6.8618\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.7182 - val_loss: 11.8820\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.3650 - val_loss: 4.6405\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 4.0675 - val_loss: 5.4480\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 4.2742 - val_loss: 4.6144\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0779 - val_loss: 4.5805\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.1724 - val_loss: 10.3116\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.2200 - val_loss: 10.4027\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.6241 - val_loss: 4.5471\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 4.1971 - val_loss: 6.7540\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 3.6881 - val_loss: 4.6931\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.6108 - val_loss: 4.7670\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.6346 - val_loss: 4.6519\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.5298 - val_loss: 4.6074\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.5766 - val_loss: 5.0984\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.8850 - val_loss: 4.6124\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.5152 - val_loss: 4.6425\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.5525 - val_loss: 4.7558\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.5770 - val_loss: 4.5960\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.5875 - val_loss: 4.9118\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.6177 - val_loss: 4.6621\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.5675 - val_loss: 4.6269\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.5191 - val_loss: 4.5969\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.5554 - val_loss: 4.5870\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.5126 - val_loss: 4.5783\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.5228 - val_loss: 4.5783\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.5694 - val_loss: 4.5815\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.5329 - val_loss: 4.5765\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.5371 - val_loss: 4.5789\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.6009 - val_loss: 4.5835\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.5777 - val_loss: 4.5835\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.5456 - val_loss: 4.5828\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.5557 - val_loss: 4.5824\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.5287 - val_loss: 4.5822\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.5145 - val_loss: 4.5815\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.5760 - val_loss: 4.5811\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.5848 - val_loss: 4.5808\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 1986.8758 - val_loss: 2061.4077\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 955.3499 - val_loss: 2166.8826\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 908.4272 - val_loss: 2090.3252\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 887.9523 - val_loss: 2149.9417\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 919.1057 - val_loss: 1836.1385\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 883.3096 - val_loss: 1910.7017\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 894.8018 - val_loss: 1902.1971\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 887.2931 - val_loss: 1750.2188\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 917.4895 - val_loss: 1746.1039\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 874.1376 - val_loss: 1886.3199\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 888.1093 - val_loss: 1958.1104\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 873.3954 - val_loss: 1835.9625\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 907.4666 - val_loss: 1929.9000\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 873.7917 - val_loss: 2040.5078\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 890.6954 - val_loss: 1936.1355\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 883.6733 - val_loss: 1904.8921\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 897.4604 - val_loss: 1693.5984\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 954.2708 - val_loss: 1761.4072\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 869.6609 - val_loss: 1811.3552\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 913.3965 - val_loss: 1964.2686\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 910.0082 - val_loss: 1976.6617\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 936.2130 - val_loss: 2121.5679\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 937.6112 - val_loss: 1846.8646\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 857.2684 - val_loss: 1958.5875\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 878.9181 - val_loss: 1999.3481\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 880.8273 - val_loss: 1817.5817\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 893.8337 - val_loss: 1843.1827\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 849.1092 - val_loss: 1854.6572\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 821.4880 - val_loss: 1840.0938\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 841.5752 - val_loss: 1828.1376\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 882.7768 - val_loss: 1830.6946\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 852.8943 - val_loss: 1852.0228\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 844.6233 - val_loss: 1842.1700\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 841.3233 - val_loss: 1837.5005\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 858.9164 - val_loss: 1838.3024\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 865.9474 - val_loss: 1851.8301\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 847.9601 - val_loss: 1830.7290\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 854.7392 - val_loss: 1837.3376\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 774.7738 - val_loss: 1836.3738\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 866.8594 - val_loss: 1837.0709\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 870.1499 - val_loss: 1837.5928\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 817.1225 - val_loss: 1837.6260\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 838.4241 - val_loss: 1839.0488\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 833.8853 - val_loss: 1836.4231\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 832.8767 - val_loss: 1839.3335\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 837.7964 - val_loss: 1840.0436\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 830.8869 - val_loss: 1840.4755\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 871.1980 - val_loss: 1840.2875\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 840.3871 - val_loss: 1840.3646\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 790.2267 - val_loss: 1840.5337\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 4.1502 - val_loss: 2.5897\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2253 - val_loss: 2.5667\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1974 - val_loss: 2.5888\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1636 - val_loss: 2.5993\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.1646 - val_loss: 2.5453\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1510 - val_loss: 2.5389\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1428 - val_loss: 2.6316\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1311 - val_loss: 2.5113\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1210 - val_loss: 2.5362\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.1130 - val_loss: 2.5133\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1189 - val_loss: 2.5189\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.1173 - val_loss: 2.5074\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1289 - val_loss: 2.5182\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1121 - val_loss: 2.5052\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1202 - val_loss: 2.5152\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1059 - val_loss: 2.5115\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1267 - val_loss: 2.5132\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1301 - val_loss: 2.5344\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.1134 - val_loss: 2.5125\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1143 - val_loss: 2.5110\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1377 - val_loss: 2.5287\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1245 - val_loss: 2.5026\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1756 - val_loss: 2.5713\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1780 - val_loss: 2.5165\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3111 - val_loss: 2.5154\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1621 - val_loss: 2.5615\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1997 - val_loss: 2.7946\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.2179 - val_loss: 2.5521\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2533 - val_loss: 2.5198\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2894 - val_loss: 2.4958\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1652 - val_loss: 2.4872\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3147 - val_loss: 2.5070\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5688 - val_loss: 2.4481\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2657 - val_loss: 2.5637\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2582 - val_loss: 2.5324\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3581 - val_loss: 2.4540\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3662 - val_loss: 4.9292\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6162 - val_loss: 2.4678\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.2414 - val_loss: 2.5084\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4028 - val_loss: 2.6044\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0793 - val_loss: 2.5321\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2949 - val_loss: 19.4437\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.3377 - val_loss: 2.4647\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.0289 - val_loss: 2.4422\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0240 - val_loss: 2.4385\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0625 - val_loss: 2.4400\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0250 - val_loss: 2.4403\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0385 - val_loss: 2.4405\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0265 - val_loss: 2.4385\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0271 - val_loss: 2.4381\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 24ms/step - loss: 2604.7424 - val_loss: 2338.6826\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 1084.6993 - val_loss: 2022.9147\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 904.5299 - val_loss: 2001.7356\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 871.6915 - val_loss: 1880.7572\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 895.1928 - val_loss: 1864.9862\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 968.7850 - val_loss: 1983.5128\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 945.3658 - val_loss: 1868.0624\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 873.6426 - val_loss: 2045.5432\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 941.6557 - val_loss: 1818.9784\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 859.0442 - val_loss: 1857.7456\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 874.4658 - val_loss: 1804.6348\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 920.5748 - val_loss: 1865.8390\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 820.4588 - val_loss: 1822.9540\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 831.4798 - val_loss: 1869.1863\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 858.1474 - val_loss: 1884.9382\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 873.6007 - val_loss: 1842.2970\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 884.2264 - val_loss: 2006.5327\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 852.2127 - val_loss: 1770.9255\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 871.4644 - val_loss: 1855.5442\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 886.5483 - val_loss: 1774.7675\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 913.1465 - val_loss: 1851.8552\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 844.1921 - val_loss: 2004.8027\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 865.3016 - val_loss: 1909.1693\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 840.3363 - val_loss: 1812.0695\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 864.0666 - val_loss: 1996.5869\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 862.9516 - val_loss: 1813.4734\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 876.4428 - val_loss: 1875.8291\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 877.3864 - val_loss: 1770.2236\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 851.3220 - val_loss: 1864.6790\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 888.3908 - val_loss: 1841.7507\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 856.1254 - val_loss: 1824.2515\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 798.7809 - val_loss: 1863.1819\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 814.3563 - val_loss: 1901.9641\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 845.6556 - val_loss: 1796.6752\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 883.5383 - val_loss: 1778.7510\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 829.6041 - val_loss: 1830.1838\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 854.7493 - val_loss: 1925.0437\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 937.7722 - val_loss: 1973.2426\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 872.5053 - val_loss: 1877.6606\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 812.8073 - val_loss: 1845.4963\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 843.9918 - val_loss: 1854.0784\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 857.4837 - val_loss: 1853.7136\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 845.6969 - val_loss: 1844.9738\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 863.1765 - val_loss: 1853.1161\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 865.5622 - val_loss: 1837.3386\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 844.7647 - val_loss: 1827.2867\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 815.1482 - val_loss: 1827.1576\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 861.1644 - val_loss: 1865.7220\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 849.3861 - val_loss: 1857.6425\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 847.6759 - val_loss: 1853.9724\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.2534 - val_loss: 4.9257\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1876 - val_loss: 4.9160\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.1330 - val_loss: 4.8787\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1076 - val_loss: 4.8577\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0930 - val_loss: 4.8320\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0619 - val_loss: 4.8207\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0408 - val_loss: 4.8440\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0143 - val_loss: 4.8143\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0506 - val_loss: 4.8205\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0371 - val_loss: 4.8318\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0337 - val_loss: 4.8119\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0500 - val_loss: 4.7983\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0316 - val_loss: 4.8003\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0303 - val_loss: 4.8105\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0351 - val_loss: 4.8008\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0342 - val_loss: 4.8178\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0451 - val_loss: 4.7932\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0337 - val_loss: 4.7943\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0290 - val_loss: 4.7965\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0122 - val_loss: 4.7959\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0252 - val_loss: 4.7949\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0292 - val_loss: 4.7969\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0501 - val_loss: 4.7994\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0692 - val_loss: 4.7947\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0104 - val_loss: 4.7908\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0380 - val_loss: 4.7892\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0252 - val_loss: 4.7918\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0222 - val_loss: 4.7935\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 4.0169 - val_loss: 4.7885\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9990 - val_loss: 4.7896\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0040 - val_loss: 4.7926\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0326 - val_loss: 4.7977\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0277 - val_loss: 4.7931\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0343 - val_loss: 4.7952\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0347 - val_loss: 4.7893\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9901 - val_loss: 4.7888\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0152 - val_loss: 4.7902\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9950 - val_loss: 4.7889\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0057 - val_loss: 4.7942\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9443 - val_loss: 4.7157\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9467 - val_loss: 4.7170\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9328 - val_loss: 4.7155\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9449 - val_loss: 4.7153\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9550 - val_loss: 4.7154\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9530 - val_loss: 4.7155\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9441 - val_loss: 4.7162\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9692 - val_loss: 4.7156\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9505 - val_loss: 4.7153\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9460 - val_loss: 4.7173\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9228 - val_loss: 4.7155\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 7754.4837 - val_loss: 15292.7979\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7314.7699 - val_loss: 15292.7314\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7696.4622 - val_loss: 15292.7314\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7432.1126 - val_loss: 15292.7031\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7797.0648 - val_loss: 15292.6758\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7621.4028 - val_loss: 15292.6533\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7756.4611 - val_loss: 15292.6445\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7224.2422 - val_loss: 15292.6309\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7297.8161 - val_loss: 15292.6152\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7889.4562 - val_loss: 15292.6143\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7586.0604 - val_loss: 15292.6113\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7252.8938 - val_loss: 15292.5918\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7325.0254 - val_loss: 15292.5986\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7632.4471 - val_loss: 15292.5830\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7682.0110 - val_loss: 15292.5889\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7568.9143 - val_loss: 15292.5771\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7947.5138 - val_loss: 15292.5664\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7257.2804 - val_loss: 15292.5654\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7642.7723 - val_loss: 15292.5742\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7536.1234 - val_loss: 15292.5654\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7662.6684 - val_loss: 15292.5625\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7576.6066 - val_loss: 15292.5576\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7782.1100 - val_loss: 15292.5596\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7331.6021 - val_loss: 15292.5566\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7624.2350 - val_loss: 15292.5566\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7643.5346 - val_loss: 15292.5537\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7644.9531 - val_loss: 15292.5488\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7489.4914 - val_loss: 15292.5488\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7618.9021 - val_loss: 15292.5449\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8019.7058 - val_loss: 15292.5439\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7813.0608 - val_loss: 15292.5439\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7472.3758 - val_loss: 15292.5430\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7682.5007 - val_loss: 15292.5430\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7318.2993 - val_loss: 15292.5410\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7590.8814 - val_loss: 15292.5391\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7615.4207 - val_loss: 15292.5400\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7665.4593 - val_loss: 15292.5381\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7593.3932 - val_loss: 15292.5400\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7761.0794 - val_loss: 15292.5361\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7648.7001 - val_loss: 15292.5381\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7821.8736 - val_loss: 15292.5352\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7933.3911 - val_loss: 15292.5361\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7791.6588 - val_loss: 15292.5352\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7490.4876 - val_loss: 15292.5332\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7546.6823 - val_loss: 15292.5342\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7662.1713 - val_loss: 15292.5332\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7797.3851 - val_loss: 15292.5332\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7591.2390 - val_loss: 15292.5312\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8015.5895 - val_loss: 15292.5332\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7906.1876 - val_loss: 15292.5332\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.2767 - val_loss: 4.9335\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1573 - val_loss: 4.9066\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1153 - val_loss: 4.8882\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.1076 - val_loss: 4.8669\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0863 - val_loss: 4.8507\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0832 - val_loss: 4.8399\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0578 - val_loss: 4.8296\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0506 - val_loss: 4.8211\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0561 - val_loss: 4.8165\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0196 - val_loss: 4.8126\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0338 - val_loss: 4.8031\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0248 - val_loss: 4.8022\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0047 - val_loss: 4.8001\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9987 - val_loss: 4.7998\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0074 - val_loss: 4.7860\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0044 - val_loss: 4.7854\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0149 - val_loss: 4.7819\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0233 - val_loss: 4.7810\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0087 - val_loss: 4.7757\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0232 - val_loss: 4.7725\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0126 - val_loss: 4.7715\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9992 - val_loss: 4.7775\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9813 - val_loss: 4.7680\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9746 - val_loss: 4.7661\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9826 - val_loss: 4.7633\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9809 - val_loss: 4.7782\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0047 - val_loss: 4.7630\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9600 - val_loss: 4.7606\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9848 - val_loss: 4.7599\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9743 - val_loss: 4.7564\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9735 - val_loss: 4.7557\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9679 - val_loss: 4.7559\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9844 - val_loss: 4.7535\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9976 - val_loss: 4.7537\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9888 - val_loss: 4.7548\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9834 - val_loss: 4.7521\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9621 - val_loss: 4.7496\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9967 - val_loss: 4.7506\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9872 - val_loss: 4.7487\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9806 - val_loss: 4.7476\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9977 - val_loss: 4.7589\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9853 - val_loss: 4.7529\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9743 - val_loss: 4.7445\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9654 - val_loss: 4.7578\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9721 - val_loss: 4.7443\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9585 - val_loss: 4.7456\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9857 - val_loss: 4.7436\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9805 - val_loss: 4.7631\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9850 - val_loss: 4.7537\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9701 - val_loss: 4.7483\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7775.1859 - val_loss: 15292.9756\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7886.3500 - val_loss: 15292.9102\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7489.5714 - val_loss: 15292.8486\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7661.4563 - val_loss: 15292.8037\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7363.1495 - val_loss: 15292.7764\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7605.1605 - val_loss: 15292.7568\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7710.8514 - val_loss: 15292.7324\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7778.3498 - val_loss: 15292.7031\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7849.8789 - val_loss: 15292.7031\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7523.7712 - val_loss: 15292.6865\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8051.4262 - val_loss: 15292.6660\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7457.5390 - val_loss: 15292.6484\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7529.9670 - val_loss: 15292.6318\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7803.3051 - val_loss: 15292.6318\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7826.5866 - val_loss: 15292.6191\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7351.6145 - val_loss: 15292.6182\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7763.4618 - val_loss: 15292.6035\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7692.3079 - val_loss: 15292.5967\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7480.1081 - val_loss: 15292.5967\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7986.6982 - val_loss: 15292.5889\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7742.2833 - val_loss: 15292.5820\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7629.8636 - val_loss: 15292.5898\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7108.2973 - val_loss: 15292.5732\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7729.2878 - val_loss: 15292.5850\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7689.6371 - val_loss: 15292.5664\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7855.2913 - val_loss: 15292.5635\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7640.4486 - val_loss: 15292.5586\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8021.7766 - val_loss: 15292.5557\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7405.4452 - val_loss: 15292.5576\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7850.4170 - val_loss: 15292.5742\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7932.5298 - val_loss: 15292.5469\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7531.9925 - val_loss: 15292.5430\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7578.5428 - val_loss: 15292.5449\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7441.9211 - val_loss: 15292.5400\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7501.5291 - val_loss: 15292.5391\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7458.9448 - val_loss: 15292.5352\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7852.2007 - val_loss: 15292.5352\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7508.0855 - val_loss: 15292.5361\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7348.3034 - val_loss: 15292.5342\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7682.9939 - val_loss: 15292.5303\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7540.1113 - val_loss: 15292.5283\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7504.4399 - val_loss: 15292.5244\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7995.9747 - val_loss: 15292.5264\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7428.1803 - val_loss: 15292.5332\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7486.6060 - val_loss: 15292.5303\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7607.8801 - val_loss: 15292.5176\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7767.9324 - val_loss: 15292.5205\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7427.0409 - val_loss: 15292.5215\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8112.3090 - val_loss: 15292.5215\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7391.7467 - val_loss: 15292.5166\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: 30.6129\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 25.8886 - val_loss: 25.3903\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 21.5822 - val_loss: 22.4269\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 19.2141 - val_loss: 20.3941\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 17.5474 - val_loss: 18.8709\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 16.2453 - val_loss: 17.6689\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 15.3762 - val_loss: 16.6967\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 14.4277 - val_loss: 15.8863\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13.6884 - val_loss: 15.2039\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13.1150 - val_loss: 14.6188\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 28493.7090\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 14145.4216 - val_loss: 26011.8320\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13397.0603 - val_loss: 24580.0664\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 13238.7754 - val_loss: 23588.5117\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 12232.9143 - val_loss: 22841.9570\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11233.9931 - val_loss: 22256.1445\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11171.0576 - val_loss: 21780.8672\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11309.1694 - val_loss: 21387.7793\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 10789.0705 - val_loss: 21056.6094\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 10210.3500 - val_loss: 20772.1074\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 21.2825 - val_loss: 2.9894\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.7441 - val_loss: 3.0932\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.2901 - val_loss: 23.2573\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7.1298 - val_loss: 13.6844\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.1708 - val_loss: 3.0612\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.4532 - val_loss: 3.0060\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1634 - val_loss: 3.3543\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7.7353 - val_loss: 3.5120\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.5722 - val_loss: 3.1146\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.1057 - val_loss: 3.1408\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8.4700 - val_loss: 5.2286\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.6104 - val_loss: 2.5523\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1949 - val_loss: 2.5406\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1682 - val_loss: 2.5275\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1575 - val_loss: 2.4962\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1377 - val_loss: 2.5037\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1216 - val_loss: 2.5387\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1108 - val_loss: 2.4919\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1135 - val_loss: 2.4993\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1032 - val_loss: 2.5151\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1244 - val_loss: 2.5086\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1071 - val_loss: 2.4909\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1062 - val_loss: 2.4812\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1060 - val_loss: 2.5170\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1124 - val_loss: 2.5009\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0956 - val_loss: 2.4854\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0999 - val_loss: 2.5208\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1190 - val_loss: 2.5397\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1046 - val_loss: 2.4806\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1157 - val_loss: 2.4832\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0992 - val_loss: 2.5183\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1078 - val_loss: 2.4864\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0996 - val_loss: 2.4775\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1083 - val_loss: 2.4872\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1088 - val_loss: 2.4789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1124 - val_loss: 2.5372\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1223 - val_loss: 2.4956\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0992 - val_loss: 2.4969\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1089 - val_loss: 2.5184\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1238 - val_loss: 2.4937\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0968 - val_loss: 2.4897\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1057 - val_loss: 2.5303\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0993 - val_loss: 2.4990\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0711 - val_loss: 2.4632\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0553 - val_loss: 2.4614\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0557 - val_loss: 2.4660\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0586 - val_loss: 2.4622\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0592 - val_loss: 2.4638\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0634 - val_loss: 2.4638\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0501 - val_loss: 2.4665\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 1819.6902 - val_loss: 1817.2052\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 962.1371 - val_loss: 2234.4971\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 956.8467 - val_loss: 1946.9534\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 931.0804 - val_loss: 2098.0703\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 908.2813 - val_loss: 2011.5610\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 928.8597 - val_loss: 1756.0540\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.4352 - val_loss: 1861.5070\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 898.4925 - val_loss: 1918.5358\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 929.4894 - val_loss: 1843.3103\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 915.1032 - val_loss: 2084.5103\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 877.9339 - val_loss: 1902.5073\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 925.8044 - val_loss: 1959.9697\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 858.3500 - val_loss: 1783.3971\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 893.4728 - val_loss: 1847.3097\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 876.1982 - val_loss: 1884.2131\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 935.5456 - val_loss: 1953.8792\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 830.4968 - val_loss: 1824.1365\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 869.3994 - val_loss: 1855.6250\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 811.8386 - val_loss: 1816.8079\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 888.4354 - val_loss: 1845.3163\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 880.6017 - val_loss: 1859.5841\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 885.7423 - val_loss: 1838.8905\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 868.4877 - val_loss: 1834.5223\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 894.4785 - val_loss: 1840.3898\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 880.4971 - val_loss: 1831.0074\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 877.6977 - val_loss: 1828.4159\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 857.4259 - val_loss: 1833.6587\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 878.7627 - val_loss: 1838.1714\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 841.3628 - val_loss: 1834.3008\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 858.8142 - val_loss: 1834.5850\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 876.6125 - val_loss: 1834.8269\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 881.8246 - val_loss: 1837.7748\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 829.0452 - val_loss: 1834.0707\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 819.1056 - val_loss: 1831.1932\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 846.2534 - val_loss: 1836.5557\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 857.1814 - val_loss: 1833.9615\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 901.4590 - val_loss: 1834.1426\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 857.9115 - val_loss: 1834.1912\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.5116 - val_loss: 1834.1309\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 865.2732 - val_loss: 1834.2874\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 888.0364 - val_loss: 1834.5414\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 847.5017 - val_loss: 1834.6482\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 938.6686 - val_loss: 1834.6207\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 858.4413 - val_loss: 1834.4130\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 839.5918 - val_loss: 1834.3298\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.2021 - val_loss: 1834.4296\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 832.2899 - val_loss: 1834.4426\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 864.9238 - val_loss: 1834.4543\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 873.8949 - val_loss: 1834.4540\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 852.3075 - val_loss: 1834.4559\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.0260 - val_loss: 2.6888\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1974 - val_loss: 2.4995\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1316 - val_loss: 2.5131\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1177 - val_loss: 2.5127\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1041 - val_loss: 2.5373\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0918 - val_loss: 2.4922\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0896 - val_loss: 2.4865\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1001 - val_loss: 2.5381\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0925 - val_loss: 2.5444\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0982 - val_loss: 2.4942\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0864 - val_loss: 2.4846\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0912 - val_loss: 2.4861\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0816 - val_loss: 2.4899\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0932 - val_loss: 2.5078\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1058 - val_loss: 2.4760\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0956 - val_loss: 2.5032\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0944 - val_loss: 2.4919\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0937 - val_loss: 2.4848\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0886 - val_loss: 2.5257\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0939 - val_loss: 2.5160\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0922 - val_loss: 2.4786\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0977 - val_loss: 2.5226\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0964 - val_loss: 2.4782\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0947 - val_loss: 2.4723\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0982 - val_loss: 2.4724\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0987 - val_loss: 2.5128\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0984 - val_loss: 2.4743\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0925 - val_loss: 2.5257\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0954 - val_loss: 2.4846\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0907 - val_loss: 2.4712\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0873 - val_loss: 2.4921\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0865 - val_loss: 2.4768\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0927 - val_loss: 2.5104\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0983 - val_loss: 2.4896\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1065 - val_loss: 2.5423\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1037 - val_loss: 2.4742\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0937 - val_loss: 2.4855\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0919 - val_loss: 2.4692\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0991 - val_loss: 2.5034\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1076 - val_loss: 2.4880\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1064 - val_loss: 2.4712\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0953 - val_loss: 2.5163\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0917 - val_loss: 2.4878\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0931 - val_loss: 2.5035\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0943 - val_loss: 2.4848\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0971 - val_loss: 2.5133\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1034 - val_loss: 2.4885\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0919 - val_loss: 2.5088\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0713 - val_loss: 2.4663\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0567 - val_loss: 2.4685\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2786.5100 - val_loss: 2518.9038\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 1023.6934 - val_loss: 2018.2446\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 960.2861 - val_loss: 1923.4625\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 885.9035 - val_loss: 1771.1930\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 861.3439 - val_loss: 1808.9423\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 897.7470 - val_loss: 1920.9656\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 867.8350 - val_loss: 1906.9550\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 891.2887 - val_loss: 2005.7339\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 929.0682 - val_loss: 1909.4961\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 904.6359 - val_loss: 1825.1853\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 868.9980 - val_loss: 1886.2681\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 854.4098 - val_loss: 1889.3678\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 896.7949 - val_loss: 1865.2117\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 840.6836 - val_loss: 1887.1549\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 877.7609 - val_loss: 1843.7173\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 864.8702 - val_loss: 1864.5129\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 822.3906 - val_loss: 1852.8638\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 841.0342 - val_loss: 1838.8795\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.6533 - val_loss: 1845.4579\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 875.3295 - val_loss: 1854.9836\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 896.4378 - val_loss: 1861.2944\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 850.8946 - val_loss: 1848.7722\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 869.0010 - val_loss: 1843.8092\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 865.2477 - val_loss: 1854.2543\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 842.9941 - val_loss: 1850.0288\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 894.2660 - val_loss: 1847.3856\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 833.6865 - val_loss: 1846.2965\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 826.9162 - val_loss: 1844.6052\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 846.8530 - val_loss: 1845.3972\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 853.5052 - val_loss: 1845.4795\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 852.4819 - val_loss: 1845.5612\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 814.2271 - val_loss: 1845.0680\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 844.4461 - val_loss: 1845.1498\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 858.5496 - val_loss: 1845.2871\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 883.5426 - val_loss: 1845.2952\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 903.0292 - val_loss: 1845.1956\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 870.7621 - val_loss: 1845.1387\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 847.9175 - val_loss: 1845.0992\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 910.5191 - val_loss: 1845.0354\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 851.2209 - val_loss: 1845.1407\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 851.7709 - val_loss: 1845.1414\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 827.4855 - val_loss: 1845.1028\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 840.8093 - val_loss: 1845.0977\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.1833 - val_loss: 1844.9731\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 889.9819 - val_loss: 1844.9688\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 831.5081 - val_loss: 1844.9641\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 841.1307 - val_loss: 1844.9623\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 862.5352 - val_loss: 1844.9626\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 870.1992 - val_loss: 1844.9648\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 859.6598 - val_loss: 1844.9591\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9725 - val_loss: 4.7006\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9548 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9419 - val_loss: 4.7007\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9261 - val_loss: 4.7029\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9177 - val_loss: 4.7008\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9340 - val_loss: 4.7025\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9591 - val_loss: 4.7008\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9333 - val_loss: 4.7018\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9447 - val_loss: 4.7009\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9263 - val_loss: 4.7007\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9453 - val_loss: 4.7012\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9151 - val_loss: 4.7012\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8975 - val_loss: 4.7011\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9091 - val_loss: 4.7010\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9268 - val_loss: 4.7011\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9205 - val_loss: 4.7009\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9276 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9263 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9502 - val_loss: 4.7009\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9307 - val_loss: 4.7011\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9289 - val_loss: 4.7009\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9326 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9543 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9244 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9567 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9014 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9225 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9240 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9457 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9380 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9595 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9175 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9180 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9126 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9475 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9321 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9281 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9103 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9348 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9366 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9198 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9306 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9210 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9404 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.8930 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9343 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9319 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9389 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9155 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9390 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7528.9459 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7617.8775 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7842.2713 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7942.5763 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7717.6940 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7820.8683 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7898.9929 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7590.9852 - val_loss: 15292.4346\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: 7939.9521 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7529.5955 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7530.4270 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7679.9544 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7967.6412 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7368.4066 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7785.4132 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7566.9447 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7873.7339 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7832.3459 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7639.7257 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7656.1102 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7222.5711 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7645.5980 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7574.9319 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7681.8894 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7567.3173 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: 7073.6882 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7402.9014 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7613.9207 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7994.1529 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7808.7686 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7764.8975 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7497.6382 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7602.5176 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7632.4594 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7600.4422 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8011.7168 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7826.7644 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: 7829.9541 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7914.3550 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7574.4728 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7602.0661 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7624.9016 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7667.9718 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7532.9306 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7339.3823 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7753.3549 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7564.4951 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7558.8653 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7189.8378 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8046.5061 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 24ms/step - loss: 4.0649 - val_loss: 4.7016\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9282 - val_loss: 4.7012\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9262 - val_loss: 4.7007\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9684 - val_loss: 4.7009\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9379 - val_loss: 4.7011\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9473 - val_loss: 4.7009\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9370 - val_loss: 4.7007\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9525 - val_loss: 4.7008\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9341 - val_loss: 4.7009\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9178 - val_loss: 4.7009\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9162 - val_loss: 4.7009\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9600 - val_loss: 4.7010\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9549 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9178 - val_loss: 4.7010\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9166 - val_loss: 4.7010\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9386 - val_loss: 4.7010\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9199 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9466 - val_loss: 4.7011\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9350 - val_loss: 4.7011\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9187 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9583 - val_loss: 4.7011\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9327 - val_loss: 4.7011\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9227 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9511 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9162 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9304 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.8986 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9289 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9173 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9330 - val_loss: 4.7010\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9231 - val_loss: 4.7010\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9317 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9262 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9486 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9335 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9174 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9368 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9375 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9148 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9682 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9112 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9222 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9341 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9530 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9456 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9201 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9173 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9323 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9156 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9350 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 7431.2050 - val_loss: 15292.4385\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7547.5007 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7803.2406 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7389.6776 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7848.4969 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7550.9868 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7723.7935 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7736.5713 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7863.8735 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7546.5118 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7919.4242 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7578.7287 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7514.3626 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7963.3069 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7361.1322 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7553.0258 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7637.7190 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7806.7432 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8035.6536 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: 7674.9849 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7747.1161 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7525.6944 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7558.7591 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7979.8640 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7354.5526 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7371.6125 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8083.3740 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7503.9534 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7766.7557 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7854.3776 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7419.3249 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7100.2622 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7537.2348 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7487.0765 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7891.5808 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7540.0078 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7804.1377 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7801.8816 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7296.3203 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7471.0906 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7570.3902 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7521.5246 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7539.1896 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8010.1756 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7691.6976 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7554.7240 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7590.9358 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7348.5520 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7269.2987 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7454.0671 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 21.9552 - val_loss: 4.0129\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8019 - val_loss: 3.1990\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 24.3215 - val_loss: 10.7286\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 16.9755 - val_loss: 23.1075\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 20.3909 - val_loss: 18.4727\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 21.3328 - val_loss: 27.2491\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 20.8076 - val_loss: 31.4823\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 22.3287 - val_loss: 9.7009\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 21.8794 - val_loss: 29.8018\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 23.4421 - val_loss: 33.1378\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 22.2042 - val_loss: 8.8404\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 22.8499 - val_loss: 32.2128\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 10.9936 - val_loss: 6.6980\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.4618 - val_loss: 6.3497\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.2753 - val_loss: 6.3522\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 4.7070 - val_loss: 6.0713\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.6176 - val_loss: 5.3851\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.6529 - val_loss: 5.3699\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 4.9296 - val_loss: 8.4780\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.7140 - val_loss: 5.2240\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.4274 - val_loss: 9.0365\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 4.4848 - val_loss: 5.7712\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0190 - val_loss: 5.1645\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0003 - val_loss: 5.1564\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9139 - val_loss: 5.0920\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0117 - val_loss: 5.1044\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9680 - val_loss: 5.3670\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0458 - val_loss: 5.0725\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0065 - val_loss: 5.0696\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9750 - val_loss: 5.0621\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9684 - val_loss: 5.2562\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9784 - val_loss: 5.1785\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9520 - val_loss: 5.1319\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9841 - val_loss: 5.1273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9871 - val_loss: 5.1541\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9708 - val_loss: 5.1296\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9645 - val_loss: 5.1467\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0370 - val_loss: 5.1788\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0292 - val_loss: 5.1975\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.0052 - val_loss: 5.1684\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.0363 - val_loss: 5.2209\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9726 - val_loss: 5.1629\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.0029 - val_loss: 5.1656\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0381 - val_loss: 5.1674\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9717 - val_loss: 5.1696\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9868 - val_loss: 5.1690\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9709 - val_loss: 5.1680\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9658 - val_loss: 5.1688\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.0337 - val_loss: 5.1692\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9655 - val_loss: 5.1691\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 1680.3109 - val_loss: 2163.7903\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 949.7204 - val_loss: 1930.6320\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 874.2314 - val_loss: 1931.3357\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 853.7607 - val_loss: 1880.7709\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 899.0254 - val_loss: 1604.5293\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 884.9823 - val_loss: 1882.0225\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 963.4818 - val_loss: 1951.0612\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 911.7269 - val_loss: 1858.9908\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 883.2883 - val_loss: 1815.6627\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 863.6669 - val_loss: 2024.9758\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 888.2010 - val_loss: 1682.0229\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 950.1044 - val_loss: 1941.5013\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 903.2497 - val_loss: 1806.6042\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 905.3931 - val_loss: 2074.9009\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 874.3845 - val_loss: 1713.8844\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 841.0584 - val_loss: 1809.8892\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 864.0570 - val_loss: 1813.1426\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 874.2826 - val_loss: 1852.8099\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 844.3925 - val_loss: 1798.3101\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 827.3882 - val_loss: 1790.5344\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 863.5259 - val_loss: 1850.4324\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 883.5262 - val_loss: 1843.5658\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 864.4588 - val_loss: 1851.5927\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 858.6391 - val_loss: 1796.7896\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 859.7719 - val_loss: 1831.3573\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 829.7209 - val_loss: 1833.5382\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 906.0367 - val_loss: 1836.1738\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 873.0870 - val_loss: 1835.6520\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 846.4162 - val_loss: 1836.9817\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 854.2231 - val_loss: 1834.7358\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 881.0220 - val_loss: 1837.4778\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 845.7141 - val_loss: 1837.9758\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 878.0835 - val_loss: 1837.7844\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 826.1956 - val_loss: 1834.0402\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 868.8521 - val_loss: 1835.4475\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 823.1960 - val_loss: 1835.6488\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 864.6444 - val_loss: 1835.7882\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 812.9925 - val_loss: 1835.9315\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 872.6137 - val_loss: 1835.8726\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 845.5903 - val_loss: 1835.8181\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 819.7034 - val_loss: 1835.9363\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 860.1677 - val_loss: 1835.7906\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 815.3551 - val_loss: 1835.7307\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 903.9886 - val_loss: 1835.7338\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 854.1478 - val_loss: 1835.4590\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 807.4390 - val_loss: 1835.4453\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 846.1168 - val_loss: 1835.4487\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 847.5095 - val_loss: 1835.4845\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 878.9158 - val_loss: 1835.4910\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 826.6621 - val_loss: 1835.5172\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.6897 - val_loss: 2.6523\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1562 - val_loss: 2.5004\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1278 - val_loss: 2.5065\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1066 - val_loss: 2.5210\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0896 - val_loss: 2.6418\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.1031 - val_loss: 2.5269\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0989 - val_loss: 2.4981\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1059 - val_loss: 2.4650\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1786 - val_loss: 2.4808\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1865 - val_loss: 2.5873\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1375 - val_loss: 2.4669\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1246 - val_loss: 2.5192\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2382 - val_loss: 2.4714\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0841 - val_loss: 2.4851\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0816 - val_loss: 2.4803\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0902 - val_loss: 2.5000\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0897 - val_loss: 2.4909\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1286 - val_loss: 2.5020\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0616 - val_loss: 2.4576\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0537 - val_loss: 2.4717\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0457 - val_loss: 2.4581\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0574 - val_loss: 2.4648\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0518 - val_loss: 2.4602\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0418 - val_loss: 2.4568\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.0454 - val_loss: 2.4599\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0412 - val_loss: 2.4530\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0414 - val_loss: 2.4555\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0434 - val_loss: 2.4563\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0450 - val_loss: 2.4582\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0473 - val_loss: 2.4603\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0364 - val_loss: 2.4506\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0352 - val_loss: 2.4647\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0366 - val_loss: 2.4448\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.0368 - val_loss: 2.4538\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0377 - val_loss: 2.4458\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.0335 - val_loss: 2.4421\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0429 - val_loss: 2.4507\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0335 - val_loss: 2.4494\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0374 - val_loss: 2.4421\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0456 - val_loss: 2.4502\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0360 - val_loss: 2.4743\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0393 - val_loss: 2.4472\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0345 - val_loss: 2.4487\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0311 - val_loss: 2.4518\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0371 - val_loss: 2.4479\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0340 - val_loss: 2.4531\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0328 - val_loss: 2.4474\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0349 - val_loss: 2.4466\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0282 - val_loss: 2.4448\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0326 - val_loss: 2.4446\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 24ms/step - loss: 3140.5397 - val_loss: 2409.6440\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 1086.1678 - val_loss: 1938.8223\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 923.5575 - val_loss: 1995.0497\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 960.2211 - val_loss: 1908.3671\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 861.1061 - val_loss: 1932.9047\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 853.6639 - val_loss: 1853.8552\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 890.3109 - val_loss: 1939.1945\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 889.3767 - val_loss: 1893.7585\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 864.7095 - val_loss: 1837.7136\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 920.0200 - val_loss: 1849.4152\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 872.9707 - val_loss: 1769.9248\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 821.8920 - val_loss: 1831.2451\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 855.0191 - val_loss: 1853.4501\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 867.1394 - val_loss: 1896.1575\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 893.4136 - val_loss: 1873.0723\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 829.5724 - val_loss: 1929.5364\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 877.7781 - val_loss: 1925.3387\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 872.1798 - val_loss: 1856.9004\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 849.3083 - val_loss: 1817.2010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 833.7924 - val_loss: 1865.9408\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 889.4641 - val_loss: 1847.4042\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 826.4720 - val_loss: 1848.5930\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 853.8420 - val_loss: 1835.7974\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 884.8827 - val_loss: 1849.7037\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 879.7710 - val_loss: 1843.1398\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 818.1425 - val_loss: 1824.8776\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 863.1981 - val_loss: 1850.6937\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 845.2634 - val_loss: 1833.3588\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 843.1404 - val_loss: 1856.5392\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 864.0820 - val_loss: 1830.4980\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 882.7181 - val_loss: 1843.5457\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 854.8589 - val_loss: 1844.6089\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 23ms/step - loss: 885.9015 - val_loss: 1843.6901\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 852.4224 - val_loss: 1843.8528\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 826.2060 - val_loss: 1843.8263\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 852.3576 - val_loss: 1843.3931\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 873.2006 - val_loss: 1844.2847\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 891.2116 - val_loss: 1844.6805\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 864.4976 - val_loss: 1844.8558\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 868.6331 - val_loss: 1845.1646\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 817.3159 - val_loss: 1843.7734\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 845.1634 - val_loss: 1843.8955\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 884.1016 - val_loss: 1843.8933\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 889.1242 - val_loss: 1844.0676\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 864.5330 - val_loss: 1844.1266\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 896.9691 - val_loss: 1844.2102\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 887.3041 - val_loss: 1844.3116\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 873.9510 - val_loss: 1844.3124\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 814.0934 - val_loss: 1844.3383\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 849.6197 - val_loss: 1844.3259\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 4.1975 - val_loss: 4.9305\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.1663 - val_loss: 4.9331\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1146 - val_loss: 4.8723\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0693 - val_loss: 4.8480\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0721 - val_loss: 4.8392\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0787 - val_loss: 4.8185\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0268 - val_loss: 4.8122\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 4.0460 - val_loss: 4.8145\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0557 - val_loss: 4.8088\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0675 - val_loss: 4.8049\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0570 - val_loss: 4.8035\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0089 - val_loss: 4.7994\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0182 - val_loss: 4.8010\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0145 - val_loss: 4.8019\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0452 - val_loss: 4.9599\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0525 - val_loss: 4.7991\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0314 - val_loss: 4.7970\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0334 - val_loss: 4.7965\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0342 - val_loss: 4.7970\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0438 - val_loss: 4.8099\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9938 - val_loss: 4.7942\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0399 - val_loss: 4.7966\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0364 - val_loss: 4.7931\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0147 - val_loss: 4.7995\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0183 - val_loss: 4.7910\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 4.0232 - val_loss: 4.7917\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0072 - val_loss: 4.7995\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0233 - val_loss: 4.7899\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0395 - val_loss: 4.7959\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0344 - val_loss: 4.7895\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0141 - val_loss: 4.7894\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0121 - val_loss: 4.7887\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0354 - val_loss: 4.7941\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0083 - val_loss: 4.8024\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0283 - val_loss: 4.7882\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0275 - val_loss: 4.7916\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0304 - val_loss: 4.7872\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0393 - val_loss: 4.7908\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0008 - val_loss: 4.7909\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0181 - val_loss: 4.7915\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0502 - val_loss: 4.7894\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0208 - val_loss: 4.7884\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0303 - val_loss: 4.7896\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0226 - val_loss: 4.7906\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0391 - val_loss: 4.7870\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0030 - val_loss: 4.7893\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0070 - val_loss: 4.7885\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0065 - val_loss: 4.7943\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0109 - val_loss: 4.7890\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9853 - val_loss: 4.7898\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 7685.2120 - val_loss: 15292.8438\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7642.2331 - val_loss: 15292.7441\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7379.0046 - val_loss: 15292.7393\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7249.8724 - val_loss: 15292.7217\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7633.4716 - val_loss: 15292.6904\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7767.5809 - val_loss: 15292.6758\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7898.8274 - val_loss: 15292.6562\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7690.8470 - val_loss: 15292.6279\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7488.9040 - val_loss: 15292.6191\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 7581.6385 - val_loss: 15292.6143\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7525.8045 - val_loss: 15292.6006\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7919.4876 - val_loss: 15292.5986\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7489.1613 - val_loss: 15292.5928\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7195.3533 - val_loss: 15292.5850\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7560.0654 - val_loss: 15292.5850\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7650.5444 - val_loss: 15292.5830\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7595.9632 - val_loss: 15292.5771\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7742.2082 - val_loss: 15292.5771\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7978.5357 - val_loss: 15292.5703\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7837.4719 - val_loss: 15292.5635\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7224.5409 - val_loss: 15292.5625\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7913.7354 - val_loss: 15292.5664\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8026.3985 - val_loss: 15292.5576\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7345.7408 - val_loss: 15292.5615\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7600.0680 - val_loss: 15292.5527\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7650.1029 - val_loss: 15292.5547\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7124.7971 - val_loss: 15292.5566\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7705.4714 - val_loss: 15292.5576\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7866.3909 - val_loss: 15292.5469\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7760.4470 - val_loss: 15292.5439\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7462.6050 - val_loss: 15292.5439\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7797.2340 - val_loss: 15292.5488\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7597.4510 - val_loss: 15292.5439\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7952.3502 - val_loss: 15292.5469\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7511.2908 - val_loss: 15292.5439\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7439.5057 - val_loss: 15292.5430\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7638.3509 - val_loss: 15292.5410\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7647.2029 - val_loss: 15292.5391\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7819.7883 - val_loss: 15292.5400\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7223.7308 - val_loss: 15292.5410\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7733.9395 - val_loss: 15292.5381\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7872.6201 - val_loss: 15292.5381\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8135.4409 - val_loss: 15292.5391\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7465.2062 - val_loss: 15292.5352\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7487.3743 - val_loss: 15292.5381\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7582.0765 - val_loss: 15292.5352\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7370.8956 - val_loss: 15292.5352\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7803.1202 - val_loss: 15292.5352\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7295.5173 - val_loss: 15292.5352\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7698.3004 - val_loss: 15292.5352\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 4.3102 - val_loss: 4.9275\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.1413 - val_loss: 4.9066\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1079 - val_loss: 4.8813\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0954 - val_loss: 4.8653\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0994 - val_loss: 4.8529\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0685 - val_loss: 4.8394\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0698 - val_loss: 4.8306\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0443 - val_loss: 4.8223\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0515 - val_loss: 4.8164\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0368 - val_loss: 4.8092\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0415 - val_loss: 4.8033\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0296 - val_loss: 4.8123\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0122 - val_loss: 4.7933\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0074 - val_loss: 4.7912\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0181 - val_loss: 4.7892\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0278 - val_loss: 4.7872\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0030 - val_loss: 4.7855\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0249 - val_loss: 4.7772\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0313 - val_loss: 4.7750\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0234 - val_loss: 4.7753\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9948 - val_loss: 4.7705\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9915 - val_loss: 4.7697\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9818 - val_loss: 4.7684\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0110 - val_loss: 4.7659\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9989 - val_loss: 4.7661\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 3.9888 - val_loss: 4.7635\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0221 - val_loss: 4.7602\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0037 - val_loss: 4.7652\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9822 - val_loss: 4.7582\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9850 - val_loss: 4.7570\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0047 - val_loss: 4.7562\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9721 - val_loss: 4.7539\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9908 - val_loss: 4.7737\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9763 - val_loss: 4.7580\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9836 - val_loss: 4.7559\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9767 - val_loss: 4.7548\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9585 - val_loss: 4.7496\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9748 - val_loss: 4.7510\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9616 - val_loss: 4.7654\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9691 - val_loss: 4.7487\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9671 - val_loss: 4.7472\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9453 - val_loss: 4.7478\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9909 - val_loss: 4.7459\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0175 - val_loss: 4.7452\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9485 - val_loss: 4.7634\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9756 - val_loss: 4.7442\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9547 - val_loss: 4.7501\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9660 - val_loss: 4.7435\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9860 - val_loss: 4.7434\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9709 - val_loss: 4.7432\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7365.1493 - val_loss: 15292.9648\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7555.7886 - val_loss: 15292.8975\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7439.2325 - val_loss: 15292.8584\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8007.7321 - val_loss: 15292.8145\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7659.9261 - val_loss: 15292.7803\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7309.2239 - val_loss: 15292.7432\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7468.3665 - val_loss: 15292.7344\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7834.7845 - val_loss: 15292.7246\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7734.5597 - val_loss: 15292.7051\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7606.4780 - val_loss: 15292.6914\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7890.8047 - val_loss: 15292.6631\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7337.0143 - val_loss: 15292.6533\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7951.3527 - val_loss: 15292.6406\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7761.5257 - val_loss: 15292.6230\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7194.6464 - val_loss: 15292.6240\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7719.9836 - val_loss: 15292.6152\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7562.7521 - val_loss: 15292.6035\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7732.0396 - val_loss: 15292.5996\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7661.2475 - val_loss: 15292.5918\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7701.5868 - val_loss: 15292.5869\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7323.5297 - val_loss: 15292.5898\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7414.1356 - val_loss: 15292.5938\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7596.6100 - val_loss: 15292.5820\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7749.6601 - val_loss: 15292.5771\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7418.7371 - val_loss: 15292.5674\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7431.4846 - val_loss: 15292.5664\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8125.3436 - val_loss: 15292.5596\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7915.7317 - val_loss: 15292.5557\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7385.3516 - val_loss: 15292.5527\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7854.6902 - val_loss: 15292.5498\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7249.6039 - val_loss: 15292.5557\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7576.0284 - val_loss: 15292.5449\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7734.0069 - val_loss: 15292.5449\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7746.3537 - val_loss: 15292.5439\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7830.8318 - val_loss: 15292.5459\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7640.1979 - val_loss: 15292.5391\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7614.8884 - val_loss: 15292.5381\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7582.8578 - val_loss: 15292.5312\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7339.8755 - val_loss: 15292.5283\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7497.0994 - val_loss: 15292.5303\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7426.6296 - val_loss: 15292.5264\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7617.7775 - val_loss: 15292.5244\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7458.2523 - val_loss: 15292.5264\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7583.4731 - val_loss: 15292.5244\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7700.5367 - val_loss: 15292.5312\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7397.3188 - val_loss: 15292.5176\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7595.0935 - val_loss: 15292.5391\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7834.1654 - val_loss: 15292.5205\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7798.8771 - val_loss: 15292.5195\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7556.0589 - val_loss: 15292.5156\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 30.6129\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 25.7444 - val_loss: 25.3849\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 21.6608 - val_loss: 22.4286\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 19.1862 - val_loss: 20.3926\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 17.5506 - val_loss: 18.8692\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 16.1568 - val_loss: 17.6692\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 15.2678 - val_loss: 16.6946\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 14.3826 - val_loss: 15.8853\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 13.5696 - val_loss: 15.2027\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13.0191 - val_loss: 14.6186\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 20.9582 - val_loss: 3.4994\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.3489 - val_loss: 15.3794\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.2998 - val_loss: 2.9856\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.7483 - val_loss: 2.9066\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8.5899 - val_loss: 2.9059\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.5730 - val_loss: 3.0940\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1452 - val_loss: 2.8519\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.2035 - val_loss: 3.2975\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7.5803 - val_loss: 3.2534\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.5838 - val_loss: 3.6109\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.3384 - val_loss: 19.5533\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.0594 - val_loss: 22.1533\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.8464 - val_loss: 13.7946\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8.4765 - val_loss: 6.3857\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.4879 - val_loss: 3.6801\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.8678 - val_loss: 4.5898\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 5.7333 - val_loss: 2.8910\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.3438 - val_loss: 2.4915\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1538 - val_loss: 2.5737\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1402 - val_loss: 2.5327\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1387 - val_loss: 2.5496\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1252 - val_loss: 2.4829\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1239 - val_loss: 2.5178\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1158 - val_loss: 2.4924\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1158 - val_loss: 2.5436\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1197 - val_loss: 2.5156\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1181 - val_loss: 2.5156\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1164 - val_loss: 2.5029\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1148 - val_loss: 2.5195\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1104 - val_loss: 2.4958\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1117 - val_loss: 2.5120\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1357 - val_loss: 2.4887\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0762 - val_loss: 2.4598\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0552 - val_loss: 2.4700\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0644 - val_loss: 2.4636\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0638 - val_loss: 2.4617\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0634 - val_loss: 2.4592\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0672 - val_loss: 2.4665\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0616 - val_loss: 2.4614\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0554 - val_loss: 2.4623\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0593 - val_loss: 2.4616\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0598 - val_loss: 2.4617\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0586 - val_loss: 2.4651\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0602 - val_loss: 2.4694\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0514 - val_loss: 2.4641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0598 - val_loss: 2.4616\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0535 - val_loss: 2.4697\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0632 - val_loss: 2.4620\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0578 - val_loss: 2.4621\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0570 - val_loss: 2.4615\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 26ms/step - loss: 1402.8287 - val_loss: 2061.1179\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 940.6248 - val_loss: 1780.3378\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 915.3058 - val_loss: 2119.7700\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 899.8712 - val_loss: 1803.2844\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 869.0245 - val_loss: 1865.9698\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 921.5284 - val_loss: 2031.3112\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 895.5090 - val_loss: 1822.5494\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 966.6139 - val_loss: 1911.9424\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 893.6475 - val_loss: 2097.0901\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 907.6978 - val_loss: 1968.9503\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 887.0521 - val_loss: 1853.6605\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 853.3479 - val_loss: 1807.0895\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 862.3511 - val_loss: 1845.7374\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 873.6359 - val_loss: 1831.0364\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 929.8570 - val_loss: 1844.2639\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.2887 - val_loss: 1853.5299\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 842.7645 - val_loss: 1842.0947\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 886.0175 - val_loss: 1874.6486\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 846.2662 - val_loss: 1807.9899\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 910.2117 - val_loss: 1864.7354\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 814.9004 - val_loss: 1874.3021\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 881.4887 - val_loss: 1849.3632\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 874.7176 - val_loss: 1850.3766\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 816.5796 - val_loss: 1846.4667\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 866.1053 - val_loss: 1847.1327\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 890.4675 - val_loss: 1847.0902\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 845.4283 - val_loss: 1845.3390\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 869.9414 - val_loss: 1846.5806\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 951.3113 - val_loss: 1847.8456\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 866.9416 - val_loss: 1846.7430\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 847.1615 - val_loss: 1845.2870\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 826.4510 - val_loss: 1843.4340\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 862.0421 - val_loss: 1843.5906\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 822.5393 - val_loss: 1843.6715\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 806.2237 - val_loss: 1843.7870\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 845.0803 - val_loss: 1843.7621\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 851.1971 - val_loss: 1843.8390\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 867.6888 - val_loss: 1843.7600\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 876.3471 - val_loss: 1843.9191\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 907.6849 - val_loss: 1843.9636\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 890.3322 - val_loss: 1844.2571\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 823.3742 - val_loss: 1844.0685\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 944.2786 - val_loss: 1844.1069\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 849.7931 - val_loss: 1844.0950\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 826.8572 - val_loss: 1844.0999\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 897.8952 - val_loss: 1844.1115\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 844.9853 - val_loss: 1844.1005\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 812.2695 - val_loss: 1844.1162\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 883.5807 - val_loss: 1844.1295\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 832.2533 - val_loss: 1844.1268\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 3.8986 - val_loss: 2.5401\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1326 - val_loss: 2.5196\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1030 - val_loss: 2.4930\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0896 - val_loss: 2.4992\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1011 - val_loss: 2.4853\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0929 - val_loss: 2.4819\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0838 - val_loss: 2.4737\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0866 - val_loss: 2.4800\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0908 - val_loss: 2.4998\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0928 - val_loss: 2.4846\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0920 - val_loss: 2.4821\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0944 - val_loss: 2.5003\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0928 - val_loss: 2.5128\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1039 - val_loss: 2.4938\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0962 - val_loss: 2.5262\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1000 - val_loss: 2.5559\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1027 - val_loss: 2.4813\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0647 - val_loss: 2.4674\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0599 - val_loss: 2.4676\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0682 - val_loss: 2.4737\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0565 - val_loss: 2.4720\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0643 - val_loss: 2.4755\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0550 - val_loss: 2.4651\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0656 - val_loss: 2.4744\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0647 - val_loss: 2.4657\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0627 - val_loss: 2.4692\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0642 - val_loss: 2.4719\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0613 - val_loss: 2.4709\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0669 - val_loss: 2.4700\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0566 - val_loss: 2.4648\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0618 - val_loss: 2.4652\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0596 - val_loss: 2.4626\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0678 - val_loss: 2.4669\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0661 - val_loss: 2.4652\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0698 - val_loss: 2.4706\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0644 - val_loss: 2.4733\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0663 - val_loss: 2.4757\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0582 - val_loss: 2.4667\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0659 - val_loss: 2.4729\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0671 - val_loss: 2.4671\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0630 - val_loss: 2.4666\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0618 - val_loss: 2.4685\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0558 - val_loss: 2.4666\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0608 - val_loss: 2.4671\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0575 - val_loss: 2.4669\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0588 - val_loss: 2.4648\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0575 - val_loss: 2.4666\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0618 - val_loss: 2.4668\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0614 - val_loss: 2.4654\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0603 - val_loss: 2.4683\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 3054.2557 - val_loss: 2354.2285\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 1039.8923 - val_loss: 2269.9272\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 906.1654 - val_loss: 1903.4250\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 908.3644 - val_loss: 1864.5027\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 904.4164 - val_loss: 1944.1636\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 863.5475 - val_loss: 1881.7653\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 870.6089 - val_loss: 1814.2871\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 852.0331 - val_loss: 1749.3517\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 915.3714 - val_loss: 1798.6050\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 878.5582 - val_loss: 1983.0905\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 889.8594 - val_loss: 1806.1316\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.5762 - val_loss: 1807.6674\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 845.8324 - val_loss: 1907.9860\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 883.2344 - val_loss: 1846.6035\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 878.4023 - val_loss: 1762.5400\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 830.3322 - val_loss: 1797.5184\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 873.3799 - val_loss: 1941.2797\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 830.2510 - val_loss: 1950.2871\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 880.7547 - val_loss: 1852.7822\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 834.0880 - val_loss: 1835.4773\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 857.5578 - val_loss: 1839.8949\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 872.4825 - val_loss: 1853.8196\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 870.8375 - val_loss: 1855.3090\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 860.3739 - val_loss: 1860.2410\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 872.2038 - val_loss: 1836.6436\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 851.5849 - val_loss: 1849.1405\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 825.5728 - val_loss: 1814.8892\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 853.2072 - val_loss: 1848.1610\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 885.9745 - val_loss: 1845.0443\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.0847 - val_loss: 1843.7903\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 861.8857 - val_loss: 1843.8295\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 855.1593 - val_loss: 1842.7852\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 867.2534 - val_loss: 1842.0109\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 808.2558 - val_loss: 1841.1056\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 895.0567 - val_loss: 1841.0610\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 864.6880 - val_loss: 1840.5812\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 866.2515 - val_loss: 1841.9095\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 899.2938 - val_loss: 1842.2993\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 863.9575 - val_loss: 1842.3352\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 853.6616 - val_loss: 1842.3561\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 872.3644 - val_loss: 1842.3010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 917.1525 - val_loss: 1842.2787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 901.4708 - val_loss: 1842.2803\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.1801 - val_loss: 1842.3257\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 908.3413 - val_loss: 1842.3036\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 886.8130 - val_loss: 1842.3894\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 841.9954 - val_loss: 1842.3463\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 845.9257 - val_loss: 1842.3838\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 823.9244 - val_loss: 1842.3827\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 901.9008 - val_loss: 1842.3879\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 3.9831 - val_loss: 4.7006\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9584 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9199 - val_loss: 4.7006\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9285 - val_loss: 4.7006\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9401 - val_loss: 4.7028\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9536 - val_loss: 4.7007\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9284 - val_loss: 4.7007\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9262 - val_loss: 4.7009\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9037 - val_loss: 4.7007\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9341 - val_loss: 4.7007\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9268 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9354 - val_loss: 4.7009\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9350 - val_loss: 4.7009\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9152 - val_loss: 4.7010\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9413 - val_loss: 4.7011\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9150 - val_loss: 4.7009\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9218 - val_loss: 4.7011\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9441 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9567 - val_loss: 4.7011\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9270 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9208 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9220 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9310 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9043 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9370 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9288 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9352 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9413 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9447 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9538 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9418 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9295 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9349 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9180 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9190 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9329 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9144 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9211 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9383 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9317 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9310 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9255 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9236 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9359 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9267 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9331 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8972 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9219 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9368 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9467 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7925.5701 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7758.2628 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7766.2228 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7572.3409 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7552.6029 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7998.1913 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7576.9304 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7715.8963 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7723.0195 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7437.8646 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7578.0923 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7210.6811 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7341.1803 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7651.5037 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7434.8233 - val_loss: 15292.4346\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 7617.2114 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7965.0537 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7446.8339 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7755.6281 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7491.5558 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7667.9582 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7776.2667 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7570.0679 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7640.0762 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7302.0953 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7331.2667 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7454.3764 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7192.7799 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7402.3884 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7913.8663 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7381.5636 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7826.9696 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7513.1038 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7546.5718 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7999.0779 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7713.7719 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7832.3968 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7848.6636 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7604.9661 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7946.5551 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8075.0997 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7012.5885 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7656.7731 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7322.9141 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7788.1986 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7852.4768 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7711.7863 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7744.2394 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7659.6145 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7899.3266 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 4.0404 - val_loss: 4.7009\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9033 - val_loss: 4.7007\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9217 - val_loss: 4.7008\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9503 - val_loss: 4.7011\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9283 - val_loss: 4.7009\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9126 - val_loss: 4.7011\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9066 - val_loss: 4.7008\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9569 - val_loss: 4.7012\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9231 - val_loss: 4.7008\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9040 - val_loss: 4.7010\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9218 - val_loss: 4.7008\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9387 - val_loss: 4.7009\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9116 - val_loss: 4.7010\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9386 - val_loss: 4.7010\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9203 - val_loss: 4.7010\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9274 - val_loss: 4.7011\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9367 - val_loss: 4.7011\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9330 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9203 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9317 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9474 - val_loss: 4.7010\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9467 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9408 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9417 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9103 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9413 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9454 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9365 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9346 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9189 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9259 - val_loss: 4.7010\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9361 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9136 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9205 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9163 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9202 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9094 - val_loss: 4.7010\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9162 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9226 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9348 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9269 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9021 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9412 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9367 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9407 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9251 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9340 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9255 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9365 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9087 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7736.6691 - val_loss: 15292.4414\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7700.8902 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7666.7437 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7699.3802 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7637.6302 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7414.9328 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7207.8694 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7273.3316 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7616.3315 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7370.8081 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7846.1582 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7863.4580 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7454.2651 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7824.4530 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7569.3131 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7638.7748 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7334.8729 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7508.2544 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7682.0587 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7884.3827 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7689.8430 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7571.4467 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7858.1668 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7533.7855 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7016.1676 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7914.2799 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7746.3055 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7870.2033 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8133.0555 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7264.3243 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7136.9351 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8055.7894 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 7798.8223 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7687.9307 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 7540.5512 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7539.0124 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7765.4309 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7621.7065 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7721.2284 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7588.4798 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7518.7715 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7865.0748 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7477.1771 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7495.7741 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7405.8678 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7948.8240 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7605.2087 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7672.6934 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7561.4415 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7556.8064 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 22.0905 - val_loss: 2.6710\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.2848 - val_loss: 6.1392\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7.2679 - val_loss: 18.5345\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 17.9210 - val_loss: 10.2411\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 21.9126 - val_loss: 45.2090\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 24.2535 - val_loss: 28.7471\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 27.0629 - val_loss: 12.0887\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 26.0434 - val_loss: 61.7609\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 25.0612 - val_loss: 29.9884\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 24.2357 - val_loss: 12.6651\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 23.0026 - val_loss: 12.4337\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 8.4358 - val_loss: 7.7596\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 6.3694 - val_loss: 8.9143\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 6.6839 - val_loss: 6.8382\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 5.8970 - val_loss: 8.9827\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 6.0490 - val_loss: 10.6486\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.6867 - val_loss: 8.8491\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 5.6293 - val_loss: 6.7815\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.6155 - val_loss: 7.2134\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.4378 - val_loss: 11.1362\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.5521 - val_loss: 7.4721\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9851 - val_loss: 6.4308\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9321 - val_loss: 6.3218\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.0029 - val_loss: 6.2086\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 4.9948 - val_loss: 6.6986\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9292 - val_loss: 6.3115\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9059 - val_loss: 6.1847\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9560 - val_loss: 6.1715\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.9671 - val_loss: 6.2829\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.9909 - val_loss: 6.2097\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9926 - val_loss: 6.1897\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9174 - val_loss: 6.1984\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.9946 - val_loss: 6.2157\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9784 - val_loss: 6.2171\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9546 - val_loss: 6.2067\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.0207 - val_loss: 6.2204\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.9875 - val_loss: 6.2144\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9606 - val_loss: 6.2165\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.9928 - val_loss: 6.2079\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.9420 - val_loss: 6.2227\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 4.9904 - val_loss: 6.2172\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9233 - val_loss: 6.2152\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 4.9177 - val_loss: 6.2144\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.9374 - val_loss: 6.2137\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9393 - val_loss: 6.2136\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.9622 - val_loss: 6.2138\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.9289 - val_loss: 6.2139\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9640 - val_loss: 6.2150\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9179 - val_loss: 6.2144\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.9351 - val_loss: 6.2146\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 1692.6297 - val_loss: 2022.1311\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 928.0585 - val_loss: 1855.6617\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 893.4386 - val_loss: 1929.4481\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 910.4948 - val_loss: 1851.7786\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 890.3654 - val_loss: 2030.4220\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 859.0141 - val_loss: 1741.6520\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 916.3015 - val_loss: 1967.6681\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 930.0079 - val_loss: 1957.1566\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 866.2189 - val_loss: 2053.0771\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 902.8032 - val_loss: 1841.4501\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 879.4946 - val_loss: 1954.0963\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 911.0351 - val_loss: 1952.3291\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 933.5117 - val_loss: 1753.0597\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 945.0199 - val_loss: 2145.9270\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 889.8952 - val_loss: 1840.9930\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 854.1854 - val_loss: 1926.5764\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 888.0358 - val_loss: 1866.6108\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 873.0124 - val_loss: 1831.2511\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 868.0713 - val_loss: 1850.9376\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 853.8957 - val_loss: 1830.1749\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 857.1842 - val_loss: 1875.5128\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 855.3016 - val_loss: 1816.7844\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 880.7023 - val_loss: 1864.5599\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 870.7282 - val_loss: 1852.1617\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 896.0573 - val_loss: 1840.9324\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 869.0783 - val_loss: 1816.1705\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 858.3166 - val_loss: 1828.8219\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 869.7390 - val_loss: 1834.7267\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 860.0432 - val_loss: 1839.5436\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 862.2396 - val_loss: 1843.6394\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 840.6928 - val_loss: 1841.8435\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 857.8146 - val_loss: 1839.5964\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 820.5837 - val_loss: 1840.6699\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 828.1903 - val_loss: 1839.8428\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 880.9252 - val_loss: 1841.9897\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 832.1728 - val_loss: 1843.8988\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 855.2106 - val_loss: 1843.3960\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 820.3199 - val_loss: 1843.1437\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 848.7365 - val_loss: 1842.9170\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 850.5935 - val_loss: 1842.9360\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 842.3415 - val_loss: 1843.0577\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 851.3389 - val_loss: 1843.0876\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 827.7645 - val_loss: 1842.8331\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 833.2528 - val_loss: 1842.8009\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 881.5175 - val_loss: 1843.0817\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 824.1791 - val_loss: 1843.0853\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 876.4171 - val_loss: 1843.0813\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 887.4632 - val_loss: 1843.0615\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 881.0829 - val_loss: 1843.0658\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 839.3498 - val_loss: 1843.0558\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 4.0543 - val_loss: 2.6493\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1862 - val_loss: 2.5384\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1216 - val_loss: 2.4885\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1119 - val_loss: 2.4708\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0975 - val_loss: 2.5105\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0869 - val_loss: 2.4930\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.0788 - val_loss: 2.4924\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2271 - val_loss: 2.5075\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0966 - val_loss: 2.4776\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0860 - val_loss: 2.4981\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0796 - val_loss: 2.4741\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0761 - val_loss: 2.4832\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0996 - val_loss: 2.9611\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1147 - val_loss: 2.4914\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0534 - val_loss: 2.4615\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0543 - val_loss: 2.4615\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0513 - val_loss: 2.4631\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0498 - val_loss: 2.4600\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0469 - val_loss: 2.4533\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0529 - val_loss: 2.4578\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0497 - val_loss: 2.4531\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.0437 - val_loss: 2.4528\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.0407 - val_loss: 2.4516\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0433 - val_loss: 2.4524\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0383 - val_loss: 2.4547\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0397 - val_loss: 2.4499\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0355 - val_loss: 2.4481\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0396 - val_loss: 2.4481\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0362 - val_loss: 2.4559\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0411 - val_loss: 2.4533\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0329 - val_loss: 2.4502\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0384 - val_loss: 2.4463\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0350 - val_loss: 2.4437\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0326 - val_loss: 2.4549\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0306 - val_loss: 2.4550\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0288 - val_loss: 2.4447\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0412 - val_loss: 2.4491\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.0343 - val_loss: 2.4447\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0284 - val_loss: 2.4405\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.0280 - val_loss: 2.4379\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0279 - val_loss: 2.4498\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0335 - val_loss: 2.4504\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0329 - val_loss: 2.4422\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0303 - val_loss: 2.4412\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0242 - val_loss: 2.4489\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0310 - val_loss: 2.4458\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0239 - val_loss: 2.4401\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0253 - val_loss: 2.4395\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0272 - val_loss: 2.4492\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0230 - val_loss: 2.4434\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3035.5140 - val_loss: 2331.3062\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 1019.8426 - val_loss: 2075.0403\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 1008.3738 - val_loss: 2028.9148\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 880.3176 - val_loss: 1874.6729\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 881.9073 - val_loss: 1817.5378\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 915.1284 - val_loss: 1999.9059\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 888.8387 - val_loss: 1820.3103\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 854.3233 - val_loss: 1770.2186\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 867.1233 - val_loss: 1777.4513\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 835.6925 - val_loss: 1913.7250\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 865.0458 - val_loss: 1852.1445\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 858.6901 - val_loss: 1888.0953\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 850.0958 - val_loss: 1820.7249\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 850.3404 - val_loss: 1861.8723\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 855.3273 - val_loss: 1857.8573\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 816.0650 - val_loss: 1896.4099\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 870.4069 - val_loss: 1878.6462\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 895.6520 - val_loss: 1867.0090\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 922.7744 - val_loss: 1869.6501\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 871.3836 - val_loss: 1860.3514\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 890.7182 - val_loss: 1843.9746\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 864.6582 - val_loss: 1859.8036\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 818.4758 - val_loss: 1841.3789\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 828.1577 - val_loss: 1845.3616\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 802.0667 - val_loss: 1836.1674\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 864.8969 - val_loss: 1855.8174\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 933.1239 - val_loss: 1841.3486\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 826.2260 - val_loss: 1823.8596\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 803.1431 - val_loss: 1829.6227\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 871.4942 - val_loss: 1833.4518\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 823.5025 - val_loss: 1836.2773\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 868.2471 - val_loss: 1839.3414\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 891.4699 - val_loss: 1841.3922\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 850.8238 - val_loss: 1842.5221\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 860.2587 - val_loss: 1842.6635\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 875.8102 - val_loss: 1844.1560\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 874.7117 - val_loss: 1843.1240\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 845.0741 - val_loss: 1843.1749\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 851.8323 - val_loss: 1843.1903\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 827.3818 - val_loss: 1843.2255\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 883.7198 - val_loss: 1843.2578\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 892.4976 - val_loss: 1843.2797\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 891.4734 - val_loss: 1843.3398\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 863.1399 - val_loss: 1843.3743\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 890.9459 - val_loss: 1843.4127\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 811.3100 - val_loss: 1843.3412\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 901.8729 - val_loss: 1843.4443\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 848.4961 - val_loss: 1843.4362\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 843.5523 - val_loss: 1843.4360\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 812.7993 - val_loss: 1843.4452\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.2257 - val_loss: 4.9367\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.1926 - val_loss: 4.9252\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1473 - val_loss: 4.8955\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1310 - val_loss: 4.8516\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0594 - val_loss: 4.8302\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0653 - val_loss: 4.8337\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0529 - val_loss: 4.8239\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0467 - val_loss: 4.8289\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0182 - val_loss: 4.8071\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0568 - val_loss: 4.8042\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0578 - val_loss: 4.8052\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0431 - val_loss: 4.8034\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0281 - val_loss: 4.7990\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0213 - val_loss: 4.8056\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0115 - val_loss: 4.7973\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0447 - val_loss: 4.8060\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0116 - val_loss: 4.7954\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0334 - val_loss: 4.7987\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0364 - val_loss: 4.7956\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0366 - val_loss: 4.7991\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0340 - val_loss: 4.7942\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0332 - val_loss: 4.7940\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0390 - val_loss: 4.7932\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0142 - val_loss: 4.7957\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0431 - val_loss: 4.7940\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0278 - val_loss: 4.7907\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0131 - val_loss: 4.7936\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0249 - val_loss: 4.8006\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0418 - val_loss: 4.7964\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0222 - val_loss: 4.7917\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0233 - val_loss: 4.7905\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9986 - val_loss: 4.7951\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0466 - val_loss: 4.7927\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0211 - val_loss: 4.7905\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9958 - val_loss: 4.7904\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9948 - val_loss: 4.7893\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0531 - val_loss: 4.7887\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0286 - val_loss: 4.8022\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0144 - val_loss: 4.7891\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0121 - val_loss: 4.7895\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0325 - val_loss: 4.7901\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0037 - val_loss: 4.7916\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0130 - val_loss: 4.7880\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0203 - val_loss: 4.7898\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0003 - val_loss: 4.7920\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0129 - val_loss: 4.7904\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0081 - val_loss: 4.7899\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0185 - val_loss: 4.7872\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9950 - val_loss: 4.7943\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0343 - val_loss: 4.7863\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7490.8386 - val_loss: 15292.8438\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7625.8120 - val_loss: 15292.7441\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7677.6895 - val_loss: 15292.7275\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7494.1766 - val_loss: 15292.7158\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7616.4479 - val_loss: 15292.7012\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7863.6205 - val_loss: 15292.6797\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7898.4139 - val_loss: 15292.6484\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7631.8827 - val_loss: 15292.6396\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7592.7608 - val_loss: 15292.6152\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7752.3081 - val_loss: 15292.6113\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7841.1758 - val_loss: 15292.5908\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7486.9260 - val_loss: 15292.5938\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7825.1148 - val_loss: 15292.5957\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7777.5477 - val_loss: 15292.5850\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7950.0076 - val_loss: 15292.5820\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8077.4953 - val_loss: 15292.5781\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 7842.4019 - val_loss: 15292.5713\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7613.0009 - val_loss: 15292.5674\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7535.4614 - val_loss: 15292.5732\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7733.8584 - val_loss: 15292.5615\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7303.9637 - val_loss: 15292.5615\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7757.4035 - val_loss: 15292.5566\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8128.9643 - val_loss: 15292.5615\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8016.3860 - val_loss: 15292.5576\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7539.1821 - val_loss: 15292.5625\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7747.8260 - val_loss: 15292.5518\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7731.4675 - val_loss: 15292.5488\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7796.0969 - val_loss: 15292.5498\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7589.8126 - val_loss: 15292.5488\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7459.6411 - val_loss: 15292.5439\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7854.5473 - val_loss: 15292.5449\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7711.7222 - val_loss: 15292.5439\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7717.7100 - val_loss: 15292.5498\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7530.7153 - val_loss: 15292.5439\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7659.6258 - val_loss: 15292.5430\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8003.8555 - val_loss: 15292.5391\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7623.6459 - val_loss: 15292.5391\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7532.6716 - val_loss: 15292.5381\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7600.4943 - val_loss: 15292.5430\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7601.8969 - val_loss: 15292.5381\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7527.9473 - val_loss: 15292.5352\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7878.7623 - val_loss: 15292.5381\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7575.5655 - val_loss: 15292.5352\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7460.9153 - val_loss: 15292.5332\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7664.0546 - val_loss: 15292.5332\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7417.2464 - val_loss: 15292.5332\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7686.1250 - val_loss: 15292.5332\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7161.2075 - val_loss: 15292.5361\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7597.6869 - val_loss: 15292.5332\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7854.1521 - val_loss: 15292.5332\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.3150 - val_loss: 4.9325\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1552 - val_loss: 4.9002\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1191 - val_loss: 4.8961\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0791 - val_loss: 4.8697\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0968 - val_loss: 4.8541\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0425 - val_loss: 4.8458\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0954 - val_loss: 4.8311\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0513 - val_loss: 4.8240\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0399 - val_loss: 4.8166\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0215 - val_loss: 4.8085\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0542 - val_loss: 4.8037\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0305 - val_loss: 4.7993\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0222 - val_loss: 4.8016\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0416 - val_loss: 4.7927\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0316 - val_loss: 4.7866\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0253 - val_loss: 4.7911\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9999 - val_loss: 4.7806\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9916 - val_loss: 4.7780\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9747 - val_loss: 4.7866\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9921 - val_loss: 4.7744\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0268 - val_loss: 4.7813\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9954 - val_loss: 4.7735\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9935 - val_loss: 4.7664\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9765 - val_loss: 4.7654\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9808 - val_loss: 4.7636\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9915 - val_loss: 4.7636\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9953 - val_loss: 4.7625\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9630 - val_loss: 4.7636\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0023 - val_loss: 4.7593\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9911 - val_loss: 4.7563\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9872 - val_loss: 4.7614\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9762 - val_loss: 4.7554\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9903 - val_loss: 4.7563\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9459 - val_loss: 4.7526\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9887 - val_loss: 4.7551\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9807 - val_loss: 4.7536\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9651 - val_loss: 4.7514\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9861 - val_loss: 4.7520\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9771 - val_loss: 4.7502\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9795 - val_loss: 4.7484\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9787 - val_loss: 4.7464\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9807 - val_loss: 4.7530\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9652 - val_loss: 4.7462\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9726 - val_loss: 4.7453\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9754 - val_loss: 4.7460\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9654 - val_loss: 4.7526\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9722 - val_loss: 4.7459\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9767 - val_loss: 4.7432\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9948 - val_loss: 4.7439\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0033 - val_loss: 4.8185\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 8135.6297 - val_loss: 15292.9668\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7794.9496 - val_loss: 15292.9082\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7887.7075 - val_loss: 15292.8428\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7523.1528 - val_loss: 15292.8389\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7775.7039 - val_loss: 15292.7695\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7925.5464 - val_loss: 15292.7490\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7893.8969 - val_loss: 15292.7324\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7687.0926 - val_loss: 15292.7090\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7512.6207 - val_loss: 15292.6855\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7861.2273 - val_loss: 15292.6699\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7630.4606 - val_loss: 15292.6572\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7904.2583 - val_loss: 15292.6582\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7602.0390 - val_loss: 15292.6367\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7942.6438 - val_loss: 15292.6260\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7435.4801 - val_loss: 15292.6221\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7745.3934 - val_loss: 15292.6211\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7618.2271 - val_loss: 15292.6016\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7708.6667 - val_loss: 15292.5996\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7609.8379 - val_loss: 15292.5908\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7713.0898 - val_loss: 15292.5908\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7753.6183 - val_loss: 15292.5811\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7483.0759 - val_loss: 15292.5859\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7459.9480 - val_loss: 15292.5713\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7713.2325 - val_loss: 15292.5742\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7738.1634 - val_loss: 15292.5771\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7993.1138 - val_loss: 15292.5713\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7681.7990 - val_loss: 15292.5596\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7475.8308 - val_loss: 15292.5654\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7591.3480 - val_loss: 15292.5547\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7727.0949 - val_loss: 15292.5615\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7596.4910 - val_loss: 15292.5557\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7735.9131 - val_loss: 15292.5488\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7729.0030 - val_loss: 15292.5488\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7386.4975 - val_loss: 15292.5479\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7628.2391 - val_loss: 15292.5400\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7667.5600 - val_loss: 15292.5391\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7554.5901 - val_loss: 15292.5439\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7763.0545 - val_loss: 15292.5361\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7272.8593 - val_loss: 15292.5352\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7767.2274 - val_loss: 15292.5303\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7763.3776 - val_loss: 15292.5303\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7544.9986 - val_loss: 15292.5254\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7598.5014 - val_loss: 15292.5225\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7403.4027 - val_loss: 15292.5244\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7358.4216 - val_loss: 15292.5283\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7539.6050 - val_loss: 15292.5166\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7860.4473 - val_loss: 15292.5166\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7515.2655 - val_loss: 15292.5195\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7904.0921 - val_loss: 15292.5264\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7345.7525 - val_loss: 15292.5215\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 28493.0488\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 14352.2624 - val_loss: 26011.5918\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 13316.8565 - val_loss: 24579.4785\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 12377.8165 - val_loss: 23587.2539\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 12298.8008 - val_loss: 22841.6660\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11411.1603 - val_loss: 22255.5762\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11156.7755 - val_loss: 21780.8750\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 10951.3541 - val_loss: 21387.8145\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 10901.5709 - val_loss: 21056.0254\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11148.0245 - val_loss: 20771.3770\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 28ms/step - loss: 19.0517 - val_loss: 8.3630\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.9170 - val_loss: 15.5201\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.1450 - val_loss: 2.8580\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.7255 - val_loss: 39.0868\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 8.2355 - val_loss: 3.3789\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8.5622 - val_loss: 2.8399\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.8839 - val_loss: 3.3209\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.0974 - val_loss: 2.9958\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 5.4208 - val_loss: 15.8432\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7.5332 - val_loss: 3.1103\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.0003 - val_loss: 7.9553\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.0536 - val_loss: 3.1446\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.1580 - val_loss: 18.2735\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 6.1494 - val_loss: 25.1539\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.1253 - val_loss: 3.1642\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.3669 - val_loss: 19.5147\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.9602 - val_loss: 2.7569\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.2830 - val_loss: 2.5679\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1886 - val_loss: 2.5027\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1725 - val_loss: 2.5337\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1775 - val_loss: 2.4806\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1372 - val_loss: 2.4964\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1455 - val_loss: 2.4937\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1211 - val_loss: 2.5384\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.1164 - val_loss: 2.5388\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1289 - val_loss: 2.5140\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1372 - val_loss: 2.4753\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1155 - val_loss: 2.4919\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1315 - val_loss: 2.5087\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1239 - val_loss: 2.5026\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1095 - val_loss: 2.5003\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1144 - val_loss: 2.5196\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 2.1110 - val_loss: 2.4779\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1074 - val_loss: 2.4838\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1162 - val_loss: 2.5648\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1101 - val_loss: 2.5412\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1204 - val_loss: 2.5357\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0700 - val_loss: 2.4714\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0572 - val_loss: 2.4602\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0607 - val_loss: 2.4627\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0612 - val_loss: 2.4648\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0569 - val_loss: 2.4647\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0538 - val_loss: 2.4697\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0564 - val_loss: 2.4634\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0565 - val_loss: 2.4595\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0536 - val_loss: 2.4632\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0621 - val_loss: 2.4605\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0522 - val_loss: 2.4663\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0576 - val_loss: 2.4692\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0567 - val_loss: 2.4604\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2177.4548 - val_loss: 2035.5452\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 1036.6633 - val_loss: 1949.3950\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 920.4285 - val_loss: 2090.2781\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.5946 - val_loss: 2166.2869\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 895.0293 - val_loss: 2050.3557\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 900.2276 - val_loss: 1909.6438\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.5614 - val_loss: 1863.2023\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 902.4959 - val_loss: 1791.2189\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 862.8058 - val_loss: 1874.0872\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 879.9367 - val_loss: 1941.8153\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 884.4368 - val_loss: 1839.2921\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 864.3044 - val_loss: 1843.9095\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.0172 - val_loss: 1903.8336\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 922.5113 - val_loss: 1834.3199\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 900.2322 - val_loss: 2442.6711\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 906.0618 - val_loss: 1842.0905\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 857.9400 - val_loss: 1887.0222\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 912.4592 - val_loss: 1871.4825\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 825.0863 - val_loss: 1842.0266\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 866.6945 - val_loss: 1854.8324\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 901.6647 - val_loss: 1846.8472\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 808.7109 - val_loss: 1807.3763\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 876.3186 - val_loss: 1840.4139\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 800.5970 - val_loss: 1818.4060\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 820.7201 - val_loss: 1818.8123\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 842.8327 - val_loss: 1831.8918\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 886.6800 - val_loss: 1843.4933\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 818.9616 - val_loss: 1826.8121\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 877.1845 - val_loss: 1829.6740\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 869.7573 - val_loss: 1832.5731\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 823.0792 - val_loss: 1834.7825\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 871.0855 - val_loss: 1836.1206\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.7954 - val_loss: 1835.7766\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 825.9338 - val_loss: 1836.5516\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 842.9824 - val_loss: 1839.4038\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 858.8009 - val_loss: 1838.0784\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 859.2001 - val_loss: 1840.1318\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 863.3883 - val_loss: 1840.8148\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 814.3228 - val_loss: 1841.2084\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 853.5378 - val_loss: 1841.0309\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 818.6988 - val_loss: 1841.1996\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 833.2549 - val_loss: 1841.2672\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 856.7360 - val_loss: 1841.6122\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 907.3095 - val_loss: 1841.4379\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 866.3853 - val_loss: 1841.5854\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 834.7532 - val_loss: 1841.5225\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 884.0512 - val_loss: 1841.6882\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 883.9370 - val_loss: 1841.5922\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 864.2715 - val_loss: 1841.5819\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 859.0513 - val_loss: 1841.5676\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.0973 - val_loss: 2.5919\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.2045 - val_loss: 2.5246\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1072 - val_loss: 2.4886\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0864 - val_loss: 2.5032\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0914 - val_loss: 2.4907\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0953 - val_loss: 2.4797\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0800 - val_loss: 2.5123\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0874 - val_loss: 2.4949\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0793 - val_loss: 2.4787\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0889 - val_loss: 2.4819\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0981 - val_loss: 2.5617\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1019 - val_loss: 2.4811\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0864 - val_loss: 2.5391\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0896 - val_loss: 2.4946\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1127 - val_loss: 2.4865\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1015 - val_loss: 2.4908\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0961 - val_loss: 2.4895\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0922 - val_loss: 2.4911\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0949 - val_loss: 2.7524\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0960 - val_loss: 2.4656\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0666 - val_loss: 2.4645\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0618 - val_loss: 2.4657\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0583 - val_loss: 2.4646\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0672 - val_loss: 2.4701\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0652 - val_loss: 2.4658\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0658 - val_loss: 2.4687\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0651 - val_loss: 2.4639\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0665 - val_loss: 2.4705\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0679 - val_loss: 2.4705\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0571 - val_loss: 2.4650\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0688 - val_loss: 2.4670\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0583 - val_loss: 2.4655\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0591 - val_loss: 2.4639\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0673 - val_loss: 2.4684\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0587 - val_loss: 2.4671\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0633 - val_loss: 2.4683\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0666 - val_loss: 2.4662\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0609 - val_loss: 2.4649\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0580 - val_loss: 2.4643\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0545 - val_loss: 2.4641\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0557 - val_loss: 2.4648\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0512 - val_loss: 2.4644\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0613 - val_loss: 2.4651\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0633 - val_loss: 2.4654\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0591 - val_loss: 2.4655\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0643 - val_loss: 2.4650\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0606 - val_loss: 2.4644\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0597 - val_loss: 2.4646\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0556 - val_loss: 2.4647\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0615 - val_loss: 2.4648\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2601.0266 - val_loss: 2395.5657\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 1061.0358 - val_loss: 2069.7524\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 923.0097 - val_loss: 2099.2971\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 928.8665 - val_loss: 1943.1646\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 888.7579 - val_loss: 1793.6526\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 854.0590 - val_loss: 1840.7349\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 855.5522 - val_loss: 1862.4766\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 853.0191 - val_loss: 1974.5195\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 872.3358 - val_loss: 2011.5184\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 847.5404 - val_loss: 1859.5999\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 887.7726 - val_loss: 2025.8900\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 896.0114 - val_loss: 2003.1652\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 872.3462 - val_loss: 1772.9125\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 880.7535 - val_loss: 1779.0500\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 859.7527 - val_loss: 1884.6931\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 863.6229 - val_loss: 1821.4446\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 847.9272 - val_loss: 1871.2800\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 858.8293 - val_loss: 1843.4258\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 845.2646 - val_loss: 1843.6992\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 894.6891 - val_loss: 1722.0026\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 885.3552 - val_loss: 1879.9479\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 875.3263 - val_loss: 1873.5333\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 869.0043 - val_loss: 1804.6101\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 897.0477 - val_loss: 1878.3282\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 844.1692 - val_loss: 1860.1201\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 895.4445 - val_loss: 1872.9988\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.3383 - val_loss: 1809.7510\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.7406 - val_loss: 1813.7828\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.6705 - val_loss: 1862.8828\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 866.0450 - val_loss: 1884.2269\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 875.5911 - val_loss: 1855.2584\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 873.7028 - val_loss: 1846.6058\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 843.1752 - val_loss: 1834.2308\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 873.3041 - val_loss: 1855.2041\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.9272 - val_loss: 1843.2159\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 848.4401 - val_loss: 1846.2330\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.0308 - val_loss: 1837.3907\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 835.3899 - val_loss: 1838.0394\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 884.4079 - val_loss: 1850.8722\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 842.6791 - val_loss: 1840.5442\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 811.4702 - val_loss: 1840.1686\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 851.5170 - val_loss: 1842.0232\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 890.3925 - val_loss: 1843.6605\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 892.2026 - val_loss: 1843.3093\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 850.5620 - val_loss: 1842.9755\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 865.5715 - val_loss: 1843.3834\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 792.3778 - val_loss: 1844.3016\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 829.9364 - val_loss: 1843.7095\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 901.5288 - val_loss: 1844.7289\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 922.9734 - val_loss: 1845.1492\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9500 - val_loss: 4.7006\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9333 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9311 - val_loss: 4.7009\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9212 - val_loss: 4.7016\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: 3.9377 - val_loss: 4.7006\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9213 - val_loss: 4.7007\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9233 - val_loss: 4.7019\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9487 - val_loss: 4.7007\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9179 - val_loss: 4.7007\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9371 - val_loss: 4.7007\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9273 - val_loss: 4.7007\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9241 - val_loss: 4.7008\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9234 - val_loss: 4.7009\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9391 - val_loss: 4.7011\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9566 - val_loss: 4.7011\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9273 - val_loss: 4.7011\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9310 - val_loss: 4.7009\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9483 - val_loss: 4.7011\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9407 - val_loss: 4.7011\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9505 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9112 - val_loss: 4.7009\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9510 - val_loss: 4.7009\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9172 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9451 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9156 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9385 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9251 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9202 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9282 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9385 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9326 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9319 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9289 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9087 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9101 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9510 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9307 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9316 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9402 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9224 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9332 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9416 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9366 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9305 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9226 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9449 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9145 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9362 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8991 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9337 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7857.6653 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7803.7695 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7690.6655 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7971.3117 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7528.2848 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7723.4581 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7588.8918 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7627.4633 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7587.6816 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7885.1767 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7942.7755 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7731.6182 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7442.8875 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7824.1124 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7647.6726 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7605.9655 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7435.5767 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7822.6663 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7658.5534 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7948.3962 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7729.0230 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: 7524.8641 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7527.4020 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7718.5201 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7556.2243 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7693.4451 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7687.3292 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8015.6340 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7822.0833 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7802.6140 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7232.8567 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7330.9167 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7504.1212 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7591.7914 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7551.0598 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7034.3598 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7509.4448 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7852.0309 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8037.1046 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8058.9311 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7535.9204 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7678.5081 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7802.8790 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7812.6231 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7791.7345 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7930.3818 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7935.5708 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7309.7707 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7826.8466 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7833.7178 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 4.0778 - val_loss: 4.7010\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9292 - val_loss: 4.7013\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9294 - val_loss: 4.7013\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9252 - val_loss: 4.7008\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9261 - val_loss: 4.7008\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9187 - val_loss: 4.7009\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9107 - val_loss: 4.7015\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.8909 - val_loss: 4.7009\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9376 - val_loss: 4.7009\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9395 - val_loss: 4.7009\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9091 - val_loss: 4.7009\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9222 - val_loss: 4.7008\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9347 - val_loss: 4.7011\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9218 - val_loss: 4.7007\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9507 - val_loss: 4.7008\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9217 - val_loss: 4.7010\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9252 - val_loss: 4.7010\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9158 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9195 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9185 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9463 - val_loss: 4.7011\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9270 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9474 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9224 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9517 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9523 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9207 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9307 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9003 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9235 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9379 - val_loss: 4.7010\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9325 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9111 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9153 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9571 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9246 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9414 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9578 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9296 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9272 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9379 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9316 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9449 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9071 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9433 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9393 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9341 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9374 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9166 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8986 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7744.8403 - val_loss: 15292.4385\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7535.7373 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7907.9913 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7923.5053 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7582.6761 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7392.8436 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7639.7450 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7818.5103 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7536.7384 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: 7773.8725 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7289.2872 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7656.6997 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7494.1832 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7821.3403 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7330.0818 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7532.3996 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7859.1287 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7206.3783 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7422.6091 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7792.0894 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7380.1190 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7800.3648 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7478.5879 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7418.1144 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7472.2181 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7991.2337 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7525.4985 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7933.6853 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7615.6169 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7665.3947 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7314.2697 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7519.5476 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7652.3735 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7755.5483 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7497.9811 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7849.5511 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7430.3689 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7429.1603 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7874.1835 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7939.9613 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7729.7333 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7891.4351 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7619.0555 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7184.6510 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7289.1574 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7914.8541 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7743.7932 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7437.9725 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7804.6817 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7662.0785 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 19.6154 - val_loss: 3.8125\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.3474 - val_loss: 3.0223\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9948 - val_loss: 7.5881\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 6.5702 - val_loss: 11.3431\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 11.8410 - val_loss: 10.5452\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 14.0669 - val_loss: 55.6517\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 20.4745 - val_loss: 14.0742\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 20.7946 - val_loss: 39.2101\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 22.0945 - val_loss: 25.8482\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 22.1212 - val_loss: 9.1002\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 21.3441 - val_loss: 9.9430\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 23.7070 - val_loss: 44.0466\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 13.4955 - val_loss: 8.7837\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7.3605 - val_loss: 8.5616\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.6356 - val_loss: 7.4131\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.5795 - val_loss: 6.1395\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.4953 - val_loss: 7.2417\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.6832 - val_loss: 10.0957\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.3296 - val_loss: 6.9389\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.6136 - val_loss: 6.1249\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.4589 - val_loss: 6.1744\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.1056 - val_loss: 5.8570\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.7755 - val_loss: 5.8840\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.5362 - val_loss: 6.7816\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.4805 - val_loss: 6.2609\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.4126 - val_loss: 6.2977\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.4539 - val_loss: 5.9027\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 4.5084 - val_loss: 6.7346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.4328 - val_loss: 5.8925\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.4838 - val_loss: 5.7175\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.5088 - val_loss: 6.3619\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.4009 - val_loss: 5.9535\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.4435 - val_loss: 6.1480\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.4721 - val_loss: 6.1839\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.4539 - val_loss: 6.3797\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.4788 - val_loss: 6.3504\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.5185 - val_loss: 6.4800\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.4620 - val_loss: 6.4469\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.4999 - val_loss: 6.6178\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.4596 - val_loss: 6.3757\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.4839 - val_loss: 6.5627\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.5013 - val_loss: 6.5120\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.4652 - val_loss: 6.5326\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 4.5017 - val_loss: 6.5358\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.4267 - val_loss: 6.5409\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.4981 - val_loss: 6.5414\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.4588 - val_loss: 6.5330\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.5125 - val_loss: 6.5387\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 4.4969 - val_loss: 6.5437\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.4092 - val_loss: 6.5360\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 1314.0290 - val_loss: 2300.0176\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 964.6941 - val_loss: 2155.4099\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 924.4002 - val_loss: 2036.4609\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 871.1489 - val_loss: 1877.4484\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 869.7642 - val_loss: 1911.7372\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 897.8983 - val_loss: 1944.5228\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 925.8306 - val_loss: 2014.4628\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 875.8781 - val_loss: 1713.5591\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 896.9496 - val_loss: 2015.2607\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 825.2305 - val_loss: 1782.2640\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 916.3381 - val_loss: 1855.0187\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 929.1672 - val_loss: 1841.6049\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 880.2246 - val_loss: 1860.4696\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 896.2826 - val_loss: 1937.0814\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 894.3607 - val_loss: 1776.0439\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 884.8974 - val_loss: 1876.4567\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 884.3367 - val_loss: 1837.2371\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 898.4092 - val_loss: 1838.1301\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 869.2072 - val_loss: 1866.1344\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 830.8280 - val_loss: 1852.2782\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 831.5208 - val_loss: 1829.7712\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 851.3097 - val_loss: 1852.9609\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 833.7322 - val_loss: 1834.5682\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 836.1754 - val_loss: 1840.0868\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 918.2361 - val_loss: 1846.3199\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 843.0605 - val_loss: 1828.4100\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 860.3660 - val_loss: 1832.2671\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 809.3322 - val_loss: 1811.7438\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 826.1380 - val_loss: 1821.3877\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 924.4965 - val_loss: 1828.5477\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 863.5003 - val_loss: 1833.6262\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 842.4100 - val_loss: 1833.8822\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 812.2880 - val_loss: 1834.8082\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 875.1519 - val_loss: 1840.5737\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 820.1992 - val_loss: 1837.8168\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 815.4500 - val_loss: 1837.4089\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 867.5264 - val_loss: 1840.4111\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 904.6499 - val_loss: 1841.3514\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 852.3720 - val_loss: 1841.2098\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 907.2154 - val_loss: 1841.1135\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 816.5937 - val_loss: 1841.0618\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 854.7647 - val_loss: 1840.9607\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 870.7591 - val_loss: 1840.8358\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 855.6742 - val_loss: 1840.8076\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 828.9898 - val_loss: 1840.9203\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 868.3521 - val_loss: 1840.8948\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 843.1316 - val_loss: 1840.8678\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 895.5059 - val_loss: 1841.0850\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 882.2966 - val_loss: 1841.0739\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 844.3223 - val_loss: 1841.0745\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 4.0607 - val_loss: 2.8764\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3441 - val_loss: 2.8757\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3009 - val_loss: 2.7020\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2792 - val_loss: 2.6484\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2480 - val_loss: 2.6362\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2521 - val_loss: 2.6294\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2323 - val_loss: 2.6575\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2410 - val_loss: 2.6656\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1985 - val_loss: 2.5662\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1972 - val_loss: 2.6255\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.1912 - val_loss: 2.5674\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.2111 - val_loss: 2.6030\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1854 - val_loss: 2.5659\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.2220 - val_loss: 2.5834\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.2965 - val_loss: 3.2699\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2406 - val_loss: 2.6896\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2334 - val_loss: 2.5421\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.3901 - val_loss: 2.5534\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.3240 - val_loss: 2.5549\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.2956 - val_loss: 2.5453\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.3694 - val_loss: 3.3811\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.2084 - val_loss: 2.5192\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3466 - val_loss: 2.5059\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.3328 - val_loss: 2.5197\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2654 - val_loss: 2.5567\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3127 - val_loss: 2.5294\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1652 - val_loss: 2.5631\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.5461 - val_loss: 2.5254\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4075 - val_loss: 2.5035\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3431 - val_loss: 2.5248\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2983 - val_loss: 2.5218\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4473 - val_loss: 2.4992\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2532 - val_loss: 2.6181\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2313 - val_loss: 2.5187\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4144 - val_loss: 2.5210\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.2622 - val_loss: 7.4200\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6453 - val_loss: 2.5905\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3693 - val_loss: 2.5148\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2602 - val_loss: 2.5223\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5099 - val_loss: 2.4970\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4323 - val_loss: 2.4931\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5667 - val_loss: 2.4785\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4362 - val_loss: 2.5032\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.7289 - val_loss: 2.6011\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.3645 - val_loss: 2.5872\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4298 - val_loss: 2.4865\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4753 - val_loss: 16.5135\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.6356 - val_loss: 2.4712\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3935 - val_loss: 2.7387\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.1014 - val_loss: 2.5014\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3399.4276 - val_loss: 2387.7407\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 1072.8022 - val_loss: 2021.1075\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 968.7790 - val_loss: 1918.4949\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 875.6082 - val_loss: 1997.9077\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 867.7288 - val_loss: 1928.5964\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 883.1973 - val_loss: 1845.5077\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 861.8037 - val_loss: 1894.8253\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 872.8618 - val_loss: 1884.0913\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 882.0157 - val_loss: 1805.2583\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 870.6305 - val_loss: 1847.1180\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 873.1535 - val_loss: 1859.3193\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 852.3357 - val_loss: 1961.4927\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 877.1332 - val_loss: 1869.2040\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 861.6971 - val_loss: 1804.6780\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 826.2513 - val_loss: 1858.3333\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 848.8561 - val_loss: 1889.0007\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 835.5235 - val_loss: 1777.8191\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 879.7263 - val_loss: 1822.5969\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 862.4023 - val_loss: 1864.0519\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 866.5059 - val_loss: 1917.0027\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 864.4962 - val_loss: 1803.5100\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 875.4912 - val_loss: 1799.6918\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 875.4830 - val_loss: 1873.3103\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 843.1337 - val_loss: 1995.3124\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 874.0915 - val_loss: 2017.4229\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 830.4324 - val_loss: 1772.8593\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 874.5086 - val_loss: 1894.3290\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 809.7864 - val_loss: 1711.2366\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 825.5596 - val_loss: 1802.0900\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 891.9103 - val_loss: 1796.6521\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 819.3627 - val_loss: 1794.4116\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 929.1459 - val_loss: 1912.7764\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 823.9336 - val_loss: 1786.3378\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 883.1334 - val_loss: 1866.5769\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 854.9733 - val_loss: 1867.2500\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 827.9980 - val_loss: 1908.7361\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 894.8696 - val_loss: 1845.7821\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 902.4547 - val_loss: 1872.0177\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 865.3255 - val_loss: 1859.0385\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 857.9530 - val_loss: 1837.4703\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 855.3197 - val_loss: 1835.6699\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 823.6581 - val_loss: 1830.2068\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 854.1339 - val_loss: 1831.6324\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 860.6748 - val_loss: 1869.8823\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 870.0464 - val_loss: 1872.2426\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 841.4369 - val_loss: 1852.1600\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 831.6231 - val_loss: 1820.5248\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 843.3211 - val_loss: 1835.6178\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 875.0182 - val_loss: 1838.2722\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 840.5471 - val_loss: 1840.5463\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 4.2600 - val_loss: 4.9387\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1701 - val_loss: 4.9115\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1263 - val_loss: 4.9125\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0974 - val_loss: 4.8521\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0798 - val_loss: 4.8351\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0258 - val_loss: 4.8211\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0565 - val_loss: 4.8152\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0534 - val_loss: 4.8126\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0411 - val_loss: 4.8067\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0521 - val_loss: 4.8024\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0409 - val_loss: 4.8089\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0383 - val_loss: 4.7978\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0314 - val_loss: 4.8336\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0268 - val_loss: 4.8038\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0363 - val_loss: 4.7986\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0218 - val_loss: 4.7983\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0472 - val_loss: 4.7968\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0111 - val_loss: 4.8065\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0508 - val_loss: 4.7971\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0012 - val_loss: 4.7968\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0179 - val_loss: 4.7982\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0241 - val_loss: 4.8130\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0033 - val_loss: 4.7923\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0223 - val_loss: 4.7950\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0122 - val_loss: 4.7918\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0257 - val_loss: 4.7925\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 4.0016 - val_loss: 4.7923\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0321 - val_loss: 4.7948\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0227 - val_loss: 4.7946\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0262 - val_loss: 4.7944\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0211 - val_loss: 4.7915\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0419 - val_loss: 4.7892\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0352 - val_loss: 4.7892\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0209 - val_loss: 4.7892\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0309 - val_loss: 4.7960\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0609 - val_loss: 4.7879\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0189 - val_loss: 4.7950\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9986 - val_loss: 4.7941\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0516 - val_loss: 4.8043\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0318 - val_loss: 4.7900\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0127 - val_loss: 4.7874\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0064 - val_loss: 4.7947\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0167 - val_loss: 4.7865\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0210 - val_loss: 4.7943\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9924 - val_loss: 4.7979\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0133 - val_loss: 4.7900\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0223 - val_loss: 4.7948\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0242 - val_loss: 4.7891\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0089 - val_loss: 4.7912\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0189 - val_loss: 4.7888\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7303.5164 - val_loss: 15292.7949\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7405.4750 - val_loss: 15292.7559\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7481.4652 - val_loss: 15292.7432\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7928.3847 - val_loss: 15292.7393\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8249.6681 - val_loss: 15292.7119\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 8104.5303 - val_loss: 15292.6846\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7705.4629 - val_loss: 15292.6484\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7677.9121 - val_loss: 15292.6416\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7377.4568 - val_loss: 15292.6211\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7635.5331 - val_loss: 15292.6084\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7370.7316 - val_loss: 15292.6025\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7931.1858 - val_loss: 15292.5986\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7725.6275 - val_loss: 15292.5898\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7184.8496 - val_loss: 15292.5898\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7289.9142 - val_loss: 15292.5771\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7429.6062 - val_loss: 15292.5889\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7804.8214 - val_loss: 15292.5742\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7296.3325 - val_loss: 15292.5674\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7564.7880 - val_loss: 15292.5654\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7924.5829 - val_loss: 15292.5654\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7688.2553 - val_loss: 15292.5596\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7306.8559 - val_loss: 15292.5576\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7826.1965 - val_loss: 15292.5615\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7697.2098 - val_loss: 15292.5566\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8405.1069 - val_loss: 15292.5527\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7671.4690 - val_loss: 15292.5566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7671.3332 - val_loss: 15292.5547\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7770.0316 - val_loss: 15292.5537\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7696.6263 - val_loss: 15292.5488\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7487.7085 - val_loss: 15292.5488\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7478.9701 - val_loss: 15292.5488\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7917.3950 - val_loss: 15292.5439\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7558.2402 - val_loss: 15292.5469\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7827.1442 - val_loss: 15292.5439\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7861.6016 - val_loss: 15292.5430\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7520.9919 - val_loss: 15292.5391\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7322.4387 - val_loss: 15292.5400\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7451.1567 - val_loss: 15292.5381\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7275.6021 - val_loss: 15292.5410\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7255.5885 - val_loss: 15292.5381\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7531.5052 - val_loss: 15292.5381\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7712.8289 - val_loss: 15292.5381\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7470.8984 - val_loss: 15292.5352\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7834.6448 - val_loss: 15292.5361\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7852.3971 - val_loss: 15292.5352\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7855.5543 - val_loss: 15292.5352\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 8149.8197 - val_loss: 15292.5352\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7234.1807 - val_loss: 15292.5332\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7647.7312 - val_loss: 15292.5352\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7975.5361 - val_loss: 15292.5332\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.3024 - val_loss: 4.9273\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1396 - val_loss: 4.9010\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1040 - val_loss: 4.8794\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0902 - val_loss: 4.8634\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0790 - val_loss: 4.8507\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0897 - val_loss: 4.8398\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0535 - val_loss: 4.8333\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0458 - val_loss: 4.8218\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0454 - val_loss: 4.8183\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0511 - val_loss: 4.8206\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0150 - val_loss: 4.8027\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0467 - val_loss: 4.8000\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0304 - val_loss: 4.8013\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0107 - val_loss: 4.7951\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0136 - val_loss: 4.7890\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0061 - val_loss: 4.7923\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0011 - val_loss: 4.7814\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0393 - val_loss: 4.7857\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0469 - val_loss: 4.7757\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9985 - val_loss: 4.7740\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9978 - val_loss: 4.7703\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0053 - val_loss: 4.7713\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0099 - val_loss: 4.7732\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9709 - val_loss: 4.7657\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9724 - val_loss: 4.7649\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9777 - val_loss: 4.7636\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0054 - val_loss: 4.7620\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9940 - val_loss: 4.7615\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9709 - val_loss: 4.7602\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9796 - val_loss: 4.7785\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9635 - val_loss: 4.7561\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9706 - val_loss: 4.7693\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9638 - val_loss: 4.7543\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9926 - val_loss: 4.7538\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9686 - val_loss: 4.7531\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0024 - val_loss: 4.7529\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9989 - val_loss: 4.7733\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9696 - val_loss: 4.7501\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9743 - val_loss: 4.7490\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9638 - val_loss: 4.7494\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9961 - val_loss: 4.7623\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9789 - val_loss: 4.7476\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9937 - val_loss: 4.7464\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9527 - val_loss: 4.7479\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9611 - val_loss: 4.7582\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9610 - val_loss: 4.7444\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9615 - val_loss: 4.7474\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9753 - val_loss: 4.7497\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9776 - val_loss: 4.7508\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9684 - val_loss: 4.7467\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 7469.0022 - val_loss: 15292.9658\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7248.4320 - val_loss: 15292.9189\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7704.2226 - val_loss: 15292.8428\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 23ms/step - loss: 7685.4192 - val_loss: 15292.7998\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7770.4735 - val_loss: 15292.7705\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7518.4210 - val_loss: 15292.7500\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7743.1190 - val_loss: 15292.7324\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7791.6985 - val_loss: 15292.7002\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7557.2887 - val_loss: 15292.6846\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7496.6590 - val_loss: 15292.6699\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7773.2922 - val_loss: 15292.6602\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7272.7950 - val_loss: 15292.6484\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7722.1909 - val_loss: 15292.6377\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7591.3016 - val_loss: 15292.6221\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7830.9870 - val_loss: 15292.6172\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7940.8779 - val_loss: 15292.6172\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7410.2791 - val_loss: 15292.6055\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7491.7934 - val_loss: 15292.5938\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7633.8157 - val_loss: 15292.5938\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7475.2484 - val_loss: 15292.5859\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7974.6653 - val_loss: 15292.5811\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7861.2843 - val_loss: 15292.5762\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7768.3473 - val_loss: 15292.5713\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7822.8776 - val_loss: 15292.5674\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7735.0451 - val_loss: 15292.5664\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7759.8348 - val_loss: 15292.5723\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7649.4672 - val_loss: 15292.5635\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7790.7522 - val_loss: 15292.5586\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7888.7686 - val_loss: 15292.5635\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7545.4379 - val_loss: 15292.5566\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7429.1267 - val_loss: 15292.5469\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7686.2314 - val_loss: 15292.5430\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7607.4671 - val_loss: 15292.5410\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7927.7655 - val_loss: 15292.5391\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7860.3533 - val_loss: 15292.5391\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7099.5848 - val_loss: 15292.5381\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7325.9968 - val_loss: 15292.5352\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7704.3176 - val_loss: 15292.5391\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7497.7829 - val_loss: 15292.5381\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7333.9570 - val_loss: 15292.5371\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7701.8540 - val_loss: 15292.5283\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7874.9771 - val_loss: 15292.5273\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7690.4266 - val_loss: 15292.5244\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7585.7655 - val_loss: 15292.5449\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7357.3122 - val_loss: 15292.5215\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7140.7725 - val_loss: 15292.5244\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7793.0581 - val_loss: 15292.5205\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7865.5993 - val_loss: 15292.5166\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7608.6276 - val_loss: 15292.5332\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7430.6242 - val_loss: 15292.5244\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 30.6096\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 25.5111 - val_loss: 25.3866\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 21.4873 - val_loss: 22.4282\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 19.2213 - val_loss: 20.3949\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 17.5119 - val_loss: 18.8684\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 16.2460 - val_loss: 17.6685\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 15.1804 - val_loss: 16.6945\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 14.3448 - val_loss: 15.8860\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 13.7213 - val_loss: 15.2035\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 13.1618 - val_loss: 14.6191\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: 28494.2559\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 14307.0720 - val_loss: 26012.0664\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 13269.9229 - val_loss: 24579.9297\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 12646.0921 - val_loss: 23587.8477\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 11955.7319 - val_loss: 22841.9297\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11450.8360 - val_loss: 22256.8867\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 11022.2419 - val_loss: 21781.2305\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 11229.3199 - val_loss: 21387.6738\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 10700.0705 - val_loss: 21056.9102\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 10946.4024 - val_loss: 20773.8535\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 23.8523 - val_loss: 7.7028\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8.2576 - val_loss: 3.0778\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.3747 - val_loss: 4.0743\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.6949 - val_loss: 11.3502\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.2757 - val_loss: 2.8262\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 6.1660 - val_loss: 14.5021\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.5875 - val_loss: 8.8995\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.9109 - val_loss: 2.8605\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.1957 - val_loss: 2.9414\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.9423 - val_loss: 3.2178\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.6004 - val_loss: 3.3995\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 5.2055 - val_loss: 2.8809\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.3626 - val_loss: 3.2362\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.0837 - val_loss: 3.0559\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.5394 - val_loss: 2.8580\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.2992 - val_loss: 2.5397\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1664 - val_loss: 2.5312\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1358 - val_loss: 2.5671\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1868 - val_loss: 2.5077\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1423 - val_loss: 2.7932\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1394 - val_loss: 2.5542\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1148 - val_loss: 2.5593\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1370 - val_loss: 2.6430\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1202 - val_loss: 2.6342\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.1258 - val_loss: 2.4980\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1199 - val_loss: 2.5315\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1148 - val_loss: 2.5209\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1250 - val_loss: 2.4887\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1193 - val_loss: 2.6127\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1104 - val_loss: 2.5154\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1130 - val_loss: 2.4986\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1172 - val_loss: 2.5579\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1104 - val_loss: 2.4904\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1266 - val_loss: 2.5215\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1006 - val_loss: 2.4874\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1006 - val_loss: 2.5044\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1628 - val_loss: 2.5472\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.1121 - val_loss: 2.5215\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1119 - val_loss: 2.4929\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1006 - val_loss: 2.5199\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1169 - val_loss: 2.5005\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1077 - val_loss: 2.4978\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1069 - val_loss: 2.4846\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1001 - val_loss: 2.5269\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1086 - val_loss: 2.4934\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1025 - val_loss: 2.4859\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0966 - val_loss: 2.4857\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0973 - val_loss: 2.4909\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1106 - val_loss: 2.4781\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1026 - val_loss: 2.4976\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 1700.3307 - val_loss: 1981.1329\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 955.6152 - val_loss: 1803.4106\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 912.2080 - val_loss: 1884.5057\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 875.6538 - val_loss: 1997.4049\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 874.2067 - val_loss: 2035.5918\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 905.3650 - val_loss: 2435.2764\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 929.8175 - val_loss: 1909.2244\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 919.7860 - val_loss: 1873.6287\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 891.1539 - val_loss: 1684.8545\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 932.6436 - val_loss: 1800.7495\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 963.7959 - val_loss: 1985.6394\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 864.1528 - val_loss: 1768.2223\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 859.1250 - val_loss: 1963.7748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 884.9006 - val_loss: 1783.3398\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 879.3673 - val_loss: 1824.1921\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 909.7675 - val_loss: 1779.3650\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 861.7924 - val_loss: 1785.3033\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 862.3611 - val_loss: 1731.3254\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 927.7001 - val_loss: 1775.3303\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.2643 - val_loss: 1812.7620\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 812.5799 - val_loss: 1812.6700\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 822.5307 - val_loss: 1811.1840\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 857.8276 - val_loss: 1854.2805\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 862.0473 - val_loss: 1818.6130\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 926.3360 - val_loss: 1873.0068\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 894.6356 - val_loss: 1830.3522\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 855.4358 - val_loss: 1856.6909\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.8521 - val_loss: 1838.2100\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 838.0428 - val_loss: 1828.0551\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 843.3096 - val_loss: 1832.0964\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 874.9042 - val_loss: 1834.9446\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 877.1861 - val_loss: 1834.8232\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 883.7831 - val_loss: 1834.5833\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 874.7299 - val_loss: 1836.8361\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 829.0984 - val_loss: 1835.4150\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 895.0324 - val_loss: 1836.2991\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 831.7660 - val_loss: 1836.1550\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.6874 - val_loss: 1837.6780\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 837.2436 - val_loss: 1836.4575\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 840.8100 - val_loss: 1836.6555\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 852.7162 - val_loss: 1836.8739\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 850.0842 - val_loss: 1837.0005\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 809.6901 - val_loss: 1837.3252\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 876.0541 - val_loss: 1837.5031\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 866.8467 - val_loss: 1837.5958\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 828.3765 - val_loss: 1837.4298\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 837.3716 - val_loss: 1837.1577\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 890.6391 - val_loss: 1837.3936\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 890.4706 - val_loss: 1837.3707\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 906.4754 - val_loss: 1837.3635\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 29ms/step - loss: 3.9304 - val_loss: 2.5746\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1846 - val_loss: 2.6058\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1383 - val_loss: 2.5363\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1223 - val_loss: 2.5298\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.1024 - val_loss: 2.5035\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.1018 - val_loss: 2.5009\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1117 - val_loss: 2.5111\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1076 - val_loss: 2.4962\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1080 - val_loss: 2.4899\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1275 - val_loss: 2.5286\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1070 - val_loss: 2.5281\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1230 - val_loss: 2.5051\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1137 - val_loss: 2.4964\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1119 - val_loss: 2.5170\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1049 - val_loss: 2.7151\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1316 - val_loss: 2.5068\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1170 - val_loss: 2.6125\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1161 - val_loss: 2.5210\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1163 - val_loss: 2.5150\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0777 - val_loss: 2.4873\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0726 - val_loss: 2.4916\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0829 - val_loss: 2.4875\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0751 - val_loss: 2.4869\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0758 - val_loss: 2.4859\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0797 - val_loss: 2.4851\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0802 - val_loss: 2.4905\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0817 - val_loss: 2.4859\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0811 - val_loss: 2.4874\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0809 - val_loss: 2.4854\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0779 - val_loss: 2.4873\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0815 - val_loss: 2.4835\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0807 - val_loss: 2.4896\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0861 - val_loss: 2.4874\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0725 - val_loss: 2.4831\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0891 - val_loss: 2.4914\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0842 - val_loss: 2.4886\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0782 - val_loss: 2.4877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0785 - val_loss: 2.4848\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0856 - val_loss: 2.4863\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0727 - val_loss: 2.4816\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0757 - val_loss: 2.4875\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0828 - val_loss: 2.4820\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0788 - val_loss: 2.4846\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0746 - val_loss: 2.4836\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0769 - val_loss: 2.4865\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0724 - val_loss: 2.4828\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0793 - val_loss: 2.4841\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 2.0828 - val_loss: 2.4845\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0752 - val_loss: 2.4820\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0797 - val_loss: 2.4830\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2585.5394 - val_loss: 2373.0393\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 995.4473 - val_loss: 2037.2811\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 916.8042 - val_loss: 2082.2769\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 890.0308 - val_loss: 1853.3977\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 867.4185 - val_loss: 1989.7732\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 935.8994 - val_loss: 1922.0079\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 895.0828 - val_loss: 1831.6125\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 861.3507 - val_loss: 1952.4656\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 815.9930 - val_loss: 1840.9393\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 841.0977 - val_loss: 1877.3757\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 857.6252 - val_loss: 1886.7179\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 852.3205 - val_loss: 2003.5514\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 911.7887 - val_loss: 1785.0374\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 843.8205 - val_loss: 1832.2067\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 826.8230 - val_loss: 1850.8400\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 877.2909 - val_loss: 1822.0531\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 868.2150 - val_loss: 1786.6146\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 823.4537 - val_loss: 1837.2950\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 879.0564 - val_loss: 1875.3250\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 866.0478 - val_loss: 1879.9099\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 898.6771 - val_loss: 1829.7644\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 842.3331 - val_loss: 1852.1005\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 885.6903 - val_loss: 1920.0924\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 828.3130 - val_loss: 1830.4556\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 864.9762 - val_loss: 1852.0616\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 833.3632 - val_loss: 1835.5554\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 827.6863 - val_loss: 1824.0095\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 848.2637 - val_loss: 1850.6814\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 863.0043 - val_loss: 1837.3386\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 858.5450 - val_loss: 1839.9607\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 873.0907 - val_loss: 1854.4441\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 837.3717 - val_loss: 1830.3152\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 875.2131 - val_loss: 1824.9440\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 830.4417 - val_loss: 1829.1896\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 857.3506 - val_loss: 1833.1351\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 855.8489 - val_loss: 1834.7931\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 863.7326 - val_loss: 1836.3464\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 845.0662 - val_loss: 1836.8712\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.2529 - val_loss: 1839.1943\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 896.8638 - val_loss: 1839.5131\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 863.0094 - val_loss: 1839.2087\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 845.8157 - val_loss: 1839.5385\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.9460 - val_loss: 1839.5775\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 840.9738 - val_loss: 1839.7063\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 821.0458 - val_loss: 1839.7687\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 878.8685 - val_loss: 1839.7864\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 848.9549 - val_loss: 1839.8152\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 834.0908 - val_loss: 1839.8483\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 843.1966 - val_loss: 1839.7750\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 845.5690 - val_loss: 1839.7498\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9708 - val_loss: 4.7006\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9098 - val_loss: 4.7020\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9213 - val_loss: 4.7006\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9386 - val_loss: 4.7006\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9551 - val_loss: 4.7009\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9395 - val_loss: 4.7007\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9337 - val_loss: 4.7014\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9392 - val_loss: 4.7008\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9252 - val_loss: 4.7012\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9108 - val_loss: 4.7007\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9206 - val_loss: 4.7007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9411 - val_loss: 4.7008\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9159 - val_loss: 4.7010\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9036 - val_loss: 4.7009\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9472 - val_loss: 4.7012\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9131 - val_loss: 4.7012\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9429 - val_loss: 4.7009\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9450 - val_loss: 4.7009\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9384 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9448 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9172 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9213 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9304 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9205 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9369 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9446 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9400 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9373 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9107 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9298 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9201 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9304 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9202 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9206 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9378 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9271 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9223 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9673 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9435 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9253 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9351 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9330 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9186 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9284 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9430 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9207 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9315 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9330 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9107 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9457 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 7273.4178 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7626.4088 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7799.4592 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7652.9233 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7727.3651 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7390.4919 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7461.1903 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7072.5534 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7889.7566 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7770.1824 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7773.7563 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7865.4176 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7919.1463 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7575.6630 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7680.2097 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7524.7829 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7809.0866 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7644.1863 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7527.7669 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7782.1831 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7797.2926 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7346.2867 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7594.3835 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7288.8191 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7608.1816 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7619.1501 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7788.0477 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7642.3218 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7854.2502 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7775.3547 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7665.8120 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7737.7790 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7528.9864 - val_loss: 15292.4346\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 7405.8090 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7332.8090 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 7589.5466 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7525.9136 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7406.2163 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7394.3815 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7465.9478 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7637.8689 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7573.4887 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7883.7234 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8051.4479 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7636.9963 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7626.7641 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7470.6048 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7364.8869 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7822.6552 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7772.6402 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0870 - val_loss: 4.7013\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9329 - val_loss: 4.7009\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9392 - val_loss: 4.7013\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9208 - val_loss: 4.7013\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9472 - val_loss: 4.7009\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9313 - val_loss: 4.7013\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9353 - val_loss: 4.7009\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9281 - val_loss: 4.7008\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9435 - val_loss: 4.7009\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9404 - val_loss: 4.7008\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9533 - val_loss: 4.7013\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9398 - val_loss: 4.7013\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9415 - val_loss: 4.7010\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9361 - val_loss: 4.7008\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9536 - val_loss: 4.7009\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9252 - val_loss: 4.7009\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9550 - val_loss: 4.7008\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9291 - val_loss: 4.7012\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9240 - val_loss: 4.7010\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9506 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9073 - val_loss: 4.7010\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9427 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9150 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9192 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9377 - val_loss: 4.7011\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9280 - val_loss: 4.7011\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9402 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9282 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9268 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9191 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9558 - val_loss: 4.7010\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9443 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9277 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9376 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9235 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9370 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9438 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9383 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9231 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9169 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9643 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9270 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9166 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9406 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9140 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9273 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9323 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9137 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9476 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9272 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 7853.2329 - val_loss: 15292.4414\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7875.3960 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7751.2548 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7334.1603 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7298.3418 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7379.6222 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7450.5055 - val_loss: 15292.4346\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: 7889.7077 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7985.9316 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 8155.9638 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 8111.2958 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7710.7332 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7573.2863 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7572.2307 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7962.3849 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7453.6285 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7670.8010 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7643.6804 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7341.1902 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7959.9161 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7699.4922 - val_loss: 15292.4346\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7846.7565 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7696.9219 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7549.7879 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: 7610.9902 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7619.7425 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7395.1727 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7515.6244 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7186.2750 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7670.5734 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7539.3143 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7677.4929 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7747.6379 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7691.1594 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7577.7576 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7484.3475 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7465.4637 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7495.1686 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7813.5527 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7590.1445 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7480.0938 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7623.3450 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7672.0457 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7471.8884 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7624.2242 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7764.2601 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7327.2382 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7419.8961 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7467.9364 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7661.8908 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 22.1463 - val_loss: 19.0207\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 12.6822 - val_loss: 4.0211\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.7117 - val_loss: 7.7544\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 20.3126 - val_loss: 5.2968\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.2632 - val_loss: 4.0070\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.9535 - val_loss: 4.8822\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 8.4137 - val_loss: 4.8996\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 9.1370 - val_loss: 48.3231\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 12.5455 - val_loss: 4.6881\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 7.3361 - val_loss: 7.0022\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 5.0406 - val_loss: 5.1825\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 10.6166 - val_loss: 41.9412\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7.7755 - val_loss: 5.3436\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.8312 - val_loss: 4.0408\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 6.0247 - val_loss: 4.2209\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.1468 - val_loss: 3.4020\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.1229 - val_loss: 7.5498\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.2084 - val_loss: 3.0574\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.8866 - val_loss: 3.0069\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.9038 - val_loss: 3.0643\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8615 - val_loss: 2.9567\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.6136 - val_loss: 2.9435\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.1055 - val_loss: 2.9105\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5234 - val_loss: 2.8991\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5549 - val_loss: 2.8752\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5164 - val_loss: 2.8699\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5282 - val_loss: 4.6159\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6691 - val_loss: 2.8543\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4859 - val_loss: 2.8385\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5080 - val_loss: 2.8818\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4779 - val_loss: 2.8550\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3928 - val_loss: 2.8437\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.6697 - val_loss: 2.8286\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.3780 - val_loss: 2.8324\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5552 - val_loss: 2.8463\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5930 - val_loss: 2.8314\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3701 - val_loss: 2.8098\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3643 - val_loss: 2.8157\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3690 - val_loss: 2.8046\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3662 - val_loss: 2.8098\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4267 - val_loss: 2.7997\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5021 - val_loss: 2.7972\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4574 - val_loss: 2.8076\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.4923 - val_loss: 2.8222\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4291 - val_loss: 2.8011\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5651 - val_loss: 6.5828\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6775 - val_loss: 2.8037\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.7067 - val_loss: 2.9524\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.5416 - val_loss: 2.8023\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3618 - val_loss: 2.7914\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 1984.8373 - val_loss: 2509.4241\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 977.7883 - val_loss: 1719.9073\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 878.8632 - val_loss: 1928.1022\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 895.0477 - val_loss: 1743.2583\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 910.0213 - val_loss: 1794.9006\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 920.6806 - val_loss: 2092.5728\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 864.0120 - val_loss: 2028.2070\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 937.7299 - val_loss: 1903.6085\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 876.7287 - val_loss: 2310.8621\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 925.8116 - val_loss: 1828.3862\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 899.3216 - val_loss: 2033.3879\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 889.7257 - val_loss: 1717.7705\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 853.8090 - val_loss: 1875.7961\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 887.3951 - val_loss: 1746.3380\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 960.3189 - val_loss: 1780.6045\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 857.7698 - val_loss: 1983.7629\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 929.3219 - val_loss: 1868.7451\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 869.7341 - val_loss: 2472.7310\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 942.8787 - val_loss: 1975.7074\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 864.1968 - val_loss: 1825.9052\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 912.1268 - val_loss: 2241.7417\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 907.6469 - val_loss: 1949.2723\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 892.6234 - val_loss: 1827.1920\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 870.4917 - val_loss: 1861.1323\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 815.3358 - val_loss: 1812.6727\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 833.4213 - val_loss: 1818.7789\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 828.9025 - val_loss: 1848.2524\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 831.4003 - val_loss: 1892.7894\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 834.9747 - val_loss: 1844.3752\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 852.2747 - val_loss: 1854.7103\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 876.3202 - val_loss: 1825.4773\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 842.4348 - val_loss: 1829.8765\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 835.1968 - val_loss: 1832.4805\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 859.5135 - val_loss: 1837.1876\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 839.1682 - val_loss: 1837.3982\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 861.5506 - val_loss: 1839.0477\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 840.0957 - val_loss: 1839.0314\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 837.7269 - val_loss: 1838.8004\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 826.6507 - val_loss: 1837.3973\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 877.4614 - val_loss: 1841.9298\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 828.7304 - val_loss: 1839.0594\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 816.4389 - val_loss: 1839.9342\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 845.3935 - val_loss: 1840.3502\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 866.7798 - val_loss: 1840.4922\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 845.4420 - val_loss: 1840.5358\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 822.7074 - val_loss: 1840.9193\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 830.8537 - val_loss: 1841.1342\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 893.6894 - val_loss: 1841.2196\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 831.8147 - val_loss: 1840.9325\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 876.1987 - val_loss: 1840.9967\n",
      "(2,)\n",
      "ad_glorot_uniform_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 4.0692 - val_loss: 2.6788\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.2454 - val_loss: 2.5833\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1696 - val_loss: 2.5451\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1407 - val_loss: 2.5428\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1433 - val_loss: 2.5224\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1460 - val_loss: 2.5329\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1325 - val_loss: 2.5472\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1265 - val_loss: 2.5153\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1238 - val_loss: 2.5229\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1294 - val_loss: 2.5375\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1376 - val_loss: 2.5423\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1275 - val_loss: 2.5067\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1398 - val_loss: 2.5198\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1271 - val_loss: 2.5177\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.1857 - val_loss: 2.5059\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1776 - val_loss: 2.5133\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1559 - val_loss: 2.5038\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1631 - val_loss: 2.5174\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2461 - val_loss: 2.6003\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3028 - val_loss: 2.5094\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2180 - val_loss: 2.5615\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3281 - val_loss: 2.5211\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2729 - val_loss: 2.5631\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.6063 - val_loss: 2.5425\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.5076 - val_loss: 2.5102\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2983 - val_loss: 16.4170\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.6315 - val_loss: 2.4983\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.3394 - val_loss: 2.5328\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.6657 - val_loss: 2.5129\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.3905 - val_loss: 2.5259\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.4021 - val_loss: 3.4742\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3699 - val_loss: 2.4993\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1686 - val_loss: 2.4802\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1969 - val_loss: 2.4966\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2502 - val_loss: 2.4797\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1988 - val_loss: 2.4801\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2976 - val_loss: 2.5263\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2837 - val_loss: 2.4992\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5522 - val_loss: 2.4864\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 2.3762 - val_loss: 2.4773\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.4390 - val_loss: 2.5063\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3147 - val_loss: 2.6208\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.6857 - val_loss: 2.5371\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.3005 - val_loss: 2.5002\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.6373 - val_loss: 2.5145\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.4783 - val_loss: 2.6810\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.3799 - val_loss: 11.0437\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.5622 - val_loss: 2.4804\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2.5285 - val_loss: 9.0867\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.7180 - val_loss: 2.4718\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2636.1560 - val_loss: 2376.3350\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 1064.7722 - val_loss: 1978.0199\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 904.4386 - val_loss: 1963.5809\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 898.7876 - val_loss: 1961.8948\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 852.2515 - val_loss: 1848.3413\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 881.7942 - val_loss: 1944.0112\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 875.1638 - val_loss: 1878.4999\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 865.9894 - val_loss: 1845.9058\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 842.0959 - val_loss: 1880.9452\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 850.1270 - val_loss: 1891.7721\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 856.7879 - val_loss: 1860.2144\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 889.1740 - val_loss: 1887.9078\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 894.7049 - val_loss: 1936.4535\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 883.1349 - val_loss: 1813.0602\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 819.9504 - val_loss: 1847.3530\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 862.9607 - val_loss: 1960.3975\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 841.2788 - val_loss: 1897.4238\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 840.1981 - val_loss: 1836.0854\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 886.2013 - val_loss: 1886.5448\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 886.6725 - val_loss: 1842.4120\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 842.5067 - val_loss: 1751.2050\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 876.9848 - val_loss: 1790.7920\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 865.1160 - val_loss: 1895.0504\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 927.3837 - val_loss: 1885.3689\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 867.4681 - val_loss: 1873.8799\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 868.4004 - val_loss: 1901.0531\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 832.7107 - val_loss: 1852.7235\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 883.8055 - val_loss: 1878.2087\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 894.2982 - val_loss: 1867.4398\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 874.1675 - val_loss: 1813.0314\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 847.1150 - val_loss: 1838.0831\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 841.3502 - val_loss: 1837.7523\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 838.5534 - val_loss: 1837.6317\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 851.0598 - val_loss: 1838.5564\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 891.6221 - val_loss: 1866.5267\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 886.3557 - val_loss: 1867.5229\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 853.1572 - val_loss: 1857.6798\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 848.6934 - val_loss: 1848.0253\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 894.7542 - val_loss: 1839.4153\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 863.1669 - val_loss: 1855.2560\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 836.4633 - val_loss: 1853.8152\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 804.2782 - val_loss: 1850.2185\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 844.8723 - val_loss: 1847.9078\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 901.1386 - val_loss: 1848.1875\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 889.6173 - val_loss: 1847.9495\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 856.7847 - val_loss: 1847.0902\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 852.6897 - val_loss: 1846.9939\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 853.0018 - val_loss: 1846.8331\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 872.3404 - val_loss: 1847.4614\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 887.3844 - val_loss: 1848.3503\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 4.2318 - val_loss: 4.9275\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1430 - val_loss: 4.9100\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1309 - val_loss: 4.8759\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1020 - val_loss: 4.8641\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0496 - val_loss: 4.8428\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0707 - val_loss: 4.8184\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0372 - val_loss: 4.8193\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0285 - val_loss: 4.8098\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0290 - val_loss: 4.8093\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0312 - val_loss: 4.8056\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0268 - val_loss: 4.8005\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0509 - val_loss: 4.8043\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0014 - val_loss: 4.8346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 4.0467 - val_loss: 4.8113\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0209 - val_loss: 4.8024\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0130 - val_loss: 4.8030\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0397 - val_loss: 4.7968\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0466 - val_loss: 4.7964\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0245 - val_loss: 4.8096\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0172 - val_loss: 4.7971\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0220 - val_loss: 4.7917\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0277 - val_loss: 4.7950\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0074 - val_loss: 4.7990\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0000 - val_loss: 4.7913\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0289 - val_loss: 4.7924\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0064 - val_loss: 4.7911\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0039 - val_loss: 4.7887\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0380 - val_loss: 4.7919\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0201 - val_loss: 4.7915\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9878 - val_loss: 4.7987\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9893 - val_loss: 4.7887\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0357 - val_loss: 4.7917\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0240 - val_loss: 4.7934\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0037 - val_loss: 4.7975\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0391 - val_loss: 4.7938\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0494 - val_loss: 4.7994\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0341 - val_loss: 4.7860\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0649 - val_loss: 4.7880\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0245 - val_loss: 4.7919\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0359 - val_loss: 4.7894\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0035 - val_loss: 4.7868\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0152 - val_loss: 4.7954\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0359 - val_loss: 4.7904\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0178 - val_loss: 4.7900\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9929 - val_loss: 4.7889\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0248 - val_loss: 4.7884\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0006 - val_loss: 4.7880\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9576 - val_loss: 4.7146\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9521 - val_loss: 4.7146\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9396 - val_loss: 4.7145\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 7500.9540 - val_loss: 15292.8359\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7515.8996 - val_loss: 15292.7422\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7725.9570 - val_loss: 15292.7236\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7467.4511 - val_loss: 15292.7100\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7773.1121 - val_loss: 15292.6875\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7952.7670 - val_loss: 15292.6562\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 7672.0197 - val_loss: 15292.6357\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7441.8784 - val_loss: 15292.6416\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 7848.4970 - val_loss: 15292.6172\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7754.8751 - val_loss: 15292.6055\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7641.9757 - val_loss: 15292.6055\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7509.2842 - val_loss: 15292.5908\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7554.8636 - val_loss: 15292.5898\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7472.7085 - val_loss: 15292.5898\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7643.1909 - val_loss: 15292.5898\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7686.0943 - val_loss: 15292.5781\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8030.7869 - val_loss: 15292.5781\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7744.8050 - val_loss: 15292.5762\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7930.5947 - val_loss: 15292.5771\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 8030.4827 - val_loss: 15292.5635\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7341.9812 - val_loss: 15292.5596\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7681.1270 - val_loss: 15292.5615\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7863.3642 - val_loss: 15292.5566\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7520.4231 - val_loss: 15292.5576\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7328.8914 - val_loss: 15292.5527\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7641.5060 - val_loss: 15292.5557\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7859.6130 - val_loss: 15292.5566\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7396.4978 - val_loss: 15292.5449\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7414.1841 - val_loss: 15292.5527\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7332.1870 - val_loss: 15292.5439\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7613.9175 - val_loss: 15292.5469\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7530.3854 - val_loss: 15292.5430\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7276.5972 - val_loss: 15292.5430\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7500.9810 - val_loss: 15292.5391\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7358.0238 - val_loss: 15292.5381\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7475.1652 - val_loss: 15292.5400\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7830.2566 - val_loss: 15292.5391\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7577.0045 - val_loss: 15292.5391\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7688.0947 - val_loss: 15292.5381\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7541.6304 - val_loss: 15292.5361\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 8112.1478 - val_loss: 15292.5352\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7555.6771 - val_loss: 15292.5352\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7950.7631 - val_loss: 15292.5352\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7808.9075 - val_loss: 15292.5361\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7688.5010 - val_loss: 15292.5352\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7955.2099 - val_loss: 15292.5352\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7984.7074 - val_loss: 15292.5361\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 8040.4746 - val_loss: 15292.5312\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7359.4973 - val_loss: 15292.5332\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7896.0418 - val_loss: 15292.5312\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.3149 - val_loss: 4.9293\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1327 - val_loss: 4.9003\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1319 - val_loss: 4.8844\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0862 - val_loss: 4.8659\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0598 - val_loss: 4.8502\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 4.0436 - val_loss: 4.8415\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0662 - val_loss: 4.8317\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0443 - val_loss: 4.8294\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 4.0342 - val_loss: 4.8157\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0527 - val_loss: 4.8089\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0314 - val_loss: 4.8044\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0299 - val_loss: 4.7994\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0032 - val_loss: 4.7946\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0287 - val_loss: 4.7941\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0074 - val_loss: 4.7905\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 3.9989 - val_loss: 4.7848\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0024 - val_loss: 4.7826\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 4.0057 - val_loss: 4.7796\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 3.9932 - val_loss: 4.7797\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 3.9843 - val_loss: 4.7732\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0233 - val_loss: 4.7741\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.0014 - val_loss: 4.7693\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 3.9898 - val_loss: 4.7740\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 3.9778 - val_loss: 4.7654\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0024 - val_loss: 4.7668\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9790 - val_loss: 4.7723\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9943 - val_loss: 4.7610\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9633 - val_loss: 4.7626\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0194 - val_loss: 4.7613\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9878 - val_loss: 4.7572\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9801 - val_loss: 4.7550\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9768 - val_loss: 4.7561\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9966 - val_loss: 4.7581\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9758 - val_loss: 4.7571\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9754 - val_loss: 4.7515\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0012 - val_loss: 4.7525\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9865 - val_loss: 4.7504\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9786 - val_loss: 4.7526\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9979 - val_loss: 4.7500\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9674 - val_loss: 4.7478\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9999 - val_loss: 4.7467\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9643 - val_loss: 4.7492\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9729 - val_loss: 4.7474\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9720 - val_loss: 4.7453\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9848 - val_loss: 4.7441\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9899 - val_loss: 4.7452\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9542 - val_loss: 4.7445\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9723 - val_loss: 4.7444\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9747 - val_loss: 4.7423\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9689 - val_loss: 4.7505\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 27ms/step - loss: 7493.3218 - val_loss: 15292.9629\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7568.8046 - val_loss: 15292.9033\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7770.0026 - val_loss: 15292.8652\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7567.2107 - val_loss: 15292.8096\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7833.9382 - val_loss: 15292.7852\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7659.4583 - val_loss: 15292.7441\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7944.6698 - val_loss: 15292.7461\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7786.8054 - val_loss: 15292.7002\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7532.5593 - val_loss: 15292.6865\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7691.7287 - val_loss: 15292.6719\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7679.8498 - val_loss: 15292.6562\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7963.4227 - val_loss: 15292.6484\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7135.4645 - val_loss: 15292.6367\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7431.6429 - val_loss: 15292.6260\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7846.3712 - val_loss: 15292.6211\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7795.6757 - val_loss: 15292.6123\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7530.9824 - val_loss: 15292.6035\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 7887.6441 - val_loss: 15292.6084\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7707.7471 - val_loss: 15292.5938\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7653.6132 - val_loss: 15292.6182\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7413.6651 - val_loss: 15292.5850\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7925.2983 - val_loss: 15292.5781\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7805.6877 - val_loss: 15292.5752\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7583.4084 - val_loss: 15292.5674\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7491.8521 - val_loss: 15292.5654\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7664.3709 - val_loss: 15292.5635\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7512.3836 - val_loss: 15292.5684\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7686.6864 - val_loss: 15292.5527\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7424.4004 - val_loss: 15292.5469\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7314.7814 - val_loss: 15292.5498\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7214.4945 - val_loss: 15292.5469\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7855.9536 - val_loss: 15292.5449\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7782.7005 - val_loss: 15292.5410\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7854.8527 - val_loss: 15292.5449\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7366.9318 - val_loss: 15292.5391\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7395.2588 - val_loss: 15292.5469\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7816.3190 - val_loss: 15292.5381\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7625.1505 - val_loss: 15292.5342\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7521.5602 - val_loss: 15292.5439\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7595.6883 - val_loss: 15292.5352\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7881.4099 - val_loss: 15292.5283\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7852.3190 - val_loss: 15292.5283\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 8032.5016 - val_loss: 15292.5254\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7800.8682 - val_loss: 15292.5264\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7824.5376 - val_loss: 15292.5273\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7764.5956 - val_loss: 15292.5215\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7652.2164 - val_loss: 15292.5186\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7910.4511 - val_loss: 15292.5205\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7788.3773 - val_loss: 15292.5156\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7370.9783 - val_loss: 15292.5166\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: nan - val_loss: nan\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.01\n",
      "tensorflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 30.4082 - val_loss: 2.8529\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.0431 - val_loss: 3.4482\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 5.3169 - val_loss: 3.0159\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 6.1612 - val_loss: 2.9990\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.9848 - val_loss: 3.0089\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 6.3933 - val_loss: 8.1721\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.4414 - val_loss: 2.9287\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.7011 - val_loss: 3.3370\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 4.5955 - val_loss: 11.0310\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7.6910 - val_loss: 3.1081\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 5.6614 - val_loss: 3.1518\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.3912 - val_loss: 2.5721\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1785 - val_loss: 2.5247\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1571 - val_loss: 2.5173\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1573 - val_loss: 2.5157\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1386 - val_loss: 2.5120\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1310 - val_loss: 2.5005\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1296 - val_loss: 2.5048\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1246 - val_loss: 2.5012\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1185 - val_loss: 2.5004\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1088 - val_loss: 2.5347\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1073 - val_loss: 2.5349\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1222 - val_loss: 2.5405\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1049 - val_loss: 2.4934\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0960 - val_loss: 2.5093\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1193 - val_loss: 2.4991\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1068 - val_loss: 2.5012\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0960 - val_loss: 2.5693\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1048 - val_loss: 2.4992\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1016 - val_loss: 2.4969\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1088 - val_loss: 2.4980\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1031 - val_loss: 2.4818\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1091 - val_loss: 2.4775\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1171 - val_loss: 2.4877\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1100 - val_loss: 2.4889\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1071 - val_loss: 2.4829\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.1127 - val_loss: 2.5980\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1195 - val_loss: 2.5399\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0944 - val_loss: 2.4855\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1175 - val_loss: 2.4776\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0929 - val_loss: 3.3539\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1277 - val_loss: 2.5178\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1120 - val_loss: 2.4833\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0762 - val_loss: 2.4698\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0626 - val_loss: 2.4598\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0610 - val_loss: 2.4649\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0557 - val_loss: 2.4597\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0601 - val_loss: 2.4602\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0551 - val_loss: 2.4612\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0528 - val_loss: 2.4596\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 1404.4915 - val_loss: 2019.9962\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 964.2848 - val_loss: 1773.7496\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 916.5568 - val_loss: 1863.8904\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 929.5590 - val_loss: 1780.5172\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 870.3682 - val_loss: 1829.3567\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 860.5374 - val_loss: 1836.4248\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.8075 - val_loss: 1932.2146\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 939.6198 - val_loss: 2001.0511\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 930.2890 - val_loss: 1773.8221\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 823.2032 - val_loss: 1684.4570\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 898.8391 - val_loss: 1957.8699\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 864.6619 - val_loss: 1918.8986\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 885.9711 - val_loss: 1826.2606\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 908.8413 - val_loss: 1798.3604\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 921.6456 - val_loss: 1768.3447\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 897.6679 - val_loss: 1836.1877\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 923.0389 - val_loss: 1884.0361\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 872.1946 - val_loss: 1848.4708\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 896.4699 - val_loss: 1909.4989\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 915.7432 - val_loss: 1900.0413\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 885.9370 - val_loss: 1834.7971\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 840.8081 - val_loss: 1808.7094\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 822.9708 - val_loss: 1855.5668\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 818.5163 - val_loss: 1822.3505\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 846.0946 - val_loss: 1811.3586\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 859.7958 - val_loss: 1826.9338\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 857.7782 - val_loss: 1824.8264\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 814.6879 - val_loss: 1830.7345\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 807.4680 - val_loss: 1816.6790\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 835.7303 - val_loss: 1810.4761\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 880.3329 - val_loss: 1820.5166\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 842.8961 - val_loss: 1825.8210\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 811.5055 - val_loss: 1826.8792\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 845.1527 - val_loss: 1828.0417\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 884.4238 - val_loss: 1831.8568\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 873.6651 - val_loss: 1830.2666\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 861.0397 - val_loss: 1831.1711\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 866.5886 - val_loss: 1829.9575\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 908.3552 - val_loss: 1832.0028\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 865.9545 - val_loss: 1831.1215\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 842.5310 - val_loss: 1831.2609\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 860.5753 - val_loss: 1831.3876\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 859.6792 - val_loss: 1831.3041\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 877.7220 - val_loss: 1831.4087\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 857.8484 - val_loss: 1831.5140\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 819.3570 - val_loss: 1831.5341\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 861.0933 - val_loss: 1831.4945\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 828.9851 - val_loss: 1831.6091\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 883.1693 - val_loss: 1831.8428\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 890.6850 - val_loss: 1831.9131\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "(2,)\n",
      "ad_glorot_uniform_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 4.0204 - val_loss: 2.5925\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.2214 - val_loss: 2.5724\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1719 - val_loss: 2.5787\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1597 - val_loss: 2.5570\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1463 - val_loss: 2.5506\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1420 - val_loss: 2.5539\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1412 - val_loss: 2.5520\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0959 - val_loss: 2.5036\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.0820 - val_loss: 2.4910\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0920 - val_loss: 2.5103\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0933 - val_loss: 2.4884\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0818 - val_loss: 2.4844\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0814 - val_loss: 2.5068\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0949 - val_loss: 2.5076\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0909 - val_loss: 2.4778\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0977 - val_loss: 2.4929\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0910 - val_loss: 2.4939\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0909 - val_loss: 2.4932\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0941 - val_loss: 2.5244\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1022 - val_loss: 2.4938\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0999 - val_loss: 2.4827\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0994 - val_loss: 2.5346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1036 - val_loss: 2.4828\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1025 - val_loss: 2.4731\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0957 - val_loss: 2.4908\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0991 - val_loss: 2.5276\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1039 - val_loss: 2.4718\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0952 - val_loss: 2.5383\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1007 - val_loss: 2.4767\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.1168 - val_loss: 2.5951\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0963 - val_loss: 2.5056\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1045 - val_loss: 2.4839\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1246 - val_loss: 2.4920\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0998 - val_loss: 2.4701\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0984 - val_loss: 2.5155\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1122 - val_loss: 2.5016\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1035 - val_loss: 2.6968\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1042 - val_loss: 2.4985\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0992 - val_loss: 2.5665\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1029 - val_loss: 2.5247\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1156 - val_loss: 2.4865\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1079 - val_loss: 2.4781\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1053 - val_loss: 2.4913\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.1035 - val_loss: 2.5213\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0684 - val_loss: 2.4606\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0607 - val_loss: 2.4617\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0617 - val_loss: 2.4618\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 2.0645 - val_loss: 2.4686\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 2.0525 - val_loss: 2.4598\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 2.0541 - val_loss: 2.4593\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 2751.6586 - val_loss: 2455.0522\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 1083.4011 - val_loss: 2029.9313\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 947.7803 - val_loss: 1987.0730\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 971.7650 - val_loss: 1933.9102\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 867.9495 - val_loss: 1831.6755\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 899.0388 - val_loss: 1792.4246\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 849.1455 - val_loss: 1874.5367\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 875.1294 - val_loss: 1928.4435\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 894.3820 - val_loss: 1905.3927\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 824.5359 - val_loss: 1791.0858\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 894.7272 - val_loss: 1802.4768\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 876.8401 - val_loss: 1921.1675\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 856.1441 - val_loss: 1739.0123\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 884.4617 - val_loss: 1948.3912\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 895.2062 - val_loss: 1904.0129\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 875.1030 - val_loss: 1979.9529\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 859.3248 - val_loss: 1919.4188\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 846.8925 - val_loss: 1812.5748\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 919.9901 - val_loss: 1839.4464\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 865.3853 - val_loss: 1823.4048\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 876.6167 - val_loss: 1913.1602\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 818.1168 - val_loss: 1711.1376\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 846.4583 - val_loss: 1844.4563\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.6266 - val_loss: 1772.2443\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 898.3104 - val_loss: 2009.5730\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 848.8110 - val_loss: 1850.9852\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 849.2430 - val_loss: 1858.7426\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 867.8502 - val_loss: 1782.8936\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 860.5540 - val_loss: 1908.1108\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 866.5794 - val_loss: 1738.7015\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 861.9396 - val_loss: 1855.1228\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 871.8303 - val_loss: 1900.3286\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 857.3871 - val_loss: 1860.2745\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 869.9735 - val_loss: 1840.5208\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 875.4881 - val_loss: 1839.7699\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 809.5440 - val_loss: 1814.6952\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 851.6914 - val_loss: 1841.7053\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.6486 - val_loss: 1835.5238\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 886.4350 - val_loss: 1840.4417\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 867.0804 - val_loss: 1842.7612\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 858.3090 - val_loss: 1840.9725\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 891.7243 - val_loss: 1838.0988\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 856.2271 - val_loss: 1839.4235\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 835.4159 - val_loss: 1840.4662\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 852.8549 - val_loss: 1840.4105\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 832.9170 - val_loss: 1840.7225\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 872.4169 - val_loss: 1841.6346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 828.0400 - val_loss: 1841.0671\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 886.6188 - val_loss: 1842.7189\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 831.3434 - val_loss: 1842.5863\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9746 - val_loss: 4.7006\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9408 - val_loss: 4.7006\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9346 - val_loss: 4.7017\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9382 - val_loss: 4.7006\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9422 - val_loss: 4.7006\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9243 - val_loss: 4.7006\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.8871 - val_loss: 4.7011\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9157 - val_loss: 4.7007\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9308 - val_loss: 4.7009\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9254 - val_loss: 4.7007\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9415 - val_loss: 4.7017\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9204 - val_loss: 4.7010\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9110 - val_loss: 4.7009\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9207 - val_loss: 4.7009\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9322 - val_loss: 4.7010\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9501 - val_loss: 4.7011\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9313 - val_loss: 4.7009\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9393 - val_loss: 4.7011\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9174 - val_loss: 4.7009\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9358 - val_loss: 4.7010\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.9528 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9386 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9163 - val_loss: 4.7010\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9259 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9292 - val_loss: 4.7010\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9402 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9181 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9702 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9157 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9057 - val_loss: 4.7010\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9294 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9371 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9245 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9426 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9217 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9211 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9563 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9546 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9363 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9364 - val_loss: 4.7010\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9163 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9254 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9280 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9484 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9513 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9369 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9027 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9381 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9428 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.9230 - val_loss: 4.7010\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 7386.2180 - val_loss: 15292.4346\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7425.1005 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7368.7412 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7888.6149 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7574.6614 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7528.6276 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7702.5690 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7631.9533 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8110.4385 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7918.1248 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7562.6038 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7873.9723 - val_loss: 15292.4346\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7545.9923 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7566.5132 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7613.0564 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7625.0923 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7378.7890 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7462.6727 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7699.6766 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7297.7876 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7875.4272 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7833.6979 - val_loss: 15292.4346\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7576.2112 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7558.2337 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7354.9210 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7352.8098 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7799.9925 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7690.4466 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7495.8086 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7734.4267 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7614.5414 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7636.3582 - val_loss: 15292.4346\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7522.7707 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7833.8431 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7511.1823 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7567.0483 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7801.7422 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7712.1366 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7582.6388 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8077.4606 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7636.2305 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7410.5786 - val_loss: 15292.4346\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7710.1814 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7720.9213 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7285.3179 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7564.7370 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7497.8243 - val_loss: 15292.4346\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 7364.7330 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7616.0623 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7587.1258 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 4.0447 - val_loss: 4.7010\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9204 - val_loss: 4.7011\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9043 - val_loss: 4.7009\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9049 - val_loss: 4.7008\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9395 - val_loss: 4.7008\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9394 - val_loss: 4.7009\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9222 - val_loss: 4.7009\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9296 - val_loss: 4.7010\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9559 - val_loss: 4.7010\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9214 - val_loss: 4.7008\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9403 - val_loss: 4.7008\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9433 - val_loss: 4.7023\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9451 - val_loss: 4.7012\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9305 - val_loss: 4.7008\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9222 - val_loss: 4.7009\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9278 - val_loss: 4.7011\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9026 - val_loss: 4.7009\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9387 - val_loss: 4.7010\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9009 - val_loss: 4.7008\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9408 - val_loss: 4.7009\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9290 - val_loss: 4.7010\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9538 - val_loss: 4.7010\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9381 - val_loss: 4.7011\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9172 - val_loss: 4.7010\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9229 - val_loss: 4.7010\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9374 - val_loss: 4.7010\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9153 - val_loss: 4.7010\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9335 - val_loss: 4.7010\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9144 - val_loss: 4.7010\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9129 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9277 - val_loss: 4.7010\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9354 - val_loss: 4.7010\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9157 - val_loss: 4.7010\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9326 - val_loss: 4.7010\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9318 - val_loss: 4.7010\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9138 - val_loss: 4.7010\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9351 - val_loss: 4.7010\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9462 - val_loss: 4.7010\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9188 - val_loss: 4.7010\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9353 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9374 - val_loss: 4.7010\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9583 - val_loss: 4.7010\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9332 - val_loss: 4.7010\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9075 - val_loss: 4.7010\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9491 - val_loss: 4.7010\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9397 - val_loss: 4.7010\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.9439 - val_loss: 4.7010\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.9360 - val_loss: 4.7010\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9162 - val_loss: 4.7010\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9483 - val_loss: 4.7010\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 8008.8594 - val_loss: 15292.4414\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7396.5619 - val_loss: 15292.4346\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7777.2479 - val_loss: 15292.4346\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7576.7150 - val_loss: 15292.4346\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7673.1928 - val_loss: 15292.4346\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7612.8716 - val_loss: 15292.4346\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7257.5246 - val_loss: 15292.4346\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7616.8663 - val_loss: 15292.4346\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7504.7943 - val_loss: 15292.4346\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7668.0295 - val_loss: 15292.4346\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7647.3968 - val_loss: 15292.4346\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7306.2276 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7741.3311 - val_loss: 15292.4346\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7687.0056 - val_loss: 15292.4346\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7924.3587 - val_loss: 15292.4346\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7878.2135 - val_loss: 15292.4346\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7361.1401 - val_loss: 15292.4346\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7984.9388 - val_loss: 15292.4346\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7943.9817 - val_loss: 15292.4346\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7737.3952 - val_loss: 15292.4346\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8105.8551 - val_loss: 15292.4346\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 19ms/step - loss: 7638.3720 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7899.8508 - val_loss: 15292.4346\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7692.1335 - val_loss: 15292.4346\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7563.6849 - val_loss: 15292.4346\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7804.3440 - val_loss: 15292.4346\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7597.8169 - val_loss: 15292.4346\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7441.8241 - val_loss: 15292.4346\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7808.3775 - val_loss: 15292.4346\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7509.6753 - val_loss: 15292.4346\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7231.9969 - val_loss: 15292.4346\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7848.8588 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 8024.9236 - val_loss: 15292.4346\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7514.6716 - val_loss: 15292.4346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7444.1065 - val_loss: 15292.4346\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7704.0402 - val_loss: 15292.4346\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7587.1822 - val_loss: 15292.4346\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7374.0366 - val_loss: 15292.4346\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7446.6756 - val_loss: 15292.4346\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: 7639.5738 - val_loss: 15292.4346\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7563.8711 - val_loss: 15292.4346\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7789.4954 - val_loss: 15292.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7382.5424 - val_loss: 15292.4346\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 7534.9749 - val_loss: 15292.4346\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7144.1309 - val_loss: 15292.4346\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7438.7607 - val_loss: 15292.4346\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7656.5962 - val_loss: 15292.4346\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 7576.4744 - val_loss: 15292.4346\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7684.3278 - val_loss: 15292.4346\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 7413.5581 - val_loss: 15292.4346\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 18ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 23.8163 - val_loss: 44.0914\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 20ms/step - loss: 3.6858 - val_loss: 3.8108\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 6.3814 - val_loss: 4.2833\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 14.5004 - val_loss: 10.2158\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 10.4512 - val_loss: 15.2103\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 15.6882 - val_loss: 15.2186\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 19.9774 - val_loss: 17.7821\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 18.9451 - val_loss: 9.1958\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 20.0213 - val_loss: 42.7851\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 22.4585 - val_loss: 35.5332\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 22.7643 - val_loss: 34.9570\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 21.7970 - val_loss: 10.0066\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 6.7566 - val_loss: 11.3760\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 5.7087 - val_loss: 7.5286\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 5.0052 - val_loss: 5.2314\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 5.2745 - val_loss: 6.7770\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 4.7246 - val_loss: 4.9530\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 4.3118 - val_loss: 4.9167\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.3784 - val_loss: 4.9077\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.7838 - val_loss: 6.8240\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 4.0396 - val_loss: 4.7431\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.9553 - val_loss: 4.7038\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.7555 - val_loss: 4.7113\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.7177 - val_loss: 4.7052\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7342 - val_loss: 4.7097\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7231 - val_loss: 4.7031\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.7451 - val_loss: 4.7154\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7031 - val_loss: 4.7039\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.7626 - val_loss: 4.7061\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 3.7397 - val_loss: 4.7086\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7667 - val_loss: 4.7077\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7867 - val_loss: 4.7077\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 3.7652 - val_loss: 4.7074\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.7090 - val_loss: 4.7076\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7696 - val_loss: 4.7083\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.7387 - val_loss: 4.7080\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7641 - val_loss: 4.7083\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.7548 - val_loss: 4.7083\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7521 - val_loss: 4.7077\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7669 - val_loss: 4.7083\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7205 - val_loss: 4.7071\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7450 - val_loss: 4.7074\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.7740 - val_loss: 4.7075\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 3.7611 - val_loss: 4.7076\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7697 - val_loss: 4.7077\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7364 - val_loss: 4.7078\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.7212 - val_loss: 4.7078\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7105 - val_loss: 4.7078\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 3.7499 - val_loss: 4.7079\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 3.7572 - val_loss: 4.7079\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 1605.0169 - val_loss: 1998.9929\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 961.6341 - val_loss: 1947.9326\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 847.9848 - val_loss: 1782.6116\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 857.8235 - val_loss: 1947.5768\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 957.4676 - val_loss: 1835.5314\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 927.0871 - val_loss: 1958.4058\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 904.9876 - val_loss: 1775.6099\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 856.9677 - val_loss: 1840.8718\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 903.1620 - val_loss: 1721.5627\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 907.6293 - val_loss: 1723.5880\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 850.3725 - val_loss: 1773.2198\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 869.5616 - val_loss: 1747.4146\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 886.8540 - val_loss: 1935.3159\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 837.5054 - val_loss: 1908.6445\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 889.0673 - val_loss: 1956.8793\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 873.1663 - val_loss: 1843.7604\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 914.5686 - val_loss: 1704.7131\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 920.2824 - val_loss: 2095.5403\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 881.1101 - val_loss: 1834.4801\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 913.2753 - val_loss: 1811.1772\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 875.7055 - val_loss: 1868.6901\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 895.6455 - val_loss: 1802.1134\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 950.0315 - val_loss: 1854.7706\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 872.1569 - val_loss: 1718.2930\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 921.9811 - val_loss: 1789.4636\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 870.2500 - val_loss: 2073.0088\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 876.6300 - val_loss: 1908.4285\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 860.5370 - val_loss: 1844.3090\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 840.2745 - val_loss: 1834.3844\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 826.5626 - val_loss: 1834.2200\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 862.9828 - val_loss: 1841.7242\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 846.6747 - val_loss: 1893.6554\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 852.4339 - val_loss: 1830.5438\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 857.0446 - val_loss: 1839.8373\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 891.1103 - val_loss: 1864.7498\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 847.1341 - val_loss: 1831.7812\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 838.7053 - val_loss: 1835.7952\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 836.3233 - val_loss: 1835.8896\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 896.8293 - val_loss: 1836.6782\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 813.6390 - val_loss: 1837.2428\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 856.7128 - val_loss: 1835.8228\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 878.5944 - val_loss: 1834.8828\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 834.4228 - val_loss: 1835.0657\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 830.0667 - val_loss: 1836.5219\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 850.8805 - val_loss: 1835.0035\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 860.5260 - val_loss: 1835.9723\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 833.8607 - val_loss: 1836.0697\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 859.7491 - val_loss: 1835.2284\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 867.7870 - val_loss: 1835.0824\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 831.0498 - val_loss: 1834.9772\n",
      "(2,)\n",
      "ad_glorot_normal_0_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 4.0070 - val_loss: 2.6233\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2589 - val_loss: 2.5764\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1680 - val_loss: 2.5380\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1540 - val_loss: 2.5651\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1487 - val_loss: 2.5542\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1518 - val_loss: 2.5242\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1442 - val_loss: 2.5214\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1411 - val_loss: 2.5440\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1502 - val_loss: 2.5440\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 2.1341 - val_loss: 2.5199\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1323 - val_loss: 2.5262\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1395 - val_loss: 2.6344\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1380 - val_loss: 2.5345\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1349 - val_loss: 2.5259\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1350 - val_loss: 2.5300\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.1395 - val_loss: 2.5166\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1351 - val_loss: 2.5557\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1614 - val_loss: 2.5697\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1693 - val_loss: 2.5896\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1715 - val_loss: 2.5234\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.1812 - val_loss: 2.5306\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2030 - val_loss: 2.5170\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 2.1597 - val_loss: 2.7041\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2099 - val_loss: 2.5149\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.2472 - val_loss: 2.5176\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2132 - val_loss: 2.5101\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.1615 - val_loss: 2.5256\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.1949 - val_loss: 2.9860\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.2393 - val_loss: 2.4879\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.2738 - val_loss: 2.4680\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3103 - val_loss: 5.5130\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.5367 - val_loss: 2.5402\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 2.2474 - val_loss: 2.5200\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3662 - val_loss: 2.5346\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3915 - val_loss: 2.5092\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3073 - val_loss: 2.4799\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.3007 - val_loss: 2.5146\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.1432 - val_loss: 2.5196\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.8744 - val_loss: 2.5566\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.4809 - val_loss: 2.4877\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 2.0612 - val_loss: 2.4662\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0522 - val_loss: 2.4638\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0495 - val_loss: 2.4682\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0465 - val_loss: 2.4631\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0459 - val_loss: 2.4620\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0616 - val_loss: 2.4647\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0539 - val_loss: 2.4667\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0518 - val_loss: 2.4611\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 2.0539 - val_loss: 2.4601\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 2.0490 - val_loss: 2.4666\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 5s 21ms/step - loss: 2784.1897 - val_loss: 2370.2273\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 21ms/step - loss: 1003.6767 - val_loss: 2061.9338\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 916.4876 - val_loss: 1867.1531\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 914.2901 - val_loss: 1932.4459\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 884.8978 - val_loss: 1894.4403\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 845.2875 - val_loss: 1824.4771\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 845.0436 - val_loss: 1823.6014\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 875.9485 - val_loss: 1824.7903\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 894.6122 - val_loss: 1883.2778\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 853.1226 - val_loss: 1776.6926\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 866.3321 - val_loss: 1811.2778\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 861.7726 - val_loss: 1829.5997\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 906.3482 - val_loss: 1829.1421\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 919.8159 - val_loss: 1876.5289\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 869.9579 - val_loss: 1805.0991\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 853.9468 - val_loss: 1772.8425\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 846.4058 - val_loss: 1840.0215\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 859.6545 - val_loss: 1802.7926\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 868.0056 - val_loss: 1947.1029\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 885.4340 - val_loss: 1778.2885\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 864.3601 - val_loss: 1841.0447\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 900.6095 - val_loss: 1818.9327\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 846.8912 - val_loss: 1873.2827\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 888.6485 - val_loss: 1898.4089\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 868.5146 - val_loss: 1834.4568\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 829.8333 - val_loss: 1862.6014\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 884.4737 - val_loss: 1872.6121\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 891.2697 - val_loss: 1840.7371\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 864.7068 - val_loss: 1865.9780\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 855.6404 - val_loss: 1830.9241\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 846.4418 - val_loss: 1837.3774\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 873.3204 - val_loss: 1847.9860\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 819.7594 - val_loss: 1837.4097\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 831.6406 - val_loss: 1845.1969\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 847.0385 - val_loss: 1861.0486\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 822.8811 - val_loss: 1822.4652\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 787.7340 - val_loss: 1828.5845\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 841.8420 - val_loss: 1832.9934\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 835.5359 - val_loss: 1835.8087\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 880.7486 - val_loss: 1839.4279\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 872.5329 - val_loss: 1843.2776\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 825.0791 - val_loss: 1842.8368\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 823.1041 - val_loss: 1845.1278\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 876.6111 - val_loss: 1847.3269\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 4s 22ms/step - loss: 838.4281 - val_loss: 1846.1886\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 888.7277 - val_loss: 1847.3379\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 889.4358 - val_loss: 1847.2775\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 4s 21ms/step - loss: 867.5101 - val_loss: 1847.1473\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 851.5709 - val_loss: 1847.0177\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 4s 20ms/step - loss: 914.5905 - val_loss: 1846.9762\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.2557 - val_loss: 4.9335\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1763 - val_loss: 4.9204\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1291 - val_loss: 4.8659\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.1062 - val_loss: 4.8527\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0817 - val_loss: 4.8335\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0773 - val_loss: 4.8202\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0577 - val_loss: 4.8288\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0374 - val_loss: 4.8110\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0222 - val_loss: 4.8068\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0620 - val_loss: 4.8057\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0136 - val_loss: 4.8020\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0444 - val_loss: 4.8016\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0455 - val_loss: 4.7981\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0234 - val_loss: 4.7988\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0263 - val_loss: 4.8034\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0596 - val_loss: 4.7981\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0381 - val_loss: 4.7958\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0187 - val_loss: 4.7966\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0386 - val_loss: 4.7918\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0197 - val_loss: 4.8119\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0147 - val_loss: 4.8079\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0375 - val_loss: 4.7953\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0015 - val_loss: 4.8062\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0042 - val_loss: 4.7912\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0302 - val_loss: 4.7935\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0126 - val_loss: 4.7894\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0203 - val_loss: 4.7929\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0059 - val_loss: 4.7939\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0340 - val_loss: 4.7927\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0081 - val_loss: 4.7933\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0156 - val_loss: 4.7979\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0082 - val_loss: 4.7950\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9969 - val_loss: 4.7926\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0276 - val_loss: 4.7939\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0261 - val_loss: 4.7894\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9977 - val_loss: 4.7878\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0130 - val_loss: 4.7910\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0410 - val_loss: 4.7899\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9877 - val_loss: 4.7936\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0292 - val_loss: 4.7895\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0299 - val_loss: 4.7874\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0332 - val_loss: 4.7930\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0150 - val_loss: 4.7991\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9995 - val_loss: 4.7873\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0088 - val_loss: 4.7891\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0254 - val_loss: 4.7918\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0246 - val_loss: 4.7907\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0323 - val_loss: 4.7919\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0373 - val_loss: 4.7922\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0137 - val_loss: 4.7934\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 25ms/step - loss: 7997.2620 - val_loss: 15292.8213\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7781.3192 - val_loss: 15292.7559\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7517.7400 - val_loss: 15292.7314\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7498.6497 - val_loss: 15292.7021\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7474.1361 - val_loss: 15292.6855\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7360.6812 - val_loss: 15292.6533\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7524.0571 - val_loss: 15292.6377\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7625.2861 - val_loss: 15292.6240\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7297.4444 - val_loss: 15292.6133\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7736.9136 - val_loss: 15292.6133\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7594.9344 - val_loss: 15292.6094\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7419.2642 - val_loss: 15292.5898\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7466.3665 - val_loss: 15292.5869\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7957.3464 - val_loss: 15292.5850\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7701.1994 - val_loss: 15292.5762\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7439.3338 - val_loss: 15292.5674\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7639.4142 - val_loss: 15292.5703\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7361.5948 - val_loss: 15292.5596\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7702.9768 - val_loss: 15292.5732\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7857.9987 - val_loss: 15292.5596\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7921.6871 - val_loss: 15292.5576\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7612.1126 - val_loss: 15292.5566\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7915.9812 - val_loss: 15292.5537\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7747.7059 - val_loss: 15292.5576\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7625.8725 - val_loss: 15292.5518\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7893.3043 - val_loss: 15292.5527\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7567.1324 - val_loss: 15292.5518\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7604.6395 - val_loss: 15292.5488\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7535.0302 - val_loss: 15292.5469\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7655.8646 - val_loss: 15292.5449\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7279.4030 - val_loss: 15292.5469\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7354.4867 - val_loss: 15292.5410\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7535.7611 - val_loss: 15292.5430\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7489.4066 - val_loss: 15292.5400\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7644.7740 - val_loss: 15292.5430\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7426.1938 - val_loss: 15292.5400\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7595.7670 - val_loss: 15292.5391\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7727.5913 - val_loss: 15292.5371\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7818.8452 - val_loss: 15292.5391\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7770.9542 - val_loss: 15292.5361\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7474.8462 - val_loss: 15292.5361\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7397.0449 - val_loss: 15292.5352\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7831.6366 - val_loss: 15292.5352\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7939.6431 - val_loss: 15292.5352\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7349.9799 - val_loss: 15292.5361\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7668.6499 - val_loss: 15292.5332\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7570.9132 - val_loss: 15292.5352\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7693.4316 - val_loss: 15292.5332\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7830.5880 - val_loss: 15292.5312\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7299.1689 - val_loss: 15292.5312\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_sigmoid_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: 4.2840 - val_loss: 4.9316\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1444 - val_loss: 4.9011\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1097 - val_loss: 4.8794\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0995 - val_loss: 4.8716\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.1132 - val_loss: 4.8579\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 4.0541 - val_loss: 4.8448\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0498 - val_loss: 4.8295\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0649 - val_loss: 4.8226\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0252 - val_loss: 4.8147\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0394 - val_loss: 4.8099\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0631 - val_loss: 4.8039\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0408 - val_loss: 4.7995\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9925 - val_loss: 4.7941\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0266 - val_loss: 4.7909\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0005 - val_loss: 4.7883\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0136 - val_loss: 4.7875\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9770 - val_loss: 4.7822\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 4.0142 - val_loss: 4.7794\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 4.0228 - val_loss: 4.7765\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9998 - val_loss: 4.7743\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9934 - val_loss: 4.7699\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9958 - val_loss: 4.7905\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9878 - val_loss: 4.7672\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9875 - val_loss: 4.7654\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9825 - val_loss: 4.7631\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0004 - val_loss: 4.7620\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9938 - val_loss: 4.7611\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0142 - val_loss: 4.7698\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9773 - val_loss: 4.7572\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0079 - val_loss: 4.7560\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0034 - val_loss: 4.7587\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9790 - val_loss: 4.7602\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9887 - val_loss: 4.7553\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9761 - val_loss: 4.7529\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9477 - val_loss: 4.7667\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 3.9943 - val_loss: 4.7560\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9626 - val_loss: 4.7535\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9778 - val_loss: 4.7491\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 4.0043 - val_loss: 4.7504\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9760 - val_loss: 4.7484\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 3.9886 - val_loss: 4.7488\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 3.9917 - val_loss: 4.7602\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 3.9718 - val_loss: 4.7460\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9687 - val_loss: 4.7447\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9787 - val_loss: 4.7461\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9725 - val_loss: 4.7441\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 3.9764 - val_loss: 4.7472\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9844 - val_loss: 4.7471\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 3.9869 - val_loss: 4.7445\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 3.9781 - val_loss: 4.7419\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 31ms/step - loss: 7473.5145 - val_loss: 15292.9609\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7473.4891 - val_loss: 15292.9102\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7656.3248 - val_loss: 15292.8652\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7498.8213 - val_loss: 15292.8135\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7781.8232 - val_loss: 15292.7988\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7633.1443 - val_loss: 15292.7393\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7765.6232 - val_loss: 15292.7305\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7626.6721 - val_loss: 15292.7090\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7317.8361 - val_loss: 15292.6826\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 7665.6887 - val_loss: 15292.6729\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7128.8065 - val_loss: 15292.6650\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7790.0723 - val_loss: 15292.6514\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7901.5908 - val_loss: 15292.6357\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 8091.5525 - val_loss: 15292.6240\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7117.8677 - val_loss: 15292.6152\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7959.5235 - val_loss: 15292.6211\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7325.9873 - val_loss: 15292.6104\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7747.6322 - val_loss: 15292.5967\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7804.0384 - val_loss: 15292.5938\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7905.7450 - val_loss: 15292.5889\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7558.6254 - val_loss: 15292.5811\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7646.7452 - val_loss: 15292.5889\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7813.2341 - val_loss: 15292.5723\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7380.6986 - val_loss: 15292.5664\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7760.5650 - val_loss: 15292.5645\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7836.3340 - val_loss: 15292.5645\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7749.1593 - val_loss: 15292.5713\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7713.6617 - val_loss: 15292.5566\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7609.3531 - val_loss: 15292.5635\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7719.1123 - val_loss: 15292.5635\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 5s 25ms/step - loss: 7805.2820 - val_loss: 15292.5488\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7501.5443 - val_loss: 15292.5449\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7245.2765 - val_loss: 15292.5410\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7548.1763 - val_loss: 15292.5459\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7320.9477 - val_loss: 15292.5391\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7783.6849 - val_loss: 15292.5371\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7455.4421 - val_loss: 15292.5391\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7481.8856 - val_loss: 15292.5371\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7524.4493 - val_loss: 15292.5283\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7558.9705 - val_loss: 15292.5293\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7698.1838 - val_loss: 15292.5283\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7696.2055 - val_loss: 15292.5488\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7402.7503 - val_loss: 15292.5264\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7741.3165 - val_loss: 15292.5244\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7300.3709 - val_loss: 15292.5264\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7532.3754 - val_loss: 15292.5186\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7470.8550 - val_loss: 15292.5166\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7566.6573 - val_loss: 15292.5215\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 7688.8839 - val_loss: 15292.5205\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 7630.2727 - val_loss: 15292.5146\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 6s 26ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_tanh_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: 30.6114\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 25.6862 - val_loss: 25.3873\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 21.5617 - val_loss: 22.4273\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 19.1664 - val_loss: 20.3936\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 17.5550 - val_loss: 18.8694\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 16.2779 - val_loss: 17.6698\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 15.2359 - val_loss: 16.6940\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 14.4200 - val_loss: 15.8851\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 13.7391 - val_loss: 15.2030\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 13.1025 - val_loss: 14.6187\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: inf - val_loss: inf\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: inf - val_loss: inf\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: inf - val_loss: inf\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: inf - val_loss: inf\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: inf\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: inf - val_loss: 28497.0039\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 14440.0949 - val_loss: 26013.4492\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 13736.9582 - val_loss: 24582.3691\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 12714.2407 - val_loss: 23589.1191\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 11674.4848 - val_loss: 22842.5117\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 11381.6084 - val_loss: 22256.4375\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 11091.1875 - val_loss: 21782.2793\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 11173.6600 - val_loss: 21388.3477\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 10895.4161 - val_loss: 21056.4121\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 11051.0314 - val_loss: 20772.7715\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.01\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 31ms/step - loss: 24.2657 - val_loss: 3.3673\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 5.4633 - val_loss: 3.1032\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.4768 - val_loss: 2.8509\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.8983 - val_loss: 3.1548\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.3833 - val_loss: 2.8660\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.4798 - val_loss: 2.9400\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 4.2448 - val_loss: 2.8506\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.0128 - val_loss: 3.3039\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 6.7710 - val_loss: 5.6932\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.7794 - val_loss: 3.1044\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 5.3083 - val_loss: 2.9030\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 6.3095 - val_loss: 3.8866\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 6s 28ms/step - loss: 4.8885 - val_loss: 3.2814\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 6.7298 - val_loss: 3.1571\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 5.5982 - val_loss: 2.8567\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.5986 - val_loss: 3.1522\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.8661 - val_loss: 3.0301\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.4164 - val_loss: 2.5398\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.1762 - val_loss: 2.5228\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.1524 - val_loss: 2.5432\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.1486 - val_loss: 2.4852\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.1265 - val_loss: 2.5111\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.1257 - val_loss: 2.5194\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.1226 - val_loss: 2.5709\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.1129 - val_loss: 2.5483\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.1231 - val_loss: 2.5475\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.1121 - val_loss: 2.6657\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.1129 - val_loss: 2.4861\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.1028 - val_loss: 2.5159\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.1173 - val_loss: 2.5194\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.1098 - val_loss: 2.4893\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0742 - val_loss: 2.4664\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0671 - val_loss: 2.4661\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0651 - val_loss: 2.4691\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0609 - val_loss: 2.4648\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0618 - val_loss: 2.4641\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 2.0568 - val_loss: 2.4594\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0596 - val_loss: 2.4653\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0542 - val_loss: 2.4572\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0565 - val_loss: 2.4603\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0578 - val_loss: 2.4650\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0705 - val_loss: 2.4618\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0592 - val_loss: 2.4630\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0632 - val_loss: 2.4607\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0538 - val_loss: 2.4626\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0560 - val_loss: 2.4621\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0539 - val_loss: 2.4581\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0537 - val_loss: 2.4575\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0538 - val_loss: 2.4596\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0504 - val_loss: 2.4596\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 31ms/step - loss: 1659.0231 - val_loss: 1928.6903\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 977.5999 - val_loss: 2228.9236\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 925.4657 - val_loss: 1654.4144\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 875.9763 - val_loss: 2101.8855\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 920.5491 - val_loss: 1781.1353\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 910.5833 - val_loss: 2013.6581\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 850.6220 - val_loss: 2186.9158\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 947.0916 - val_loss: 1719.8380\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 937.8845 - val_loss: 1874.4039\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 902.8703 - val_loss: 1988.5248\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 872.5312 - val_loss: 1760.3754\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 896.1118 - val_loss: 1890.0627\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 866.1890 - val_loss: 1933.0111\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 912.3863 - val_loss: 1853.8724\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 853.7305 - val_loss: 1847.6410\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 836.4914 - val_loss: 1811.0406\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 872.7283 - val_loss: 1833.6560\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 842.2886 - val_loss: 1844.9713\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 850.8613 - val_loss: 1828.1984\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 854.3626 - val_loss: 1848.2557\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 849.0071 - val_loss: 1820.5576\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 871.7459 - val_loss: 1864.2092\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 834.9536 - val_loss: 1837.1727\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 827.8494 - val_loss: 1835.0597\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 846.8689 - val_loss: 1836.1106\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 7s 32ms/step - loss: 822.6981 - val_loss: 1837.2095\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 865.5366 - val_loss: 1838.8246\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 829.1027 - val_loss: 1838.5321\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 902.5108 - val_loss: 1839.6501\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 839.6885 - val_loss: 1838.5448\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 853.1612 - val_loss: 1837.8934\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 827.4197 - val_loss: 1838.1038\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 885.8658 - val_loss: 1840.7003\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 855.0262 - val_loss: 1840.5366\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 826.7684 - val_loss: 1840.7554\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 6s 30ms/step - loss: 871.3498 - val_loss: 1840.6199\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 826.7337 - val_loss: 1840.4574\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 862.5415 - val_loss: 1840.2744\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 848.3783 - val_loss: 1840.2219\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 805.9082 - val_loss: 1840.2343\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 832.8327 - val_loss: 1840.1005\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 871.0368 - val_loss: 1840.1772\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 862.1222 - val_loss: 1840.0267\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 862.8096 - val_loss: 1840.0389\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 921.2064 - val_loss: 1840.0490\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 863.5165 - val_loss: 1840.0511\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 921.4142 - val_loss: 1840.0574\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 845.3999 - val_loss: 1840.0620\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 842.9974 - val_loss: 1840.0732\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 887.2298 - val_loss: 1840.0935\n",
      "(2,)\n",
      "ad_glorot_normal_1e-05_relu_tied0.001\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 30ms/step - loss: 3.9095 - val_loss: 2.5469\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.1705 - val_loss: 2.5342\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.1041 - val_loss: 2.5104\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0893 - val_loss: 2.4883\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0880 - val_loss: 2.5120\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0827 - val_loss: 2.4774\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 2.0851 - val_loss: 2.5377\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0875 - val_loss: 2.4860\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0843 - val_loss: 2.5203\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0823 - val_loss: 2.5418\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0857 - val_loss: 2.4637\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0900 - val_loss: 2.4797\n",
      "Epoch 13/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0851 - val_loss: 2.4788\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0927 - val_loss: 2.4764\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 2.0928 - val_loss: 2.4829\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0954 - val_loss: 2.4831\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0841 - val_loss: 2.4765\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0839 - val_loss: 2.4785\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.1010 - val_loss: 2.4725\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0980 - val_loss: 2.5354\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0877 - val_loss: 2.4701\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0686 - val_loss: 2.4634\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0579 - val_loss: 2.4634\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0622 - val_loss: 2.4629\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0556 - val_loss: 2.4601\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0506 - val_loss: 2.4647\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0568 - val_loss: 2.4640\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0525 - val_loss: 2.4633\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0520 - val_loss: 2.4619\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0498 - val_loss: 2.4568\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0509 - val_loss: 2.4609\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0559 - val_loss: 2.4622\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0471 - val_loss: 2.4618\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0460 - val_loss: 2.4567\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 2.0533 - val_loss: 2.4590\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0588 - val_loss: 2.4639\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0467 - val_loss: 2.4589\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0521 - val_loss: 2.4588\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0528 - val_loss: 2.4588\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0542 - val_loss: 2.4608\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0596 - val_loss: 2.4647\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0538 - val_loss: 2.4545\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0534 - val_loss: 2.4607\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0478 - val_loss: 2.4704\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0600 - val_loss: 2.4572\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0524 - val_loss: 2.4587\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 2.0519 - val_loss: 2.4538\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 2.0489 - val_loss: 2.4598\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0459 - val_loss: 2.4611\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 2.0507 - val_loss: 2.4569\n",
      "(2,)\n",
      "tensorflow\n",
      "(5543, 2288)\n",
      "Epoch 1/50\n",
      "206/206 [==============================] - 7s 31ms/step - loss: 2705.7892 - val_loss: 2404.0408\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 1036.5237 - val_loss: 2052.0222\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 925.0286 - val_loss: 2091.1135\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 893.5649 - val_loss: 1978.7260\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 8s 37ms/step - loss: 898.9303 - val_loss: 1921.9575\n",
      "Epoch 6/50\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 877.6054 - val_loss: 1854.9136\n",
      "Epoch 7/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.4625 - val_loss: 1903.3163\n",
      "Epoch 8/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 900.6024 - val_loss: 1906.0406\n",
      "Epoch 9/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 864.4011 - val_loss: 1939.6000\n",
      "Epoch 10/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 876.0277 - val_loss: 1874.2209\n",
      "Epoch 11/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 878.2851 - val_loss: 1918.9644\n",
      "Epoch 12/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 858.1339 - val_loss: 1936.9919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 878.8991 - val_loss: 1792.9685\n",
      "Epoch 14/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 867.4519 - val_loss: 1927.1096\n",
      "Epoch 15/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 827.2152 - val_loss: 1870.5656\n",
      "Epoch 16/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 844.9817 - val_loss: 1874.8228\n",
      "Epoch 17/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 890.9647 - val_loss: 1939.5317\n",
      "Epoch 18/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 882.5926 - val_loss: 1895.2770\n",
      "Epoch 19/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 875.0974 - val_loss: 1855.9805\n",
      "Epoch 20/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 824.7744 - val_loss: 1822.7340\n",
      "Epoch 21/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 857.9073 - val_loss: 1868.2041\n",
      "Epoch 22/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 866.7014 - val_loss: 1834.3394\n",
      "Epoch 23/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 856.7839 - val_loss: 1801.6895\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 24/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 886.0921 - val_loss: 1842.7662\n",
      "Epoch 25/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 877.8084 - val_loss: 1842.8610\n",
      "Epoch 26/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 880.7749 - val_loss: 1835.4447\n",
      "Epoch 27/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 850.2829 - val_loss: 1844.2949\n",
      "Epoch 28/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 833.7171 - val_loss: 1852.9131\n",
      "Epoch 29/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 904.2787 - val_loss: 1855.1534\n",
      "Epoch 30/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 908.3625 - val_loss: 1846.6366\n",
      "Epoch 31/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.5159 - val_loss: 1847.9767\n",
      "Epoch 32/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 846.7403 - val_loss: 1841.9458\n",
      "Epoch 33/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 881.2239 - val_loss: 1820.7456\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 34/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 837.7270 - val_loss: 1826.5674\n",
      "Epoch 35/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 880.2397 - val_loss: 1830.8932\n",
      "Epoch 36/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 855.7835 - val_loss: 1833.3141\n",
      "Epoch 37/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 854.0763 - val_loss: 1835.2308\n",
      "Epoch 38/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 825.9347 - val_loss: 1835.2035\n",
      "Epoch 39/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 818.7498 - val_loss: 1836.6322\n",
      "Epoch 40/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 882.3786 - val_loss: 1836.3645\n",
      "Epoch 41/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 864.1556 - val_loss: 1837.9012\n",
      "Epoch 42/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 818.4085 - val_loss: 1838.2681\n",
      "Epoch 43/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 870.0185 - val_loss: 1840.0450\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 44/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 821.0461 - val_loss: 1840.0270\n",
      "Epoch 45/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 812.0160 - val_loss: 1840.1992\n",
      "Epoch 46/50\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 873.4563 - val_loss: 1840.1350\n",
      "Epoch 47/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 899.1779 - val_loss: 1840.1648\n",
      "Epoch 48/50\n",
      "206/206 [==============================] - 5s 24ms/step - loss: 886.6553 - val_loss: 1840.2394\n",
      "Epoch 49/50\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 879.1306 - val_loss: 1840.1609\n",
      "Epoch 50/50\n",
      "206/206 [==============================] - 6s 27ms/step - loss: 910.3743 - val_loss: 1840.1956\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "inits = ['glorot_uniform', 'glorot_normal']\n",
    "L1_norm = [0, 1e-5]\n",
    "L2_norm = [0, 1e-5]\n",
    "act_fun = ['sigmoid', 'tanh','relu']\n",
    "tied = [True]\n",
    "lr = [.01,.001]\n",
    "\n",
    "model_dict_da = {\n",
    "    \"tf_adage\": tf_adage\n",
    "}\n",
    "model_dict_dca = {\n",
    "    \"tf_adage\": tf_adage\n",
    "}\n",
    "\n",
    "for seed in range(10):\n",
    "    for i in inits:\n",
    "    #print(i)\n",
    "        for l in L1_norm:\n",
    "        #print(l)\n",
    "            for a in act_fun:\n",
    "            #print(a)\n",
    "                for t in lr:\n",
    "                    name = 'ad_' + i + '_' + str(l) + '_' + a + '_tied' + str(t)\n",
    "                    print(name)\n",
    "                    mseq = run_count_autoencoder.run_count_autoencoder('../data_files/rnaseq_compendium_filtered.csv',seed=seed+560,lr=t,kl1=l, act = a, tied = True, epochs=50, init=i, batch_size = 10)\n",
    "                    marr = run_count_autoencoder.run_count_autoencoder('../data_files/rnaseq_compendium_filtered_counts_floor.csv',seed=seed+560,lr=t,kl1=l, act = a, tied = True, epochs=50, init=i, batch_size = 10)\n",
    "\n",
    "                    model_dict_da[name] = marr\n",
    "                    model_dict_dca[name] = mseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tf_adage', 'ad_glorot_uniform_0_sigmoid_tied0.01', 'ad_glorot_uniform_0_sigmoid_tied0.001', 'ad_glorot_uniform_0_tanh_tied0.01', 'ad_glorot_uniform_0_tanh_tied0.001', 'ad_glorot_uniform_0_relu_tied0.01', 'ad_glorot_uniform_0_relu_tied0.001', 'ad_glorot_uniform_1e-05_sigmoid_tied0.01', 'ad_glorot_uniform_1e-05_sigmoid_tied0.001', 'ad_glorot_uniform_1e-05_tanh_tied0.01', 'ad_glorot_uniform_1e-05_tanh_tied0.001', 'ad_glorot_uniform_1e-05_relu_tied0.01', 'ad_glorot_uniform_1e-05_relu_tied0.001', 'ad_glorot_normal_0_sigmoid_tied0.01', 'ad_glorot_normal_0_sigmoid_tied0.001', 'ad_glorot_normal_0_tanh_tied0.01', 'ad_glorot_normal_0_tanh_tied0.001', 'ad_glorot_normal_0_relu_tied0.01', 'ad_glorot_normal_0_relu_tied0.001', 'ad_glorot_normal_1e-05_sigmoid_tied0.01', 'ad_glorot_normal_1e-05_sigmoid_tied0.001', 'ad_glorot_normal_1e-05_tanh_tied0.01', 'ad_glorot_normal_1e-05_tanh_tied0.001', 'ad_glorot_normal_1e-05_relu_tied0.01', 'ad_glorot_normal_1e-05_relu_tied0.001'])\n",
      "dict_keys(['tf_adage', 'ad_glorot_uniform_0_sigmoid_tied0.01', 'ad_glorot_uniform_0_sigmoid_tied0.001', 'ad_glorot_uniform_0_tanh_tied0.01', 'ad_glorot_uniform_0_tanh_tied0.001', 'ad_glorot_uniform_0_relu_tied0.01', 'ad_glorot_uniform_0_relu_tied0.001', 'ad_glorot_uniform_1e-05_sigmoid_tied0.01', 'ad_glorot_uniform_1e-05_sigmoid_tied0.001', 'ad_glorot_uniform_1e-05_tanh_tied0.01', 'ad_glorot_uniform_1e-05_tanh_tied0.001', 'ad_glorot_uniform_1e-05_relu_tied0.01', 'ad_glorot_uniform_1e-05_relu_tied0.001', 'ad_glorot_normal_0_sigmoid_tied0.01', 'ad_glorot_normal_0_sigmoid_tied0.001', 'ad_glorot_normal_0_tanh_tied0.01', 'ad_glorot_normal_0_tanh_tied0.001', 'ad_glorot_normal_0_relu_tied0.01', 'ad_glorot_normal_0_relu_tied0.001', 'ad_glorot_normal_1e-05_sigmoid_tied0.01', 'ad_glorot_normal_1e-05_sigmoid_tied0.001', 'ad_glorot_normal_1e-05_tanh_tied0.01', 'ad_glorot_normal_1e-05_tanh_tied0.001', 'ad_glorot_normal_1e-05_relu_tied0.01', 'ad_glorot_normal_1e-05_relu_tied0.001'])\n"
     ]
    }
   ],
   "source": [
    "print(model_dict_da.keys())\n",
    "print(model_dict_dca.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x1152 with 24 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "yd = len(inits) * len(L1_norm)\n",
    "xd = len(act_fun) * len(lr)\n",
    "fig, ax = plt.subplots(xd, yd,figsize=(xd*4 ,yd *4))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "xi = 0\n",
    "yi = 0\n",
    "for l in L1_norm:\n",
    "    for i in inits:\n",
    "        xi = 0\n",
    "        for a in act_fun:\n",
    "            for t in lr:\n",
    "                name = 'ad_' + i + '_' + str(l) + '_' + a + '_tied' + str(t) \n",
    "                #print(name)\n",
    "                model_temp = model_dict_dca[name]\n",
    "                ax[xi,yi].plot(list(range(0,50)), model_temp.loss, linewidth=1, markersize=2, color = 'orange')\n",
    "                ax[xi,yi].set(title = name, xlabel = 'Epochs', ylabel = 'Loss')\n",
    "                xi = xi+1\n",
    "        yi=yi+1\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x1152 with 24 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "yd = len(inits) * len(L1_norm)\n",
    "xd = len(act_fun) * len(lr)\n",
    "fig, ax = plt.subplots(xd, yd,figsize=(xd*4 ,yd *4))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "xi = 0\n",
    "yi = 0\n",
    "for l in L1_norm:\n",
    "    for i in inits:\n",
    "        xi = 0\n",
    "        for a in act_fun:\n",
    "            for t in lr:\n",
    "                name = 'ad_' + i + '_' + str(l) + '_' + a + '_tied' + str(t) \n",
    "                #print(name)\n",
    "                model_temp = model_dict_da[name]\n",
    "                ax[xi,yi].plot(list(range(0,50)), model_temp.loss, linewidth=1, markersize=2, color = 'orange')\n",
    "                ax[xi,yi].set(title = name, xlabel = 'Epochs', ylabel = 'Loss')\n",
    "                xi = xi+1\n",
    "        yi=yi+1\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
